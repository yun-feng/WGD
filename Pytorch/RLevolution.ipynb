{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-20.6548,  -0.4692,  -0.7382], grad_fn=<MulBackward0>)\n",
      "tensor([1, 1, 1])\n",
      "tensor(142.4626, grad_fn=<MeanBackward0>)\n",
      "tensor([[[0., 0., 0.]]])\n",
      "tensor([[[-0.1921, -0.3661, -0.4447]]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "#Reward.py\n",
    "#Define the reward function r(s,a) when a is not a special action END\n",
    "\n",
    "#For each chromosome, we consider 50 SNP loci\n",
    "chrom_width=50;\n",
    "#44 normal chromosomes in human genome\n",
    "num_chromosome=44\n",
    "\n",
    "\n",
    "#normalisation constant to make normal_const*(\\sum_i a_i) <1, so that the possibility of all CNV sum to a real value smaller than 1\n",
    "normal_const=5e-5;\n",
    "#probability of single locus gain/loss\n",
    "single_loci_loss=normal_const*(1-2e-1);\n",
    "#probability of WGD\n",
    "WGD=normal_const*0.6;\n",
    "\n",
    "#log probability of CNV\n",
    "#used for calculating the distribution of p(a) when a is a focal CNV\n",
    "const1=normal_const*(1-1e-1);\n",
    "const2=2;\n",
    "\n",
    "#Whole chromosome change probability\n",
    "Whole_Chromosome_CNV=normal_const*0.99/10;\n",
    "Half_Chromosome_CNV=normal_const*0.99/15;\n",
    "\n",
    "max_copy=20\n",
    "\n",
    "def Reward(Start,End):\n",
    "    Start=Start.to(torch.float32)\n",
    "    End=End.to(torch.float32)\n",
    "    reward=torch.log(const1/(const2+torch.log(End-Start)))\n",
    "    #chromosome changes\n",
    "    for i in range(Start.shape[0]):\n",
    "        #full chromosome\n",
    "        if End[i]-Start[i]>chrom_width-0.5:\n",
    "            reward[i]=math.log(Whole_Chromosome_CNV)\n",
    "        #arm level changes\n",
    "        if chrom_width-End[i]<0.5 and abs(chrom_width//2-Start[i])<1.5:\n",
    "            reward[i]=math.log(Half_Chromosome_CNV)\n",
    "        if Start[i]<0.5 and abs(chrom_width//2-End[i])<1.5:\n",
    "            reward[i]=math.log(Half_Chromosome_CNV)\n",
    "    return reward\n",
    "\n",
    "\n",
    "\n",
    "#Q-function.py\n",
    "#defining the Q-function \n",
    "#Q(s,a) in manuscript\n",
    "\n",
    "\n",
    "#switch structure mentioned in section 3.3.4\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_switch = [40,60,120]\n",
    "activatiion_wgd=F.relu\n",
    "class WGD_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WGD_Net, self).__init__()\n",
    "        #chromosome permutation invariant structure as described in section 3.3.3\n",
    "        #slide for chromosome is 1 and the filter length in this dimension is also 1\n",
    "        #thus, the same filter goes through all chromosomes in the same fashion\n",
    "        self.conv1=nn.Conv2d(1, nkernels_switch[0], (1,3),(1,1),(0,1))\n",
    "        self.conv2=nn.Conv2d(nkernels_switch[0],nkernels_switch[1] , (1,3),(1,1), (0,1))\n",
    "        self.conv3=nn.Conv2d(nkernels_switch[1],nkernels_switch[2] , (1,5),(1,1), (0,0))\n",
    "        self.linear=nn.Linear(nkernels_switch[2],1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y=x.mean((1,2,3))\n",
    "        y=y.reshape(x.shape[0],1)\n",
    "        x=x.reshape(x.shape[0],1,num_chromosome,chrom_width)\n",
    "        x=F.max_pool2d(activatiion_wgd(self.conv1(x)),(1,5),(1,5),(0,0))\n",
    "        x=F.max_pool2d(activatiion_wgd(self.conv2(x)),(1,2),(1,2),(0,0))\n",
    "        x=(activatiion_wgd(self.conv3(x))).sum((2,3))\n",
    "        x=self.linear(x)\n",
    "        #residule representation in x as described in section 3.3.4\n",
    "        x=20*(y-1.5)+x\n",
    "        x=torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "#chromosome evaluation net \n",
    "#Used in Chrom_NN (which is Q_{phi_1}(s,c) in section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_chr = [80,120,160]\n",
    "activation_cnp=F.relu\n",
    "class CNP_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNP_Val, self).__init__()\n",
    "        self.conv1=nn.Conv2d(1, nkernels_chr[0], (1,5),(1,1),(0,2))\n",
    "        self.conv2=nn.Conv2d(nkernels_chr[0],nkernels_chr[1] , (1,3),(1,1), (0,1))\n",
    "        self.conv3=nn.Conv2d(nkernels_chr[1],nkernels_chr[2] , (1,3),(1,1), (0,1))\n",
    "        self.conv4=nn.Conv2d(nkernels_chr[2],1, (1,5),(1,1), (0,0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=F.max_pool2d(activation_cnp(self.conv1(x)),(1,3),(1,3),(0,1))\n",
    "        x=F.max_pool2d(activation_cnp(self.conv2(x)),(1,2),(1,2),(0,1))\n",
    "        x=F.max_pool2d(activation_cnp(self.conv3(x)),(1,2),(1,2),(0,1))\n",
    "        #KL divergence is always nonpositive\n",
    "        x=0.25+activation_cnp(self.conv4(x),0.25)\n",
    "        #number of sample * 44 chromosomes\n",
    "        x=x.reshape(x.shape[0],num_chromosome)\n",
    "        return x\n",
    "\n",
    "#Implemts Q_{phi_1}(s,c) in section 3.3.1\n",
    "#It combines two chromosome evaluation nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_1}(s,c)\n",
    "class Chrom_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Chrom_NN,self).__init__()\n",
    "        #two parts of the Chrom_NN\n",
    "        #NN for CNP without WGD \n",
    "        self.Val_noWGD=CNP_Val()\n",
    "        #NN for CNP with WGD\n",
    "        self.Val_WGD=CNP_Val()\n",
    "    \n",
    "    def forward(self,x,sigma):\n",
    "        #probability for WGD, which is computed by switch structure\n",
    "        sigma=sigma.expand(-1,num_chromosome)\n",
    "        #we assume the copy number for each loci ranges from 0~9\n",
    "        #for samples without WGD\n",
    "        \n",
    "        #y represents if a chromosome have abnormal copy numbers (positions with copy number other than 1)\n",
    "        y=torch.ceil((torch.abs(x-1)).mean(3)/max_copy)\n",
    "        y=y.reshape(x.shape[0],num_chromosome)\n",
    "        y=y.detach()\n",
    "        #Residule representation mentioned in section 3.3.4\n",
    "        #the value for Q_{phi_1}(s,c) is computed as Val_no (the value estimated by the neural net, the residule part)+ y (the empirial estimation) \n",
    "        Val_no=self.Val_noWGD.forward(x)\n",
    "        #chromosome with all 1 copies don't need any CNV and thus will be less likely mutated.\n",
    "        Val_no=y*math.log(single_loci_loss)*2+(((1-y)*Val_no).sum(1)).reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
    "        #for samples with WGD\n",
    "        #it is similar to the previsou part, where z is an equivalent for y and Val_wgd is an equivalent for Val_no\n",
    "        z=torch.ceil((torch.abs(x-2*(x//2))).mean(3)/max_copy)\n",
    "        z=z.reshape(x.shape[0],num_chromosome)\n",
    "        z=z.detach()\n",
    "        Val_wgd=self.Val_WGD.forward(x)\n",
    "        Val_wgd=z*math.log(single_loci_loss)*2+((1-z)*Val_wgd).sum(1).reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
    "        \n",
    "        #combine two NN with switch as defined in Section 3.3.4\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        x=-x\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "#starting point and gain or loss (defined as sp in manuscript) \n",
    "#Used in CNV_NN (which is Q_{phi_2}(s,c,sp) on section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_CNV = [80,120,160,10]\n",
    "activation_cnv=F.relu\n",
    "class CNV_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNV_Val,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1, nkernels_CNV[0], (1,7),(1,1),(0,3))\n",
    "        self.conv2=nn.Conv2d(nkernels_CNV[0],nkernels_CNV[1] , (1,7),(1,1), (0,3))\n",
    "        self.conv3=nn.Conv2d(nkernels_CNV[1],nkernels_CNV[2] , (1,7),(1,1), (0,3))\n",
    "        self.conv4=nn.Conv2d(nkernels_CNV[2], nkernels_CNV[3], (1,7),(1,1), (0,3))\n",
    "        self.linear=nn.Linear(nkernels_CNV[3]*chrom_width,2*chrom_width-1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=activation_cnv(self.conv1(x))\n",
    "        x=activation_cnv(self.conv2(x))\n",
    "        x=activation_cnv(self.conv3(x))\n",
    "        x=activation_cnv(self.conv4(x))\n",
    "        x=x.reshape(x.shape[0],nkernels_CNV[3]*chrom_width)\n",
    "        x=self.linear(x)\n",
    "        #number of samples* [(50 regions)*(2(gain or loss))-1] \n",
    "        #Only have 50*2-1=99 output dimensions because we fix the average these output\n",
    "        #The average of them could be arbitrary because of the partitioning\n",
    "        return x\n",
    "\n",
    "#Implemts Q_{phi_2}(s,c,sp) in section 3.3.1\n",
    "#It combines two CNV_Val nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_2}(s,c,sp)\n",
    "class CNV_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNV_NN,self).__init__()\n",
    "        #two network setting\n",
    "        self.CNV_noWGD=CNV_Val()\n",
    "        self.CNV_WGD=CNV_Val()\n",
    "    \n",
    "    def forward(self,x,sigma):\n",
    "        #as in section 3.3.4\n",
    "        #y is the empirical estimation\n",
    "        #Val_no is the redidule representation\n",
    "        y=torch.Tensor(x.shape[0],chrom_width,2)\n",
    "        y[:,:,0]=F.relu(1-x)\n",
    "        y[:,:,1]=F.relu(x-1)\n",
    "        y=y.reshape(x.shape[0],2*chrom_width)\n",
    "        y=y[:,1:(2*chrom_width)]-y[:,0:1].expand(-1,2*chrom_width-1)\n",
    "        y=y.detach()\n",
    "        Val_no=self.CNV_noWGD.forward(x.reshape(x.shape[0],1,1,chrom_width))\n",
    "        Val_no=y+Val_no\n",
    "        \n",
    "        z=((torch.abs(x-2*(x//2))).reshape(x.shape[0],chrom_width,1)).expand(-1,-1,2)\n",
    "        z=z.reshape(x.shape[0],2*chrom_width)\n",
    "        z=z[:,1:(2*chrom_width)]-z[:,0:1].expand(-1,2*chrom_width-1)\n",
    "        z=z.detach()\n",
    "        Val_wgd=self.CNV_WGD.forward(x.reshape(x.shape[0],1,1,chrom_width))\n",
    "        Val_wgd=z+Val_wgd\n",
    "        #switch\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        return(x)\n",
    "    \n",
    "     \n",
    "    def find_one_cnv(self,chrom,sigma):\n",
    "        #used for finding the cnv during deconvolution\n",
    "        #it is not used in training process\n",
    "        \n",
    "        res_cnv=self.forward(chrom,sigma)\n",
    "        #if there is originally a break point for start\n",
    "        #rule system \n",
    "        break_start=torch.zeros(chrom.shape[0],50,2,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(chrom.shape[0],50,requires_grad=False)\n",
    "        chrom_shift[:,1:]=chrom[:,:49]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_start[:,:,1]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_start[:,0,1]=1\n",
    "        #don't allow lose one copy when copy number equalls 0\n",
    "        break_start[:,:,0]=break_start[:,:,1]\n",
    "        break_start[:,:,0]=break_start[:,:,0]*torch.ceil((chrom/2-0.5)/max_copy)\n",
    "        break_start=break_start.reshape(chrom.shape[0],100)\n",
    "        res_cnv_full=torch.zeros(chrom.shape[0],100)\n",
    "        res_cnv_full[:,1:]=res_cnv\n",
    "        #Prior_rule=break_start\n",
    "        res_cnv_full=res_cnv_full+torch.log(break_start)\n",
    "        #best cnv according to the current Q\n",
    "        cnv_max_val,cnv_max=torch.max(res_cnv_full,1)\n",
    "        return int(cnv_max[0])\n",
    "    \n",
    "\n",
    "\n",
    "#end point\n",
    "#Used in End_Point_NN (which is Q_{phi_3}(s,c,sp,ep) on section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_End = [80,120,240]\n",
    "activation_end=F.relu\n",
    "class End_Point_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(End_Point_Val,self).__init__()\n",
    "        self.conv1=nn.Conv2d(2, nkernels_End[0], (1,7),(1,1),(0,3))\n",
    "        self.conv2=nn.Conv2d(nkernels_End[0],nkernels_End[1] , (1,7),(1,1), (0,3))\n",
    "        self.conv3=nn.Conv2d(nkernels_End[1],nkernels_End[2] , (1,7),(1,1), (0,3))\n",
    "        self.linear=nn.Linear(nkernels_End[2]*chrom_width,chrom_width-1)\n",
    "    \n",
    "    def forward(self,old,new):\n",
    "        x=torch.Tensor(old.shape[0],2,1,chrom_width)\n",
    "        x[:,0,0,:]=old\n",
    "        x[:,1,0,:]=new\n",
    "        x=x.detach()\n",
    "        x=activation_end(self.conv1(x))\n",
    "        x=activation_end(self.conv2(x))\n",
    "        x=activation_end(self.conv3(x))\n",
    "        x=x.reshape(x.shape[0],nkernels_End[2]*chrom_width)\n",
    "        x=self.linear(x)\n",
    "        #number of samples* [(chrom_width regions)-1] \n",
    "        #Only have chrom_width-1=49 output dimensions because we fix the average these output\n",
    "        #The average of them could be arbitrary because of the partitioning\n",
    "        return x\n",
    "    \n",
    "#Implemts Q_{phi_3}(s,c,sp,ep) in section 3.3.1\n",
    "#It combines two End_Point_Val nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_3}(s,c,sp,ep)\n",
    "class End_Point_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(End_Point_NN,self).__init__()\n",
    "        #two network setting\n",
    "        self.Val_noWGD=End_Point_Val()\n",
    "        self.Val_WGD=End_Point_Val()\n",
    "    \n",
    "    def forward(self,old,new,sigma):\n",
    "        \n",
    "        y=F.relu((old-1)*(old-new))\n",
    "        y=y[:,1:chrom_width]-y[:,0:1].expand(-1,chrom_width-1)\n",
    "        y=y.detach()\n",
    "        Val_no=self.Val_noWGD.forward(old,new)\n",
    "        Val_no=Val_no+y\n",
    "        \n",
    "        z=(old-2*(old//2))*(1-(new-2*(new//2)))\n",
    "        z=z[:,1:chrom_width]-z[:,0:1].expand(-1,chrom_width-1)\n",
    "        z=z.detach()\n",
    "        Val_wgd=self.Val_WGD.forward(old,new)\n",
    "        Val_wgd=Val_wgd+z\n",
    "        #switch\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def find_end(self,old,new,sigma,start_loci,cnv,valid):\n",
    "        #used for finding the end during loading data\n",
    "        res_end=self.forward(old,new,sigma)\n",
    "        \n",
    "        break_end=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:49]=old[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(old-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        \n",
    "        for i in range(old.shape[0]):\n",
    "            #can't end before starting point\n",
    "            break_end[i,:int(start_loci[i])]=0*break_end[i,:int(start_loci[i])]\n",
    "            #don't allow lose one copy when copy number equalls 1\n",
    "            if(cnv[i]<0.5):\n",
    "                j=int(start_loci[i])+1\n",
    "                while(j<chrom_width):\n",
    "                    if(chrom[i][j]<1.5):\n",
    "                        break\n",
    "                    j=j+1\n",
    "                break_end[i,j:chrom_width]=0*break_end[i,j:chrom_width]\n",
    "        res_end_full=torch.zeros(old.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        #Prior_rule=break_end\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max=torch.max(res_end_full,1)\n",
    "        return end_max+1\n",
    "    \n",
    "    \n",
    "    def find_one_end(self,old,new,sigma,start,cnv):\n",
    "        #used for finding the end during deconvolution\n",
    "        res_end=self.forward(old,new,sigma)\n",
    "        \n",
    "        break_end=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:chrom_width-1]=old[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(old-chrom_shift)/10)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        #can't end before starting point\n",
    "        break_end[0,:start]=0*break_end[0,:start]\n",
    "        #don't allow lose one copy when copy number equalls 1\n",
    "        if(cnv<0.5):\n",
    "            j=start+1\n",
    "            while(j<chrom_width):\n",
    "                if(chrom[0][j]<1.5):\n",
    "                    break\n",
    "                j=j+1\n",
    "            break_end[0,j:chrom_width]=0*break_end[0,j:chrom_width]\n",
    "        res_end_full=torch.zeros(old.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        #Prior_rule=break_end\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max=torch.max(res_end_full,1)\n",
    "        end_max=int(end_max[0])\n",
    "        return end_max+1\n",
    "        \n",
    "\n",
    "#combine all separate networks\n",
    "#add Rule system\n",
    "\n",
    "#calculating the softmax\n",
    "#prevent inf when taking log(exp(x))\n",
    "#log_exp is always gonna be between 1 and the total number of elements\n",
    "def Soft_update(val1,soft1,val2,soft2):\n",
    "    bias=val1.clone()\n",
    "    log_exp=soft1.clone()\n",
    "    set1=[torch.ge(val1,val2)]\n",
    "    bias[set1]=val1[set1]\n",
    "    log_exp[set1]=soft1[set1]+soft2[set1]*torch.exp(val2[set1]-val1[set1])\n",
    "    set2=[torch.lt(val1,val2)]\n",
    "    bias[set2]=val2[set2]\n",
    "    log_exp[set2]=soft2[set2]+soft1[set2]*torch.exp(val1[set2]-val2[set2])\n",
    "    return bias,log_exp\n",
    "\n",
    "\n",
    "\n",
    "#Combine all the separate modules mentioned above\n",
    "#Implementation of Q(s,a)\n",
    "class Q_learning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_learning,self).__init__()\n",
    "        self.switch=WGD_Net()\n",
    "        #the output refer to Q_{\\phi_1}(s,c)\n",
    "        self.Chrom_model=Chrom_NN()\n",
    "        #the output refer to Q_{\\phi_2}(s,c,sp)\n",
    "        self.CNV=CNV_NN()\n",
    "        #the output refer to Q_{\\phi_3}(s,sp,c,ep)\n",
    "        self.End=End_Point_NN()\n",
    "    \n",
    "    \n",
    "    def forward(self,state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid):\n",
    "        '''\n",
    "        computing the final advantage(loss) used for training\n",
    "        loss in Thereom1\n",
    "        state: s in Q(s,a)\n",
    "        next_state: s' in softmaxQ(s',a')\n",
    "        Chr,cnv,end_loci: a in Q(s,a)\n",
    "        chrom,chrom_new,start_loci,end_loci: intermediate results from s,a, which is preprossed to make computation faster\n",
    "            e.g. chrom is CNP of the Chr(part of a) from the state(s)\n",
    "            They could be seen as a mapping without parameters to learn:f(s,a)\n",
    "        valid: a boolean array, indicating if a training sample is valid (e.g. have non negative copy numbers for all loci)\n",
    "        '''\n",
    "        \n",
    "        #computing softmaxQ(s',a')\n",
    "        #It is a tradition in RL that gradient does not backpropogate through softmaxQ(s',a'), but only through Q(s,a) to make convergence faster\n",
    "        #there is no theoritical guarantee behind, and it is only a practical trick\n",
    "        sigma_next=self.switch(next_state)\n",
    "        x,y=self.Softmax(next_state,sigma_next)\n",
    "        x=x+torch.log(y)\n",
    "        #computing r(s,a)\n",
    "        x=x+Reward(start_loci,end_loci)\n",
    "        x=x.detach()\n",
    "        \n",
    "        #computing Q(s,a)\n",
    "        sigma=self.switch.forward(state)\n",
    "        #Q_{phi_1}(s,c)\n",
    "        res_chrom=self.Chrom_model.forward(state,sigma)\n",
    "        \n",
    "        #Q_{phi_2}(s,c,sp)\n",
    "        res_cnv=self.CNV.forward(chrom,sigma)\n",
    "        #if there is originally a break point for start\n",
    "        #real world constraint as described in section 3.3.2\n",
    "        #only allow starting points (sp) to be the break points of CNP\n",
    "        break_start=torch.zeros(state.shape[0],chrom_width,2,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,1:]=chrom[:,:(chrom_width-1)]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_start[:,:,1]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_start[:,0,1]=1\n",
    "        #don't allow lose one copy when copy number equals 0, otherwise there is going to be negative copy numbers\n",
    "        break_start[:,:,0]=break_start[:,:,1]\n",
    "        break_start[:,:,0]=break_start[:,:,0]*torch.ceil((chrom/2-0.5)/max_copy)\n",
    "        break_start=break_start.reshape(state.shape[0],2*chrom_width)\n",
    "        res_cnv_full=torch.zeros(state.shape[0],2*chrom_width)\n",
    "        res_cnv_full[:,1:]=res_cnv\n",
    "        res_cnv_full=res_cnv_full+torch.log(break_start)\n",
    "        \n",
    "        #Q_{phi_2}(s,c,sp)-softmax(Q_{phi_2}(s,c,sp)) as described in section 3.3.1\n",
    "        cnv_max_val,cnv_max=torch.max(res_cnv_full,1)\n",
    "        cnv_softmax=res_cnv_full-cnv_max_val.reshape(state.shape[0],1).expand(-1,2*chrom_width)\n",
    "        cnv_softmax=torch.exp(cnv_softmax).sum(1)\n",
    "        x=x+cnv_max_val+torch.log(cnv_softmax)\n",
    "        \n",
    "        #Q_{phi_3}(s,c,sp,ep)\n",
    "        res_end=self.End.forward(chrom,chrom_new,sigma)\n",
    "        #if there is originally a break point for end\n",
    "        #and if this is after the starting point\n",
    "        #real world constraint in section 3.3.2\n",
    "        break_end=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:(chrom_width-1)]=chrom[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        for i in range(state.shape[0]):\n",
    "            #can't end before starting point\n",
    "            break_end[i,:int(start_loci[i])]=0*break_end[i,:int(start_loci[i])]\n",
    "            #don't allow lose one copy when copy number equalls 1\n",
    "            if(cnv[i]<0.5):\n",
    "                j=int(start_loci[i])+1\n",
    "                while(j<chrom_width):\n",
    "                    if(chrom[i][j]<1.5):\n",
    "                        break\n",
    "                    j=j+1\n",
    "                break_end[i,j:chrom_width]=0*break_end[i,j:chrom_width]\n",
    "            \n",
    "        res_end_full=torch.zeros(state.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        \n",
    "        #real world constraint described in section 3.3.2\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max_temp=torch.max(res_end_full,1)\n",
    "        end_softmax=res_end_full-end_max_val.reshape(state.shape[0],1).expand(-1,chrom_width)\n",
    "        end_softmax=torch.exp(end_softmax).sum(1)\n",
    "        #Q_{phi_3}(s,c,sp,ep)-softmax(Q_{phi_3}(s,c,sp,ep)) as described in section 3.3.1\n",
    "        x=x+end_max_val+torch.log(end_softmax)\n",
    "        \n",
    "        for i in range(state.shape[0]):\n",
    "            if valid[i]>0.5:#check validity to prevent inf-inf which ends in nan\n",
    "                x[i]=x[i]-res_chrom[i][int(Chr[i])]\n",
    "                cnv_rank=int(start_loci[i]*2+cnv[i])\n",
    "                x[i]=x[i]-res_cnv_full[i][cnv_rank]\n",
    "                end_rank=int(end_loci[i]-1)\n",
    "                x[i]=x[i]-res_end_full[i][end_rank]\n",
    "        \n",
    "        #remove training data which include invalid actions\n",
    "        x=x*valid\n",
    "        #return avdantage as well as a best cnv and sigma used for generating training data\n",
    "        #used for training in the next step\n",
    "        return x,cnv_max,sigma,res_chrom,res_cnv_full,res_end_full\n",
    "     \n",
    "    def Softmax(self,next_state,sigma):\n",
    "        #compute softmax_{a'} Q(s',a')\n",
    "        x=self.Chrom_model.forward(next_state,sigma)\n",
    "        max_chrom=torch.max(x,1)[0]\n",
    "        softmax_chrom=x-max_chrom.reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
    "        softmax_chrom=torch.exp(softmax_chrom).sum(1)\n",
    "        #special action END\n",
    "        #all the remaining abnormal loci are treated to be caused by several independent single locus copy number changes\n",
    "        end_val=torch.sum(torch.abs(next_state-1),(1,2,3))*math.log(single_loci_loss)\n",
    "        max_chrom,softmax_chrom=Soft_update(max_chrom,softmax_chrom,end_val,torch.ones(x.shape[0]))\n",
    "        #if there is a WGD followed immediately\n",
    "        for i in range(x.shape[0]):\n",
    "            #real world constraint as described in section 3.3.2\n",
    "            #do not allow (reversing) WGD when the CNP contain odd numbers for some loci\n",
    "            if (not torch.any(next_state[i]-2*torch.floor(next_state[i]/2)>0.5)) and torch.any(next_state[i]>0.5):\n",
    "                sigma_wgd=self.switch(torch.floor(next_state[i:(i+1)]/2))\n",
    "                sigma_wgd=sigma_wgd.detach()\n",
    "                wgd_val,wgd_soft=self.Softmax(torch.floor(next_state[i:(i+1)]/2),sigma_wgd)\n",
    "                max_chrom[i],softmax_chrom[i]=Soft_update(torch.ones(1)*max_chrom[i],torch.ones(1)*softmax_chrom[i],torch.ones(1)*wgd_val,torch.ones(1)*wgd_soft)\n",
    "        \n",
    "        return max_chrom,softmax_chrom\n",
    "  \n",
    "\n",
    "#Minimum example\n",
    "if __name__ == \"__main__\":\n",
    "    #test different parts separately\n",
    "    '''\n",
    "    switch=WGD_Net()\n",
    "    Chrom_model=Chrom_NN()\n",
    "    print(Chrom_model)\n",
    "    #test the structure of permutation invariant structure\n",
    "    x=torch.ones(3,1,num_chromosome,50)\n",
    "    x[0][0][0][0:50]=2\n",
    "    x[2][0][1][0:50]=2\n",
    "    prob=switch.forward(x)\n",
    "    print(prob)\n",
    "    res=Chrom_model.forward(x,prob)\n",
    "    print(res)\n",
    "    res=-float('inf')\n",
    "    res=torch.LongTensor(3)\n",
    "    \n",
    "    print(torch.log(res.type(torch.DoubleTensor)))\n",
    "    #CNV\n",
    "    CNV=CNV_NN()\n",
    "    res=CNV.forward(x[:,0,0,0:50],prob)\n",
    "    print(CNV)\n",
    "    print(res.shape)\n",
    "    #END\n",
    "    End=End_Point_NN()\n",
    "    res=End.forward(x[:,0,0,0:50],x[:,0,0,0:50]+1,prob)\n",
    "    print(End)\n",
    "    print(res.shape)\n",
    "    '''\n",
    "    #test Q-learning\n",
    "    x=torch.ones(3,1,num_chromosome,50)\n",
    "    y=torch.ones(3,1,num_chromosome,50)\n",
    "    x[0][0][0][0:50]=2\n",
    "    x[2][0][1][0:50]=2\n",
    "    chrom=x[:,0,0,:]\n",
    "    chrom_new=y[:,0,0,:]\n",
    "    Chr=torch.zeros(3)\n",
    "    cnv=torch.ones(3)\n",
    "    start_loci=torch.zeros(3)\n",
    "    end_loci=torch.ones(3)*50\n",
    "    valid=torch.ones(3)\n",
    "    Q_model=Q_learning()\n",
    "    res,cnv_max,sigma,t,t2,t3=Q_model.forward(x,y,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid)\n",
    "    print(res)\n",
    "    print(cnv_max)\n",
    "    loss=res.pow(2).mean()\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    params = list(Q_model.parameters())\n",
    "    print(params[0].grad[0])\n",
    "    print(Q_model.switch.conv1.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train_data.py\n",
    "import torch\n",
    "import math\n",
    "\n",
    "batch_size=15\n",
    "#\n",
    "#during training\n",
    "#data are simulated backwards\n",
    "#when step==0, it means it is the last step for the trajectory\n",
    "#and step++ to make CNP more complex\n",
    "def Simulate_train_data(first_step_flag=True,state=None,next_state=None,advantage=None,Chr=None,step=None,wgd=None,valid=None):\n",
    "    #Simulate data for training (similar to the case when a machine is playing a game against itself)\n",
    "    #Thus, we don't need real world data during training, as long as the reward is similar to the real world probability\n",
    "    #As in theorem 1, there is no specific destribution required to compute expectation over (s,a) pairs\n",
    "    #Any distribution with broad support over all (s,a) will do the job\n",
    "    if first_step_flag:\n",
    "        #The first simulated sample\n",
    "        state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "        next_state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "        Chr=torch.ones(batch_size,requires_grad=False).type(torch.LongTensor)\n",
    "        step=torch.zeros(batch_size,requires_grad=False)\n",
    "        advantage=torch.zeros(batch_size)\n",
    "        wgd=torch.zeros(batch_size,requires_grad=False)\n",
    "        valid=torch.ones(batch_size,requires_grad=False)\n",
    "    \n",
    "    #sample starting point, end point, gain or loss  \n",
    "    #because of the permutation invariant structure in section 3.3.3\n",
    "    #it is not necessary to resample the chromosome everytime\n",
    "    start_loci=torch.randint(high=chrom_width,size=(batch_size,),requires_grad=False)\n",
    "    end_loci=torch.LongTensor(batch_size)\n",
    "    cnv=torch.ones(batch_size,requires_grad=False)\n",
    "    chrom=torch.Tensor(batch_size,chrom_width)\n",
    "    chrom_new=torch.Tensor(batch_size,chrom_width)\n",
    "    #probability of resetting the training trajectory back to step=0\n",
    "    step_prob=0.15+0.8/(1+math.exp(-1e-3*counter_global+2))\n",
    "    for i in range(batch_size):\n",
    "        #if the model is poorly trained until the current step\n",
    "        #go back to the state 0\n",
    "        #to ensure small error for short trajectories\n",
    "        if(torch.rand(1)[0]>step_prob or torch.abs(advantage[i])>=15):\n",
    "            state[i]=torch.ones(1,num_chromosome,chrom_width,requires_grad=False)\n",
    "            next_state[i]=torch.ones(1,num_chromosome,chrom_width,requires_grad=False)\n",
    "            step[i]=0\n",
    "        #if model is fully trained for the current step\n",
    "        #and there is no invalid operations been sampled\n",
    "        #go to next step\n",
    "        elif(valid[i]>0 and torch.abs(advantage[i])<7):\n",
    "            next_state[i]=state[i].clone()\n",
    "            step[i]=step[i]+1\n",
    "        #stay to further train the current step\n",
    "        #or resample another action\n",
    "        else:\n",
    "            state[i]=next_state[i].clone()\n",
    "    \n",
    "        #reset advantage and valid after they have been checked\n",
    "        advantage[i]=0\n",
    "        valid[i]=1\n",
    "        end_loci[i]=1+torch.randint(low=start_loci[i],high=50,size=(1,))[0]\n",
    "        #change the chromosone that CNV is on with some probability\n",
    "        #otherwise, all CNV will be on the same chromosome\n",
    "        if torch.rand(1)[0]>0.3:\n",
    "            Chr[i]=torch.randint(high=num_chromosome,size=(1,))[0]\n",
    "        #adding probability to sample whole chromosomal changes during training\n",
    "        if torch.rand(1)[0]>0.8:\n",
    "            start_loci[i]=0\n",
    "            end_loci[i]=chrom_width\n",
    "        #adding probability to sample losses starting from the start of chromosome\n",
    "        if torch.rand(1)[0]>0.3:\n",
    "            cnv[i]=0\n",
    "        #increasing the probability to sample WGD during training\n",
    "        prob_wgd=0.4/(1+math.exp(-step[i]+5))\n",
    "        #starting to modify state and next state\n",
    "        #extract preprocessing data\n",
    "        #wgd          \n",
    "        if (torch.abs(advantage[i])<2*chrom_width and torch.rand(1)[0]<prob_wgd and wgd[i]<1):\n",
    "            wgd[i]=1\n",
    "            state[i]=state[i]*2\n",
    "            next_state[i]=next_state[i]*2\n",
    "        #adding cnv effect\n",
    "        #increasing copies when no wgd\n",
    "        #decreasing copies when wgd\n",
    "        if wgd[i]>0.5:\n",
    "            cnv[i]=1-cnv[i]\n",
    "        state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]=state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]-(cnv[i]-0.5)*2\n",
    "        chrom[i]=state[i][0][Chr[i]][:]\n",
    "        #reverse effect on chrom_new\n",
    "        chrom_new[i]=state[i][0][Chr[i]][:]\n",
    "        chrom_new[i][(start_loci[i]):]=chrom_new[i][(start_loci[i]):]+(cnv[i]-0.5)*2\n",
    "        #not going to negative values\n",
    "        if(torch.any(state[i][0][Chr[i]][(start_loci[i])]< -0.5)):\n",
    "            valid[i]=0\n",
    "        #not joining breakpoints\n",
    "        if(start_loci[i]>0.5 and torch.abs(chrom[i][start_loci[i]]-chrom[i][start_loci[i]-1])<0.5):\n",
    "            valid[i]=0\n",
    "        if(end_loci[i]<chrom_width-0.5 and torch.abs(chrom[i][end_loci[i]-1]-chrom[i][end_loci[i]])<0.5):\n",
    "            valid[i]=0\n",
    "        if cnv[i]>0.5 and (torch.any(state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]< 0.5)):\n",
    "            valid[i]=0\n",
    "    return state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,wgd,step,advantage,valid\n",
    "\n",
    "def Modify_data(state,chrom,Chr,valid,cnv_max,model,sigma):\n",
    "    #Modify the training data to train the Q values for the best action\n",
    "    #place takers\n",
    "    #make sure they are of correct tensor types\n",
    "    #make sure they are meaningful values to avoid inf if they are not valid samples\n",
    "    #otherwise nan may be generated\n",
    "    start_loci=torch.randint(high=chrom_width,size=(batch_size,),requires_grad=False)\n",
    "    end_loci=start_loci.clone()\n",
    "    cnv=torch.ones(batch_size,requires_grad=False)\n",
    "    next_state=state.clone()\n",
    "    chrom_new=chrom.clone()\n",
    "    advantage=torch.zeros(batch_size)\n",
    "    for i in range(batch_size):\n",
    "        #only deal with valid samples\n",
    "        if valid[i]>0.5:\n",
    "            start_loci[i]=cnv_max[i]//2\n",
    "            cnv[i]=cnv_max[i]-start_loci[i]*2\n",
    "            #update chrom_new\n",
    "            chrom_new[i][(start_loci[i]):]=chrom_new[i][(start_loci[i]):]+(cnv[i]-0.5)*2\n",
    "    \n",
    "    end_loci=model.find_end(chrom,chrom_new,sigma,start_loci,cnv,valid)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        next_state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]=next_state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]+(cnv[i]-0.5)*2\n",
    "      \n",
    "      \n",
    "    return state,next_state,chrom,chrom_new,cnv,start_loci,end_loci,advantage\n",
    "\n",
    "#simulate data for testing\n",
    "def Simulate_data(batch_size=15,Number_of_step=70):\n",
    "    state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "    next_state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "    Chr=torch.ones(batch_size,requires_grad=False).type(torch.LongTensor)\n",
    "    step=torch.zeros(batch_size,requires_grad=False)\n",
    "    wgd=torch.zeros(batch_size,requires_grad=False)\n",
    "    valid=torch.ones(batch_size,requires_grad=False)\n",
    "    \n",
    "    start_loci=torch.randint(high=chrom_width,size=(batch_size,),requires_grad=False)\n",
    "    end_loci=torch.LongTensor(batch_size)\n",
    "    cnv=torch.ones(batch_size,requires_grad=False)\n",
    "    chrom=torch.Tensor(batch_size,chrom_width)\n",
    "    chrom_new=torch.Tensor(batch_size,chrom_width)\n",
    "    \n",
    "    step_counter=0\n",
    "    while(step_counter<Number_of_step):\n",
    "        for i in range(batch_size):\n",
    "            #reset valid after they have been checked\n",
    "            valid[i]=1\n",
    "            end_loci[i]=1+torch.randint(low=start_loci[i],high=50,size=(1,))[0]\n",
    "            #change the chromosone that CNV is on with some probability\n",
    "            if torch.rand(1)[0]>0.5:\n",
    "                Chr[i]=torch.randint(high=num_chromosome,size=(1,))[0]\n",
    "            #adding probability to sample chromosomal changes during training\n",
    "            if torch.rand(1)[0]>0.8:\n",
    "                start_loci[i]=0\n",
    "                end_loci[i]=chrom_width\n",
    "            #cnv\n",
    "            if torch.rand(1)[0]>0.7:\n",
    "                cnv[i]=0\n",
    "            #modifying cnp\n",
    "            prob_wgd=0.4/(1+math.exp(-step[i]+5))\n",
    "            #wgd          \n",
    "            if (torch.rand(1)[0]<prob_wgd and wgd[i]<1):\n",
    "                wgd[i]=1\n",
    "                state[i]=state[i]*2\n",
    "                next_state[i]=next_state[i]*2\n",
    "                #adding cnv effect\n",
    "                #increasing copies when no wgd\n",
    "                #decreasing copies when wgd\n",
    "            if wgd[i]>0.5:\n",
    "                cnv[i]=1-cnv[i]\n",
    "            state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]=state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]-(cnv[i]-0.5)*2\n",
    "            chrom[i]=state[i][0][Chr[i]][:]\n",
    "            #reverse effect on chrom_new\n",
    "            chrom_new[i]=state[i][0][Chr[i]][:]\n",
    "            chrom_new[i][(start_loci[i]):]=chrom_new[i][(start_loci[i]):]+(cnv[i]-0.5)*2\n",
    "            #not going to negative values\n",
    "            if(torch.any(state[i][0][Chr[i]][(start_loci[i])]< -0.5)):\n",
    "                valid[i]=0\n",
    "            #not joining breakpoints\n",
    "            if(start_loci[i]>0.5 and torch.abs(chrom[i][start_loci[i]]-chrom[i][start_loci[i]-1])<0.5):\n",
    "                valid[i]=0\n",
    "            if(end_loci[i]<chrom_width-0.5 and torch.abs(chrom[i][end_loci[i]-1]-chrom[i][end_loci[i]])<0.5):\n",
    "                valid[i]=0\n",
    "            if valid[i]>0 :\n",
    "                next_state[i]=state[i].clone()\n",
    "                step[i]=step[i]+1\n",
    "            #stay to further train the current step\n",
    "            #or resample another action\n",
    "            else:\n",
    "                state[i]=next_state[i].clone()\n",
    "        step_counter=step_counter+1\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(79.4469, grad_fn=<MeanBackward0>) tensor(0.)\n",
      "tensor(34.3455, grad_fn=<MeanBackward0>) tensor(0.2000)\n",
      "tensor(15.8507, grad_fn=<MeanBackward0>) tensor(0.2000)\n",
      "tensor(10.3301, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(28.3277, grad_fn=<MeanBackward0>) tensor(0.3333)\n",
      "tensor(7.8980, grad_fn=<MeanBackward0>) tensor(0.1333)\n",
      "tensor(7.9488, grad_fn=<MeanBackward0>) tensor(0.1333)\n",
      "tensor(39.5219, grad_fn=<MeanBackward0>) tensor(0.2000)\n",
      "tensor(10.4677, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(19.5669, grad_fn=<MeanBackward0>) tensor(0.1333)\n",
      "tensor(23.2042, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(29.1963, grad_fn=<MeanBackward0>) tensor(0.3333)\n",
      "tensor(5.5911, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(22.3558, grad_fn=<MeanBackward0>) tensor(0.1333)\n",
      "tensor(1.2978, grad_fn=<MeanBackward0>) tensor(0.)\n",
      "tensor(18.3575, grad_fn=<MeanBackward0>) tensor(0.1333)\n",
      "tensor(34.0939, grad_fn=<MeanBackward0>) tensor(0.3333)\n",
      "tensor(5.8567, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(11.3406, grad_fn=<MeanBackward0>) tensor(0.1333)\n",
      "tensor(8.6092, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(0.6449, grad_fn=<MeanBackward0>) tensor(0.)\n",
      "tensor(11.4591, grad_fn=<MeanBackward0>) tensor(0.2000)\n",
      "tensor(19.6238, grad_fn=<MeanBackward0>) tensor(0.2667)\n",
      "tensor(8.2219, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(34.3365, grad_fn=<MeanBackward0>) tensor(0.2000)\n",
      "tensor(27.7492, grad_fn=<MeanBackward0>) tensor(0.3333)\n",
      "tensor(0.8732, grad_fn=<MeanBackward0>) tensor(0.)\n",
      "tensor(22.1173, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(2.7328, grad_fn=<MeanBackward0>) tensor(0.)\n",
      "tensor(0.6262, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(2.0730, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(28.3145, grad_fn=<MeanBackward0>) tensor(0.2667)\n",
      "tensor(36.8779, grad_fn=<MeanBackward0>) tensor(0.3333)\n",
      "tensor(10.7594, grad_fn=<MeanBackward0>) tensor(0.1333)\n",
      "tensor(0.6350, grad_fn=<MeanBackward0>) tensor(0.1333)\n",
      "tensor(55.7142, grad_fn=<MeanBackward0>) tensor(0.4000)\n",
      "tensor(16.1604, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(16.5206, grad_fn=<MeanBackward0>) tensor(0.2000)\n",
      "tensor(24.3921, grad_fn=<MeanBackward0>) tensor(0.2000)\n",
      "tensor(10.0733, grad_fn=<MeanBackward0>) tensor(0.2667)\n",
      "tensor(42.0205, grad_fn=<MeanBackward0>) tensor(0.3333)\n",
      "tensor(9.8197, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(10.6555, grad_fn=<MeanBackward0>) tensor(0.2000)\n",
      "tensor(7.8134, grad_fn=<MeanBackward0>) tensor(0.1333)\n",
      "tensor(3.1927, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(0.3302, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(0.0559, grad_fn=<MeanBackward0>) tensor(0.3333)\n",
      "tensor(23.1686, grad_fn=<MeanBackward0>) tensor(0.2667)\n",
      "tensor(21.1144, grad_fn=<MeanBackward0>) tensor(0.1333)\n",
      "tensor(16.0561, grad_fn=<MeanBackward0>) tensor(0.2000)\n",
      "tensor(22.5687, grad_fn=<MeanBackward0>) tensor(0.2667)\n",
      "tensor(8.4016, grad_fn=<MeanBackward0>) tensor(0.0667)\n",
      "tensor(11.7493, grad_fn=<MeanBackward0>) tensor(0.0667)\n"
     ]
    }
   ],
   "source": [
    "#main.py\n",
    "import torch\n",
    "import math\n",
    "import torch.optim as optim\n",
    "#import Policy\n",
    "#import Data_train\n",
    "\n",
    "#setting up counter\n",
    "counter_global=0\n",
    "#Model\n",
    "Q_model=Q_learning()\n",
    "#Load initial data\n",
    "state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,wgd,step,advantage,valid=Simulate_train_data()\n",
    "#setting up optimizer\n",
    "optimizer = optim.Adam(Q_model.parameters(), lr=1e-3,betas=(0.9, 0.99), eps=1e-08, weight_decay=1e-6)\n",
    "\n",
    "\n",
    "#start training\n",
    "while(counter_global< 3e8):\n",
    "    counter_global=counter_global+1\n",
    "    #load data\n",
    "    state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,wgd,step,advantage,valid=Simulate_train_data(False,state,next_state,advantage,Chr,step,wgd,valid)\n",
    "    #compute advantage\n",
    "    optimizer.zero_grad()\n",
    "    advantage,cnv_max,sigma,temp,t2,t3=Q_model.forward(state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid)\n",
    "    #compute loss\n",
    "    loss=advantage.pow(2).mean()\n",
    "    #train the model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(loss)\n",
    "    \n",
    "    #training with the best action\n",
    "    #temp for the values both used in training and loading new data\n",
    "    state_temp,next_state_temp,chrom,chrom_new,cnv,start_loci,end_loci,advantage_temp=Modify_data(state,chrom,Chr,valid,cnv_max,Q_model.End,sigma)\n",
    "    #compute advantage\n",
    "    optimizer.zero_grad()\n",
    "    advantage,cnv_max,sigma,temp,temp2,temp3=Q_model.forward(state_temp,next_state_temp,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid)\n",
    "    #compute loss\n",
    "    loss=advantage.pow(2).mean()\n",
    "    #print(loss)\n",
    "    if(counter_global%10==0):\n",
    "        print(loss,step.mean())\n",
    "        #torch.save(Q_model.state_dict(), PATH)\n",
    "        #print(temp[0])\n",
    "        #train the model\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deconvolution.py\n",
    "#used for deconvolution of CNP history\n",
    "\n",
    "def Deconvolute(model,cnp,Chr,CNV,End):\n",
    "    '''\n",
    "    Deconvolution samples the maximum action in a greedy way\n",
    "    model:the trained Q-learning model\n",
    "    cnp: the input CNP, shape: Number of CNP,1,44 (#Chr),50 (#regions for one chromosome)\n",
    "    Chr,CNV,END: output tensor,shape: Number of CNP, maximum length of history\n",
    "    output: for Chr: -1 indicates WGD, 0 indicates no action, 1~44 the chromosome\n",
    "            for CNV: only valid if Chr is not -1 or 0\n",
    "                     indicates the starting point (CNV//2) and the type of CNV (CNV%2==1 for gain and CNV%2==0 for loss)\n",
    "            for End: only valid if Chr is not -1 or 0\n",
    "                     indicates the end point for a CNV.\n",
    "    '''\n",
    "    max_step=int(Chr.shape[1])\n",
    "    for i in range(cnp.shape[0]):\n",
    "        current_cnp=cnp[i:(i+1)]\n",
    "        step=0\n",
    "        while(step<max_step):\n",
    "            #it is also possible to manually set the switch if deemed necessary\n",
    "            sigma=model.switch(current_cnp)\n",
    "            res_chrom=model.Chrom_model(current_cnp,sigma)\n",
    "            #find the chromosome with the maximum probability\n",
    "            val,(temp_Chr)=res_chrom.max(1)\n",
    "            temp_Chr=int(temp_Chr)\n",
    "            Chr[i][step]=temp_Chr+1\n",
    "            #WGD\n",
    "            if (not torch.any(current_cnp-2*torch.floor(current_cnp/2)>0.5)) and torch.any(current_cnp>0.5):\n",
    "                sigma_wgd=model.switch(torch.floor(current_cnp/2))\n",
    "                res_chrom_wgd=model.Chrom_model(torch.floor(current_cnp/2),sigma_wgd)\n",
    "                val_wgd,temp=res_chrom_wgd.max(1)\n",
    "                if val_wgd>=val:\n",
    "                    val=val_wgd\n",
    "                    Chr[i][step]=-1\n",
    "            #special action END\n",
    "            val_end=torch.sum(torch.abs(current_cnp-1))*math.log(single_loci_loss)\n",
    "            if val_end>=val:\n",
    "                val=val_end\n",
    "                Chr[i][step]=0\n",
    "                break\n",
    "            #if WGD\n",
    "            if Chr[i][step]< -0.5:\n",
    "                current_cnp=torch.floor(current_cnp/2)\n",
    "            #if not WGD or END\n",
    "            elif Chr[i][step]>0.5:\n",
    "                #find best CNV\n",
    "                chrom=current_cnp[:,0,temp_Chr,:]\n",
    "                CNV[i][step]=model.CNV.find_one_cnv(chrom,sigma)\n",
    "                cnv_temp=int(CNV[i][step]%2)\n",
    "                start_temp=int(CNV[i][step]//2)\n",
    "                #find best End\n",
    "                chrom_new=chrom.clone()\n",
    "                chrom_new[:,start_temp:]=chrom_new[:,start_temp:]+(cnv_temp-0.5)*2\n",
    "                End[i][step]=model.End.find_one_end(chrom,chrom_new,sigma,start_temp,cnv_temp)\n",
    "                #updata cnp\n",
    "                current_cnp[:,0,temp_Chr,start_temp:int(End[i][step])]=current_cnp[:,0,temp_Chr,start_temp:int(End[i][step])]+(cnv_temp-0.5)*2\n",
    "            step=step+1\n",
    "    return Chr,CNV,End\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    model = Q_learning()\n",
    "    model=Q_model\n",
    "    #model.load_state_dict(torch.load(PATH))\n",
    "    model.eval()\n",
    "    counter_global=torch.randint(10000,(1,))[0]\n",
    "    #loading simulated data\n",
    "    state=Simulate_data(batch_size=3,Number_of_step=2)\n",
    "    print(state[0])\n",
    "    Chr=torch.zeros(state.shape[0],2)\n",
    "    CNV=torch.zeros(state.shape[0],2)\n",
    "    End=torch.zeros(state.shape[0],2)\n",
    "    state_copy=state.clone()\n",
    "    #deconvolution\n",
    "    Chr,CNV,End=Deconvolute(Q_model,state,Chr,CNV,End)\n",
    "    print(Chr)\n",
    "    #check if the model picks the correct chromosome\n",
    "    print(state_copy[2][0][int(Chr[2,0]-1)])\n",
    "    print(CNV)\n",
    "    print(End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLevolution",
   "language": "python",
   "name": "rlevolution"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
