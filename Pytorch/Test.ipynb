{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reward.py\n",
    "#Define the reward function r(s,a) when a is not a special action END\n",
    "\n",
    "#For each chromosome, we consider 50 SNP loci\n",
    "chrom_width=50;\n",
    "#44 normal chromosomes in human genome\n",
    "num_chromosome=44\n",
    "\n",
    "\n",
    "#normalisation constant to make normal_const*(\\sum_i a_i) <1, so that the possibility of all CNV sum to a real value smaller than 1\n",
    "normal_const=5e-5;\n",
    "#probability of single locus gain/loss\n",
    "single_loci_loss=normal_const*(1-2e-1);\n",
    "#probability of WGD\n",
    "WGD=normal_const*0.6;\n",
    "\n",
    "#log probability of CNV\n",
    "#used for calculating the distribution of p(a) when a is a focal CNV\n",
    "const1=normal_const*(1-1e-1);\n",
    "const2=2;\n",
    "\n",
    "#Whole chromosome change probability\n",
    "Whole_Chromosome_CNV=normal_const*0.99/10;\n",
    "Half_Chromosome_CNV=normal_const*0.99/15;\n",
    "\n",
    "max_copy=20\n",
    "\n",
    "def Reward(Start,End):\n",
    "    Start=Start.to(torch.float32)\n",
    "    End=End.to(torch.float32)\n",
    "    reward=torch.log(const1/(const2+torch.log(End-Start)))\n",
    "    #chromosome changes\n",
    "    for i in range(Start.shape[0]):\n",
    "        #full chromosome\n",
    "        if End[i]-Start[i]>chrom_width-0.5:\n",
    "            reward[i]=math.log(Whole_Chromosome_CNV)\n",
    "        #arm level changes\n",
    "        if chrom_width-End[i]<0.5 and abs(chrom_width//2-Start[i])<1.5:\n",
    "            reward[i]=math.log(Half_Chromosome_CNV)\n",
    "        if Start[i]<0.5 and abs(chrom_width//2-End[i])<1.5:\n",
    "            reward[i]=math.log(Half_Chromosome_CNV)\n",
    "    return reward\n",
    "\n",
    "\n",
    "\n",
    "#Q-function.py\n",
    "#defining the Q-function \n",
    "#Q(s,a) in manuscript\n",
    "\n",
    "\n",
    "#switch structure mentioned in section 3.3.4\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_switch = [20,40,80]\n",
    "activatiion_wgd=torch.tanh\n",
    "class WGD_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WGD_Net, self).__init__()\n",
    "        #chromosome permutation invariant structure as described in section 3.3.3\n",
    "        #slide for chromosome is 1 and the filter length in this dimension is also 1\n",
    "        #thus, the same filter goes through all chromosomes in the same fashion\n",
    "        self.conv1=nn.Conv2d(1, nkernels_switch[0], (1,3),(1,1),(0,1))\n",
    "        self.conv2=nn.Conv2d(nkernels_switch[0],nkernels_switch[1] , (1,3),(1,1), (0,1))\n",
    "        self.conv3=nn.Conv2d(nkernels_switch[1],nkernels_switch[2] , (1,5),(1,1), (0,0))\n",
    "        self.linear=nn.Linear(nkernels_switch[2],1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #y=torch.clamp((F.relu(x.mean(3)-1)-0.5+0.5*F.relu(1-x.mean(3))),-1,1).sum((1,2))\n",
    "        y=20*(x.mean((1,2,3))-1.5)\n",
    "        y=y.reshape(x.shape[0],1).detach()\n",
    "        x=x.reshape(x.shape[0],1,num_chromosome,chrom_width)\n",
    "        x=F.max_pool2d(activatiion_wgd(self.conv1(x)),(1,5),(1,5),(0,0))\n",
    "        x=F.max_pool2d(activatiion_wgd(self.conv2(x)),(1,2),(1,2),(0,0))\n",
    "        x=(activatiion_wgd(self.conv3(x))).sum((2,3))\n",
    "        x=self.linear(x)\n",
    "        x=x/2\n",
    "        #residule representation in x as described in section 3.3.4\n",
    "        x=y/2+x\n",
    "        x=torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "#chromosome evaluation net \n",
    "#Used in Chrom_NN (which is Q_{phi_1}(s,c) in section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_chr = [80,120,160]\n",
    "activation_cnp=torch.tanh\n",
    "class CNP_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNP_Val, self).__init__()\n",
    "        self.conv1=nn.Conv2d(1, nkernels_chr[0], (1,5),(1,1),(0,2))\n",
    "        self.conv2=nn.Conv2d(nkernels_chr[0],nkernels_chr[1] , (1,3),(1,1), (0,1))\n",
    "        self.conv3=nn.Conv2d(nkernels_chr[1],nkernels_chr[2] , (1,3),(1,1), (0,1))\n",
    "        self.conv4=nn.Conv2d(nkernels_chr[2],1, (1,5),(1,1), (0,0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=F.max_pool2d(activation_cnp(self.conv1(x)),(1,3),(1,3),(0,1))\n",
    "        x=F.max_pool2d(activation_cnp(self.conv2(x)),(1,2),(1,2),(0,1))\n",
    "        x=F.max_pool2d(activation_cnp(self.conv3(x)),(1,2),(1,2),(0,1))\n",
    "        #KL divergence is always nonpositive\n",
    "        x=0.25+F.elu(self.conv4(x),0.25)\n",
    "        #number of sample * 44 chromosomes\n",
    "        x=x.reshape(x.shape[0],num_chromosome)\n",
    "        return x\n",
    "\n",
    "#Implemts Q_{phi_1}(s,c) in section 3.3.1\n",
    "#It combines two chromosome evaluation nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_1}(s,c)\n",
    "class Chrom_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Chrom_NN,self).__init__()\n",
    "        #two parts of the Chrom_NN\n",
    "        #NN for CNP without WGD \n",
    "        self.Val_noWGD=CNP_Val()\n",
    "        #NN for CNP with WGD\n",
    "        self.Val_WGD=CNP_Val()\n",
    "    \n",
    "    def forward(self,x,sigma):\n",
    "        #probability for WGD, which is computed by switch structure\n",
    "        sigma=sigma.expand(-1,num_chromosome)\n",
    "        #we assume the copy number for each loci ranges from 0~9\n",
    "        #for samples without WGD\n",
    "        \n",
    "        #y represents if a chromosome have abnormal copy numbers (positions with copy number other than 1)\n",
    "        y=torch.ceil((torch.abs(x-1)).mean(3)/max_copy)\n",
    "        y=y.reshape(x.shape[0],num_chromosome)\n",
    "        y=y.detach()\n",
    "        #Residule representation mentioned in section 3.3.4\n",
    "        #the value for Q_{phi_1}(s,c) is computed as Val_no (the value estimated by the neural net, the residule part)+ y (the empirial estimation) \n",
    "        Val_no=self.Val_noWGD.forward(x)\n",
    "        #chromosome with all 1 copies don't need any CNV and thus will be less likely mutated.\n",
    "        Val_no=-(1-y)*math.log(single_loci_loss)*2+((y*Val_no).sum(1)).reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
    "        #for samples with WGD\n",
    "        #it is similar to the previsou part, where z is an equivalent for y and Val_wgd is an equivalent for Val_no\n",
    "        z=torch.ceil((torch.abs(x-2*(x//2))).mean(3)/max_copy)\n",
    "        z=z.reshape(x.shape[0],num_chromosome)\n",
    "        z=z.detach()\n",
    "        Val_wgd=self.Val_WGD.forward(x)\n",
    "        Val_wgd=-(1-z)*math.log(single_loci_loss)*2+(z*Val_wgd).sum(1).reshape(x.shape[0],1).expand(-1,num_chromosome)-math.log(WGD)\n",
    "        \n",
    "        #combine two NN with switch as defined in Section 3.3.4\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        x=-x\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "#starting point and gain or loss (defined as sp in manuscript) \n",
    "#Used in CNV_NN (which is Q_{phi_2}(s,c,sp) on section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_CNV = [80,120,160,10]\n",
    "activation_cnv=torch.tanh\n",
    "class CNV_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNV_Val,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1, nkernels_CNV[0], (1,7),(1,1),(0,3))\n",
    "        self.conv2=nn.Conv2d(nkernels_CNV[0],nkernels_CNV[1] , (1,7),(1,1), (0,3))\n",
    "        self.conv3=nn.Conv2d(nkernels_CNV[1],nkernels_CNV[2] , (1,7),(1,1), (0,3))\n",
    "        self.conv4=nn.Conv2d(nkernels_CNV[2], nkernels_CNV[3], (1,7),(1,1), (0,3))\n",
    "        self.linear=nn.Linear(nkernels_CNV[3]*chrom_width,2*chrom_width-1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=activation_cnv(self.conv1(x))\n",
    "        x=activation_cnv(self.conv2(x))\n",
    "        x=activation_cnv(self.conv3(x))\n",
    "        x=activation_cnv(self.conv4(x))\n",
    "        x=x.reshape(x.shape[0],nkernels_CNV[3]*chrom_width)\n",
    "        x=self.linear(x)\n",
    "        #number of samples* [(50 regions)*(2(gain or loss))-1] \n",
    "        #Only have 50*2-1=99 output dimensions because we fix the average these output\n",
    "        #The average of them could be arbitrary because of the partitioning\n",
    "        return x\n",
    "\n",
    "#Implemts Q_{phi_2}(s,c,sp) in section 3.3.1\n",
    "#It combines two CNV_Val nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_2}(s,c,sp)\n",
    "class CNV_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNV_NN,self).__init__()\n",
    "        #two network setting\n",
    "        self.CNV_noWGD=CNV_Val()\n",
    "        self.CNV_WGD=CNV_Val()\n",
    "    \n",
    "    def forward(self,x,sigma):\n",
    "        #as in section 3.3.4\n",
    "        #y is the empirical estimation\n",
    "        #Val_no is the redidule representation\n",
    "        y=torch.Tensor(x.shape[0],chrom_width,2)\n",
    "        y[:,:,0]=F.relu(x-1)\n",
    "        y[:,:,1]=F.relu(1-x)\n",
    "        y=y.reshape(x.shape[0],2*chrom_width)\n",
    "        y=y[:,1:(2*chrom_width)]-y[:,0:1].expand(-1,2*chrom_width-1)\n",
    "        y=-y.detach()*math.log(single_loci_loss)\n",
    "        Val_no=self.CNV_noWGD.forward(x.reshape(x.shape[0],1,1,chrom_width))\n",
    "        Val_no=y+Val_no\n",
    "        \n",
    "        z=((torch.abs(x-2*(x//2))).reshape(x.shape[0],chrom_width,1)).expand(-1,-1,2)\n",
    "        z=z.reshape(x.shape[0],2*chrom_width)\n",
    "        z=z[:,1:(2*chrom_width)]-z[:,0:1].expand(-1,2*chrom_width-1)\n",
    "        z=-z.detach()*math.log(single_loci_loss)\n",
    "        Val_wgd=self.CNV_WGD.forward(x.reshape(x.shape[0],1,1,chrom_width))\n",
    "        Val_wgd=z+Val_wgd\n",
    "        #switch\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        return(x)\n",
    "    \n",
    "     \n",
    "    def find_one_cnv(self,chrom,sigma):\n",
    "        #used for finding the cnv during deconvolution\n",
    "        #it is not used in training process\n",
    "        \n",
    "        res_cnv=self.forward(chrom,sigma)\n",
    "        #if there is originally a break point for start\n",
    "        #rule system \n",
    "        break_start=torch.zeros(chrom.shape[0],50,2,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(chrom.shape[0],50,requires_grad=False)\n",
    "        chrom_shift[:,1:]=chrom[:,:49]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_start[:,:,1]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_start[:,0,1]=1\n",
    "        #don't allow lose one copy when copy number equalls 1\n",
    "        break_start[:,:,0]=break_start[:,:,1]\n",
    "        break_start[:,:,0]=break_start[:,:,0]*torch.ceil((chrom/2-0.5)/max_copy)\n",
    "        break_start=break_start.reshape(chrom.shape[0],100)\n",
    "        res_cnv_full=torch.zeros(chrom.shape[0],100)\n",
    "        res_cnv_full[:,1:]=res_cnv\n",
    "        \n",
    "        #Prior_rule=break_start\n",
    "        res_cnv_full=res_cnv_full+torch.log(break_start)\n",
    "        #best cnv according to the current Q\n",
    "        cnv_max_val,cnv_max=torch.max(res_cnv_full,1)\n",
    "        #print(res_cnv_full)\n",
    "        return int(cnv_max[0])\n",
    "    \n",
    "\n",
    "\n",
    "#end point\n",
    "#Used in End_Point_NN (which is Q_{phi_3}(s,c,sp,ep) on section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_End = [80,120,240]\n",
    "activation_end=torch.tanh\n",
    "class End_Point_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(End_Point_Val,self).__init__()\n",
    "        self.conv1=nn.Conv2d(2, nkernels_End[0], (1,7),(1,1),(0,3))\n",
    "        self.conv2=nn.Conv2d(nkernels_End[0],nkernels_End[1] , (1,7),(1,1), (0,3))\n",
    "        self.conv3=nn.Conv2d(nkernels_End[1],nkernels_End[2] , (1,7),(1,1), (0,3))\n",
    "        self.linear=nn.Linear(nkernels_End[2]*chrom_width,chrom_width-1)\n",
    "    \n",
    "    def forward(self,old,new):\n",
    "        x=torch.Tensor(old.shape[0],2,1,chrom_width)\n",
    "        x[:,0,0,:]=old\n",
    "        x[:,1,0,:]=new\n",
    "        x=x.detach()\n",
    "        x=activation_end(self.conv1(x))\n",
    "        x=activation_end(self.conv2(x))\n",
    "        x=activation_end(self.conv3(x))\n",
    "        x=x.reshape(x.shape[0],nkernels_End[2]*chrom_width)\n",
    "        x=self.linear(x)\n",
    "        #number of samples* [(chrom_width regions)-1] \n",
    "        #Only have chrom_width-1=49 output dimensions because we fix the average these output\n",
    "        #The average of them could be arbitrary because of the partitioning\n",
    "        return x\n",
    "    \n",
    "#Implemts Q_{phi_3}(s,c,sp,ep) in section 3.3.1\n",
    "#It combines two End_Point_Val nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_3}(s,c,sp,ep)\n",
    "class End_Point_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(End_Point_NN,self).__init__()\n",
    "        #two network setting\n",
    "        self.Val_noWGD=End_Point_Val()\n",
    "        self.Val_WGD=End_Point_Val()\n",
    "    \n",
    "    def forward(self,old,new,sigma):\n",
    "        \n",
    "        y=F.relu((old-1)*(old-new))\n",
    "        y=y[:,1:chrom_width]-y[:,0:1].expand(-1,chrom_width-1)\n",
    "        y=-y.detach()*math.log(single_loci_loss)\n",
    "        Val_no=self.Val_noWGD.forward(old,new)\n",
    "        Val_no=Val_no+y\n",
    "        \n",
    "        z=(old-2*(old//2))*(1-(new-2*(new//2)))\n",
    "        z=z[:,1:chrom_width]-z[:,0:1].expand(-1,chrom_width-1)\n",
    "        z=-z.detach()*math.log(single_loci_loss)\n",
    "        Val_wgd=self.Val_WGD.forward(old,new)\n",
    "        Val_wgd=Val_wgd+z\n",
    "        #switch\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def find_end(self,old,new,sigma,start_loci,cnv,valid):\n",
    "        #used for finding the end during loading data\n",
    "        res_end=self.forward(old,new,sigma)\n",
    "        \n",
    "        break_end=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:49]=old[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(old-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        \n",
    "        for i in range(old.shape[0]):\n",
    "            #can't end before starting point\n",
    "            break_end[i,:int(start_loci[i])]=0*break_end[i,:int(start_loci[i])]\n",
    "            #don't allow lose one copy when copy number equalls 1\n",
    "            if(cnv[i]<0.5):\n",
    "                j=int(start_loci[i])+1\n",
    "                while(j<chrom_width):\n",
    "                    if(old[i][j]<1.5):\n",
    "                        break\n",
    "                    j=j+1\n",
    "                break_end[i,j:chrom_width]=0*break_end[i,j:chrom_width]\n",
    "        res_end_full=torch.zeros(old.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        \n",
    "        #Prior_rule=break_end\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max=torch.max(res_end_full,1)\n",
    "        return end_max+1\n",
    "    \n",
    "    \n",
    "    def find_one_end(self,old,new,sigma,start,cnv):\n",
    "        #used for finding the end during deconvolution\n",
    "        res_end=self.forward(old,new,sigma)\n",
    "        \n",
    "        break_end=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:chrom_width-1]=old[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(old-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        #can't end before starting point\n",
    "        break_end[0,:start]=0*break_end[0,:start]\n",
    "        #don't allow lose one copy when copy number equalls 1\n",
    "        if(cnv<0.5):\n",
    "            j=start+1\n",
    "            while(j<chrom_width):\n",
    "                if(old[0][j]<1.5):\n",
    "                    break\n",
    "                j=j+1\n",
    "            break_end[0,j:chrom_width]=0*break_end[0,j:chrom_width]\n",
    "        res_end_full=torch.zeros(old.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        #Prior_rule=break_end\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max=torch.max(res_end_full,1)\n",
    "        end_max=int(end_max[0])\n",
    "        return end_max+1\n",
    "        \n",
    "\n",
    "#combine all separate networks\n",
    "#add Rule system\n",
    "\n",
    "#calculating the softmax\n",
    "#prevent inf when taking log(exp(x))\n",
    "#log_exp is always gonna be between 1 and the total number of elements\n",
    "def Soft_update(val1,soft1,val2,soft2):\n",
    "    bias=val1.clone()\n",
    "    log_exp=soft1.clone()\n",
    "    set1=[torch.ge(val1,val2)]\n",
    "    bias[set1]=val1[set1]\n",
    "    log_exp[set1]=soft1[set1]+soft2[set1]*torch.exp(val2[set1]-val1[set1])\n",
    "    set2=[torch.lt(val1,val2)]\n",
    "    bias[set2]=val2[set2]\n",
    "    log_exp[set2]=soft2[set2]+soft1[set2]*torch.exp(val1[set2]-val2[set2])\n",
    "    return bias,log_exp\n",
    "\n",
    "\n",
    "\n",
    "#Combine all the separate modules mentioned above\n",
    "#Implementation of Q(s,a)\n",
    "class Q_learning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_learning,self).__init__()\n",
    "        self.switch=WGD_Net()\n",
    "        #the output refer to Q_{\\phi_1}(s,c)\n",
    "        self.Chrom_model=Chrom_NN()\n",
    "        #the output refer to Q_{\\phi_2}(s,c,sp)\n",
    "        self.CNV=CNV_NN()\n",
    "        #the output refer to Q_{\\phi_3}(s,sp,c,ep)\n",
    "        self.End=End_Point_NN()\n",
    "    \n",
    "    \n",
    "    def forward(self,state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid):\n",
    "        '''\n",
    "        computing the final advantage(loss) used for training\n",
    "        loss in Thereom1\n",
    "        state: s in Q(s,a)\n",
    "        next_state: s' in softmaxQ(s',a')\n",
    "        Chr,cnv,end_loci: a in Q(s,a)\n",
    "        chrom,chrom_new,start_loci,end_loci: intermediate results from s,a, which is preprossed to make computation faster\n",
    "            e.g. chrom is CNP of the Chr(part of a) from the state(s)\n",
    "            They could be seen as a mapping without parameters to learn:f(s,a)\n",
    "        valid: a boolean array, indicating if a training sample is valid (e.g. have non negative copy numbers for all loci)\n",
    "        '''\n",
    "        \n",
    "        #computing softmaxQ(s',a')\n",
    "        #It is a tradition in RL that gradient does not backpropogate through softmaxQ(s',a'), but only through Q(s,a) to make convergence faster\n",
    "        #there is no theoritical guarantee behind, and it is only a practical trick\n",
    "        sigma_next=self.switch(next_state)\n",
    "        x,y=self.Softmax(next_state,sigma_next)\n",
    "        x=x+torch.log(y)\n",
    "        #computing r(s,a)\n",
    "        x=x+Reward(start_loci,end_loci)\n",
    "        x=x.detach()\n",
    "        \n",
    "        #computing Q(s,a)\n",
    "        sigma=self.switch.forward(state)\n",
    "        #Q_{phi_1}(s,c)\n",
    "        res_chrom=self.Chrom_model.forward(state,sigma)\n",
    "        \n",
    "        #Q_{phi_2}(s,c,sp)\n",
    "        res_cnv=self.CNV.forward(chrom,sigma)\n",
    "        #if there is originally a break point for start\n",
    "        #real world constraint as described in section 3.3.2\n",
    "        #only allow starting points (sp) to be the break points of CNP\n",
    "        break_start=torch.zeros(state.shape[0],chrom_width,2,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,1:]=chrom[:,:(chrom_width-1)]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_start[:,:,1]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_start[:,0,1]=1\n",
    "        #don't allow lose one copy when copy number equals 0, otherwise there is going to be negative copy numbers\n",
    "        break_start[:,:,0]=break_start[:,:,1]\n",
    "        break_start[:,:,0]=break_start[:,:,0]*torch.ceil((chrom/2-0.5)/max_copy)\n",
    "        break_start=break_start.reshape(state.shape[0],2*chrom_width)\n",
    "        res_cnv_full=torch.zeros(state.shape[0],2*chrom_width)\n",
    "        res_cnv_full[:,1:]=res_cnv\n",
    "        res_cnv_full=res_cnv_full+torch.log(break_start)\n",
    "        \n",
    "        #Q_{phi_2}(s,c,sp)-softmax(Q_{phi_2}(s,c,sp)) as described in section 3.3.1\n",
    "        cnv_max_val,cnv_max=torch.max(res_cnv_full,1)\n",
    "        cnv_softmax=res_cnv_full-cnv_max_val.reshape(state.shape[0],1).expand(-1,2*chrom_width)\n",
    "        cnv_softmax=torch.exp(cnv_softmax).sum(1)\n",
    "        x=x+cnv_max_val+torch.log(cnv_softmax)\n",
    "        \n",
    "        #Q_{phi_3}(s,c,sp,ep)\n",
    "        res_end=self.End.forward(chrom,chrom_new,sigma)\n",
    "        #if there is originally a break point for end\n",
    "        #and if this is after the starting point\n",
    "        #real world constraint in section 3.3.2\n",
    "        break_end=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:(chrom_width-1)]=chrom[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        for i in range(state.shape[0]):\n",
    "            #can't end before starting point\n",
    "            break_end[i,:int(start_loci[i])]=0*break_end[i,:int(start_loci[i])]\n",
    "            #don't allow lose one copy when copy number equalls 1\n",
    "            if(cnv[i]<0.5):\n",
    "                j=int(start_loci[i])+1\n",
    "                while(j<chrom_width):\n",
    "                    if(chrom[i][j]<1.5):\n",
    "                        break\n",
    "                    j=j+1\n",
    "                break_end[i,j:chrom_width]=0*break_end[i,j:chrom_width]\n",
    "            \n",
    "        res_end_full=torch.zeros(state.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        \n",
    "        #real world constraint described in section 3.3.2\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max_temp=torch.max(res_end_full,1)\n",
    "        end_softmax=res_end_full-end_max_val.reshape(state.shape[0],1).expand(-1,chrom_width)\n",
    "        end_softmax=torch.exp(end_softmax).sum(1)\n",
    "        #Q_{phi_3}(s,c,sp,ep)-softmax(Q_{phi_3}(s,c,sp,ep)) as described in section 3.3.1\n",
    "        x=x+end_max_val+torch.log(end_softmax)\n",
    "        \n",
    "        for i in range(state.shape[0]):\n",
    "            if valid[i]>0.5:#check validity to prevent inf-inf which ends in nan\n",
    "                x[i]=x[i]-res_chrom[i][int(Chr[i])]\n",
    "                cnv_rank=int(start_loci[i]*2+cnv[i])\n",
    "                x[i]=x[i]-res_cnv_full[i][cnv_rank]\n",
    "                end_rank=int(end_loci[i]-1)\n",
    "                x[i]=x[i]-res_end_full[i][end_rank]\n",
    "        \n",
    "        #remove training data which include invalid actions\n",
    "        x=x*valid\n",
    "        #return avdantage as well as a best cnv and sigma used for generating training data\n",
    "        #used for training in the next step\n",
    "        return x,cnv_max,sigma,res_chrom,res_cnv_full,res_end_full\n",
    "     \n",
    "    def Softmax(self,next_state,sigma):\n",
    "        #compute softmax_{a'} Q(s',a')\n",
    "        x=self.Chrom_model.forward(next_state,sigma)\n",
    "        max_chrom=torch.max(x,1)[0]\n",
    "        softmax_chrom=x-max_chrom.reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
    "        softmax_chrom=torch.exp(softmax_chrom).sum(1)\n",
    "        #special action END\n",
    "        #all the remaining abnormal loci are treated to be caused by several independent single locus copy number changes\n",
    "        end_val=torch.sum(torch.abs(next_state-1),(1,2,3))*math.log(single_loci_loss)\n",
    "        max_chrom,softmax_chrom=Soft_update(max_chrom,softmax_chrom,end_val,torch.ones(x.shape[0]))\n",
    "        #if there is a WGD followed immediately\n",
    "        for i in range(x.shape[0]):\n",
    "            #real world constraint as described in section 3.3.2\n",
    "            #do not allow (reversing) WGD when the CNP contain odd numbers for some loci\n",
    "            if (not torch.any(next_state[i]-2*torch.floor(next_state[i]/2)>0.5)) and torch.any(next_state[i]>0.5):\n",
    "                sigma_wgd=self.switch(torch.floor(next_state[i:(i+1)]/2))\n",
    "                sigma_wgd=sigma_wgd.detach()\n",
    "                wgd_val,wgd_soft=self.Softmax(torch.floor(next_state[i:(i+1)]/2),sigma_wgd)\n",
    "                max_chrom[i],softmax_chrom[i]=Soft_update(torch.ones(1)*max_chrom[i],torch.ones(1)*softmax_chrom[i],torch.ones(1)*wgd_val,torch.ones(1)*wgd_soft)\n",
    "        \n",
    "        return max_chrom,softmax_chrom\n",
    "  \n",
    "\n",
    "#Minimum example\n",
    "if __name__ == \"__main__\":\n",
    "    #test different parts separately\n",
    "    '''\n",
    "    switch=WGD_Net()\n",
    "    Chrom_model=Chrom_NN()\n",
    "    print(Chrom_model)\n",
    "    #test the structure of permutation invariant structure\n",
    "    x=torch.ones(3,1,num_chromosome,50)\n",
    "    x[0][0][0][0:50]=2\n",
    "    x[2][0][1][0:50]=2\n",
    "    prob=switch.forward(x)\n",
    "    print(prob)\n",
    "    res=Chrom_model.forward(x,prob)\n",
    "    print(res)\n",
    "    res=-float('inf')\n",
    "    res=torch.LongTensor(3)\n",
    "    \n",
    "    print(torch.log(res.type(torch.DoubleTensor)))\n",
    "    #CNV\n",
    "    CNV=CNV_NN()\n",
    "    res=CNV.forward(x[:,0,0,0:50],prob)\n",
    "    print(CNV)\n",
    "    print(res.shape)\n",
    "    #END\n",
    "    End=End_Point_NN()\n",
    "    res=End.forward(x[:,0,0,0:50],x[:,0,0,0:50]+1,prob)\n",
    "    print(End)\n",
    "    print(res.shape)\n",
    "    '''\n",
    "    #test Q-learning\n",
    "    x=torch.ones(3,1,num_chromosome,50)\n",
    "    y=torch.ones(3,1,num_chromosome,50)\n",
    "    x[0][0][0][0:50]=2\n",
    "    x[2][0][1][0:50]=2\n",
    "    chrom=x[:,0,0,:]\n",
    "    chrom_new=y[:,0,0,:]\n",
    "    Chr=torch.zeros(3)\n",
    "    cnv=torch.ones(3)\n",
    "    start_loci=torch.zeros(3)\n",
    "    end_loci=torch.ones(3)*50\n",
    "    valid=torch.ones(3)\n",
    "    Q_model=Q_learning()\n",
    "    #res,cnv_max,sigma,t,t2,t3=Q_model.forward(x,y,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid)\n",
    "    #print(res)\n",
    "    #print(cnv_max)\n",
    "    #loss=res.pow(2).mean()\n",
    "    #print(loss)\n",
    "    #loss.backward()\n",
    "    #params = list(Q_model.parameters())\n",
    "    #print(params[0].grad[0])\n",
    "    #print(Q_model.switch.conv1.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate data for testing\n",
    "def Simulate_data(batch_size=15,Number_of_step=70):\n",
    "    state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "    next_state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "    Chr=torch.ones(batch_size,requires_grad=False).type(torch.LongTensor)\n",
    "    step=torch.zeros(batch_size,requires_grad=False)\n",
    "    wgd=torch.zeros(batch_size,requires_grad=False)\n",
    "    valid=torch.ones(batch_size,requires_grad=False)\n",
    "    \n",
    "    start_loci=torch.randint(high=chrom_width,size=(batch_size,),requires_grad=False)\n",
    "    end_loci=torch.LongTensor(batch_size)\n",
    "    cnv=torch.ones(batch_size,requires_grad=False)\n",
    "    chrom=torch.Tensor(batch_size,chrom_width)\n",
    "    chrom_new=torch.Tensor(batch_size,chrom_width)\n",
    "    \n",
    "    step_counter=0\n",
    "    while(step_counter<Number_of_step):\n",
    "        for i in range(batch_size):\n",
    "            #reset valid after they have been checked\n",
    "            valid[i]=1\n",
    "            end_loci[i]=1+torch.randint(low=start_loci[i],high=50,size=(1,))[0]\n",
    "            #change the chromosone that CNV is on with some probability\n",
    "            if torch.rand(1)[0]>0.5:\n",
    "                Chr[i]=torch.randint(high=num_chromosome,size=(1,))[0]\n",
    "            #adding probability to sample chromosomal changes during training\n",
    "            if torch.rand(1)[0]>0.8:\n",
    "                start_loci[i]=0\n",
    "                end_loci[i]=chrom_width\n",
    "            #cnv\n",
    "            if torch.rand(1)[0]>0.7:\n",
    "                cnv[i]=0\n",
    "            #modifying cnp\n",
    "            prob_wgd=0.4/(1+math.exp(-step[i]+5))\n",
    "            #wgd          \n",
    "            if (torch.rand(1)[0]<prob_wgd and wgd[i]<1):\n",
    "                wgd[i]=1\n",
    "                state[i]=state[i]*2\n",
    "                next_state[i]=next_state[i]*2\n",
    "                #adding cnv effect\n",
    "                #increasing copies when no wgd\n",
    "                #decreasing copies when wgd\n",
    "            if wgd[i]>0.5:\n",
    "                cnv[i]=1-cnv[i]\n",
    "            state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]=state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]-(cnv[i]-0.5)*2\n",
    "            chrom[i]=state[i][0][Chr[i]][:]\n",
    "            #reverse effect on chrom_new\n",
    "            chrom_new[i]=state[i][0][Chr[i]][:]\n",
    "            chrom_new[i][(start_loci[i]):]=chrom_new[i][(start_loci[i]):]+(cnv[i]-0.5)*2\n",
    "            #not going to negative values\n",
    "            if(torch.any(state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]< -0.5)):\n",
    "                valid[i]=0\n",
    "            #not joining breakpoints\n",
    "            if(start_loci[i]>0.5 and torch.abs(chrom[i][start_loci[i]]-chrom[i][start_loci[i]-1])<0.5):\n",
    "                valid[i]=0\n",
    "            if(end_loci[i]<chrom_width-0.5 and torch.abs(chrom[i][end_loci[i]-1]-chrom[i][end_loci[i]])<0.5):\n",
    "                valid[i]=0\n",
    "            if valid[i]>0 :\n",
    "                next_state[i]=state[i].clone()\n",
    "                step[i]=step[i]+1\n",
    "            #stay to further train the current step\n",
    "            #or resample another action\n",
    "            else:\n",
    "                state[i]=next_state[i].clone()\n",
    "        step_counter=step_counter+1\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23., 29., -1., 18., 18., 22., 22., 22., 22., 23., 26., 26., 26., 26.,\n",
      "         31., 31.,  0.,  0.,  0.,  0.]])\n",
      "tensor([[ 7., 59., -1.,  1.,  0.,  0.,  0.,  0., 12.,  1., 12., 12., 12., 12.,\n",
      "          1.,  0.,  0.,  0.,  0.,  0.]])\n",
      "tensor([[ 6., 49., -1., 32.,  6., 50., 50., 50., 26.,  3., 26., 27., 44., 30.,\n",
      "         14.,  6.,  0.,  0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "#Deconvolution.py\n",
    "#used for deconvolution of CNP history\n",
    "\n",
    "def Deconvolute(model,cnp,Chr,CNV,End):\n",
    "    '''\n",
    "    Deconvolution samples the maximum action in a greedy way\n",
    "    model:the trained Q-learning model\n",
    "    cnp: the input CNP, shape: Number of CNP,1,44 (#Chr),50 (#regions for one chromosome)\n",
    "    Chr,CNV,END: output tensor,shape: Number of CNP, maximum length of history\n",
    "    output: for Chr: -1 indicates WGD, 0 indicates no action, 1~44 the chromosome\n",
    "            for CNV: only valid if Chr is not -1 or 0\n",
    "                     indicates the starting point (CNV//2) and the type of CNV (CNV%2==1 for gain and CNV%2==0 for loss)\n",
    "            for End: only valid if Chr is not -1 or 0\n",
    "                     indicates the end point for a CNV.\n",
    "    '''\n",
    "    max_step=int(Chr.shape[1])\n",
    "    flag=False\n",
    "    for i in range(cnp.shape[0]):\n",
    "        current_cnp=cnp[i:(i+1)]\n",
    "        step=0\n",
    "        while(step<max_step):\n",
    "            #it is also possible to manually set the switch if deemed necessary\n",
    "            sigma=model.switch(current_cnp)\n",
    "            #hard classification\n",
    "            if not flag:\n",
    "                sigma=torch.ceil(sigma)\n",
    "            #else:\n",
    "            #    sigma=torch.zeros_like(sigma)\n",
    "            #print(sigma)\n",
    "            res_chrom=model.Chrom_model(current_cnp,sigma)\n",
    "            #print(res_chrom)\n",
    "            #find the chromosome with the maximum probability\n",
    "            val,temp_Chr=res_chrom.max(1)\n",
    "            temp_Chr=int(temp_Chr)\n",
    "            Chr[i][step]=temp_Chr+1\n",
    "            #WGD\n",
    "            if (not torch.any(current_cnp-2*torch.floor(current_cnp/2)>0.5)) and torch.any(current_cnp>0.5):\n",
    "                sigma_wgd=model.switch(torch.floor(current_cnp/2))\n",
    "                res_chrom_wgd=model.Chrom_model(torch.floor(current_cnp/2),sigma_wgd)\n",
    "                val_wgd,temp=res_chrom_wgd.max(1)\n",
    "                if not flag:#val_wgd>=val and not flag:\n",
    "                    val=val_wgd\n",
    "                    Chr[i][step]=-1\n",
    "                    CNV[i][step]=-1\n",
    "                    End[i][step]=-1\n",
    "                    flag=True\n",
    "            #special action END\n",
    "            val_end=torch.sum(torch.abs(current_cnp-1))*math.log(single_loci_loss)\n",
    "            if val_end>=val:\n",
    "                val=val_end\n",
    "                Chr[i][step]=0\n",
    "                #print(val_end)\n",
    "                break\n",
    "            #if WGD\n",
    "            if Chr[i][step]< -0.5:\n",
    "                current_cnp=(current_cnp/2).floor()\n",
    "                flag=True\n",
    "            #if not WGD or END\n",
    "            elif Chr[i][step]>0.5:\n",
    "                #find best CNV\n",
    "                chrom=current_cnp[:,0,temp_Chr,:]\n",
    "                CNV[i][step]=model.CNV.find_one_cnv(chrom,sigma)\n",
    "                cnv_temp=int(CNV[i][step]%2)\n",
    "                start_temp=int(CNV[i][step]//2)\n",
    "                #find best End\n",
    "                chrom_new=chrom.clone()\n",
    "                chrom_new[:,start_temp:]=chrom_new[:,start_temp:]+(cnv_temp-0.5)*2\n",
    "                End[i][step]=model.End.find_one_end(chrom,chrom_new,sigma,start_temp,cnv_temp)\n",
    "                #updata cnp\n",
    "                #print(chrom)\n",
    "                #print(start_temp,End[i][step],cnv_temp)\n",
    "                \n",
    "                current_cnp[:,0,temp_Chr,start_temp:int(End[i][step])]=current_cnp[:,0,temp_Chr,start_temp:int(End[i][step])]+(cnv_temp-0.5)*2\n",
    "                \n",
    "            step=step+1\n",
    "    return Chr,CNV,End,current_cnp\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    model = Q_learning()\n",
    "    model.load_state_dict(torch.load(\"/data/suzaku/ted/HOME/model\"))\n",
    "    model.eval()\n",
    "    counter_global=torch.randint(10000,(1,))[0]\n",
    "    #loading simulated data\n",
    "    state=Simulate_data(batch_size=1,Number_of_step=15)\n",
    "    Chr=torch.zeros(state.shape[0],20)\n",
    "    CNV=torch.zeros(state.shape[0],20)\n",
    "    End=torch.zeros(state.shape[0],20)\n",
    "    state_copy=state.clone()\n",
    "    #deconvolution\n",
    "    Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "    print(Chr)\n",
    "    #check if the model picks the correct chromosome\n",
    "    print(CNV)\n",
    "    print(End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0014]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 1331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state=torch.ones(1,1,num_chromosome,50)\n",
    "state[0][0][:((num_chromosome//2)+0),:]=1\n",
    "state[0][0][((num_chromosome//2)+0):,:]=1\n",
    "model.switch.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0014]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 1332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.switch.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 1206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(state).mean(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 1207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0][0][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.5296, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 1208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Chrom_model(state,model.switch.forward(state))[0][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])"
      ]
     },
     "execution_count": 1320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0589]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 1284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,1]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,1]).view((22,50))\n",
    "model.switch.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 2)"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major=np.loadtxt(\"/data/suzaku/ted/WGD/major_Kuramoch_comp_samp.txt\")\n",
    "minor=np.loadtxt(\"/data/suzaku/ted/WGD/minor_Kuramoch_comp_samp.txt\")\n",
    "major.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  4.,  5.,  6.,  6.,  6.,  6.,  8.,  8.,  8.,  8.,  8.,  9.,\n",
      "         10., 10., 10., 11., 12., 12., 12., 13., 13., 14., 15., 16., 16., 17.,\n",
      "         17., 18., 19., 19., 20., 20., 21., 21., 21., 23., 24., 25., 25., 26.,\n",
      "         27., 27., 27., 28., 28., 28., 29., 30., 31., 32., 32., 33., 34., 35.,\n",
      "         35., 36., 36., 37., 38., 38., 39., 40., 41., 41., 42., 43., 43., -1.,\n",
      "          1.,  1.,  5.,  6.,  6.,  6.,  6.,  7.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "         10., 12., 13., 13., 14., 14., 14., 16., 17., 17., 19., 19., 19., 20.,\n",
      "         21., 21., 21., 21., 21., 23., 23., 25., 25., 27., 28., 29., 29., 30.,\n",
      "         32., 32., 32., 33., 35., 35., 36., 36., 36., 36., 37., 39., 39., 40.,\n",
      "         41., 43., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37.,\n",
      "         37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37.,\n",
      "         37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37.,\n",
      "         37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37.,\n",
      "         37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37., 37.,\n",
      "         37., 37., 37., 37.]])\n",
      "tensor([[ 1.,  1.,  1., 39., 71., 37., 19.,  3., 37., 87., 94., 89.,  1., 11.,\n",
      "          1., 23., 33., 99., 91., 19.,  1., 15., 11., 95.,  1., 47., 71., 41.,\n",
      "          7.,  1., 21., 49., 47.,  1.,  7.,  1.,  8.,  1.,  1., 29., 43.,  1.,\n",
      "         97.,  1., 67.,  1., 23., 41., 61., 37.,  1., 77., 47., 19.,  1., 11.,\n",
      "         13., 97.,  3.,  1.,  1., 71., 37.,  1.,  1., 93.,  1.,  1.,  3., -1.,\n",
      "          0., 42.,  0., 18., 70., 40.,  2., 99., 88., 95., 51.,  1.,  0., 60.,\n",
      "          0., 18., 10., 14.,  1., 94., 16., 68.,  0., 99.,  1.,  0., 48., 54.,\n",
      "          4.,  4.,  4.,  0.,  4., 42., 42.,  1., 28., 39., 37., 99.,  1.,  1.,\n",
      "          1., 22., 71.,  1., 10., 81., 96., 99.,  1.,  2.,  1.,  1., 45., 15.,\n",
      "         81., 27.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,\n",
      "          0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,\n",
      "          0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,\n",
      "          0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,\n",
      "          0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,\n",
      "          0.,  1.,  0.,  1.]])\n",
      "tensor([[21., 10., 50., 34., 49., 20., 11.,  9., 19., 44., 50., 47., 14., 47.,\n",
      "         16., 13., 50., 50., 50., 12.,  3., 40.,  6., 50.,  2., 34., 50., 24.,\n",
      "         10., 50., 24., 40., 27.,  2.,  4.,  2., 50., 21., 50., 15., 50., 50.,\n",
      "         50.,  9., 34.,  9., 18., 50., 35., 19., 50., 50., 35., 50., 50.,  6.,\n",
      "         40., 49.,  8.,  2., 34., 50., 22.,  7., 40., 50., 50., 13.,  2., -1.,\n",
      "         23., 23., 19., 11., 49., 22., 11., 50., 47., 50., 43., 18., 50., 50.,\n",
      "         16., 12.,  6., 40.,  1., 50., 16., 35., 10., 50., 24., 40., 41., 50.,\n",
      "          3.,  3.,  3.,  1.,  3., 23., 23., 21., 15., 33., 20., 50., 19., 18.,\n",
      "         23., 13., 38.,  9.,  6., 50., 49., 50., 48., 16.,  2., 18., 50., 50.,\n",
      "         46., 50.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "          2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "          2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "          2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "          2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "          2.,  2.,  2.,  2.]])\n",
      "tensor([[  0,   0],\n",
      "        [  0,   1],\n",
      "        [  0,   2],\n",
      "        [  0,   3],\n",
      "        [  0,   4],\n",
      "        [  0,   5],\n",
      "        [  0,   6],\n",
      "        [  0,   7],\n",
      "        [  0,   8],\n",
      "        [  0,   9],\n",
      "        [  0,  10],\n",
      "        [  0,  11],\n",
      "        [  0,  12],\n",
      "        [  0,  13],\n",
      "        [  0,  14],\n",
      "        [  0,  15],\n",
      "        [  0,  16],\n",
      "        [  0,  17],\n",
      "        [  0,  18],\n",
      "        [  0,  19],\n",
      "        [  0,  20],\n",
      "        [  0,  21],\n",
      "        [  0,  22],\n",
      "        [  0,  23],\n",
      "        [  0,  24],\n",
      "        [  0,  25],\n",
      "        [  0,  26],\n",
      "        [  0,  27],\n",
      "        [  0,  28],\n",
      "        [  0,  29],\n",
      "        [  0,  30],\n",
      "        [  0,  31],\n",
      "        [  0,  32],\n",
      "        [  0,  33],\n",
      "        [  0,  34],\n",
      "        [  0,  35],\n",
      "        [  0,  36],\n",
      "        [  0,  37],\n",
      "        [  0,  38],\n",
      "        [  0,  39],\n",
      "        [  0,  40],\n",
      "        [  0,  41],\n",
      "        [  0,  42],\n",
      "        [  0,  43],\n",
      "        [  0,  44],\n",
      "        [  0,  45],\n",
      "        [  0,  46],\n",
      "        [  0,  47],\n",
      "        [  0,  48],\n",
      "        [  0,  49],\n",
      "        [  0,  50],\n",
      "        [  0,  51],\n",
      "        [  0,  52],\n",
      "        [  0,  53],\n",
      "        [  0,  54],\n",
      "        [  0,  55],\n",
      "        [  0,  56],\n",
      "        [  0,  57],\n",
      "        [  0,  58],\n",
      "        [  0,  59],\n",
      "        [  0,  60],\n",
      "        [  0,  61],\n",
      "        [  0,  62],\n",
      "        [  0,  63],\n",
      "        [  0,  64],\n",
      "        [  0,  65],\n",
      "        [  0,  66],\n",
      "        [  0,  67],\n",
      "        [  0,  68],\n",
      "        [  0,  69],\n",
      "        [  0,  70],\n",
      "        [  0,  71],\n",
      "        [  0,  72],\n",
      "        [  0,  73],\n",
      "        [  0,  74],\n",
      "        [  0,  75],\n",
      "        [  0,  76],\n",
      "        [  0,  77],\n",
      "        [  0,  78],\n",
      "        [  0,  79],\n",
      "        [  0,  80],\n",
      "        [  0,  81],\n",
      "        [  0,  82],\n",
      "        [  0,  83],\n",
      "        [  0,  84],\n",
      "        [  0,  85],\n",
      "        [  0,  86],\n",
      "        [  0,  87],\n",
      "        [  0,  88],\n",
      "        [  0,  89],\n",
      "        [  0,  90],\n",
      "        [  0,  91],\n",
      "        [  0,  92],\n",
      "        [  0,  93],\n",
      "        [  0,  94],\n",
      "        [  0,  95],\n",
      "        [  0,  96],\n",
      "        [  0,  97],\n",
      "        [  0,  98],\n",
      "        [  0,  99],\n",
      "        [  0, 100],\n",
      "        [  0, 101],\n",
      "        [  0, 102],\n",
      "        [  0, 103],\n",
      "        [  0, 104],\n",
      "        [  0, 105],\n",
      "        [  0, 106],\n",
      "        [  0, 107],\n",
      "        [  0, 108],\n",
      "        [  0, 109],\n",
      "        [  0, 110],\n",
      "        [  0, 111],\n",
      "        [  0, 112],\n",
      "        [  0, 113],\n",
      "        [  0, 114],\n",
      "        [  0, 115],\n",
      "        [  0, 116],\n",
      "        [  0, 117],\n",
      "        [  0, 118],\n",
      "        [  0, 119],\n",
      "        [  0, 120],\n",
      "        [  0, 121],\n",
      "        [  0, 122],\n",
      "        [  0, 123],\n",
      "        [  0, 124],\n",
      "        [  0, 125],\n",
      "        [  0, 126],\n",
      "        [  0, 127],\n",
      "        [  0, 128],\n",
      "        [  0, 129],\n",
      "        [  0, 130],\n",
      "        [  0, 131],\n",
      "        [  0, 132],\n",
      "        [  0, 133],\n",
      "        [  0, 134],\n",
      "        [  0, 135],\n",
      "        [  0, 136],\n",
      "        [  0, 137],\n",
      "        [  0, 138],\n",
      "        [  0, 139],\n",
      "        [  0, 140],\n",
      "        [  0, 141],\n",
      "        [  0, 142],\n",
      "        [  0, 143],\n",
      "        [  0, 144],\n",
      "        [  0, 145],\n",
      "        [  0, 146],\n",
      "        [  0, 147],\n",
      "        [  0, 148],\n",
      "        [  0, 149],\n",
      "        [  0, 150],\n",
      "        [  0, 151],\n",
      "        [  0, 152],\n",
      "        [  0, 153],\n",
      "        [  0, 154],\n",
      "        [  0, 155],\n",
      "        [  0, 156],\n",
      "        [  0, 157],\n",
      "        [  0, 158],\n",
      "        [  0, 159],\n",
      "        [  0, 160],\n",
      "        [  0, 161],\n",
      "        [  0, 162],\n",
      "        [  0, 163],\n",
      "        [  0, 164],\n",
      "        [  0, 165],\n",
      "        [  0, 166],\n",
      "        [  0, 167],\n",
      "        [  0, 168],\n",
      "        [  0, 169],\n",
      "        [  0, 170],\n",
      "        [  0, 171],\n",
      "        [  0, 172],\n",
      "        [  0, 173],\n",
      "        [  0, 174],\n",
      "        [  0, 175],\n",
      "        [  0, 176],\n",
      "        [  0, 177],\n",
      "        [  0, 178],\n",
      "        [  0, 179],\n",
      "        [  0, 180],\n",
      "        [  0, 181],\n",
      "        [  0, 182],\n",
      "        [  0, 183],\n",
      "        [  0, 184],\n",
      "        [  0, 185],\n",
      "        [  0, 186],\n",
      "        [  0, 187],\n",
      "        [  0, 188],\n",
      "        [  0, 189],\n",
      "        [  0, 190],\n",
      "        [  0, 191],\n",
      "        [  0, 192],\n",
      "        [  0, 193],\n",
      "        [  0, 194],\n",
      "        [  0, 195],\n",
      "        [  0, 196],\n",
      "        [  0, 197],\n",
      "        [  0, 198],\n",
      "        [  0, 199]])\n"
     ]
    }
   ],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,0]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,0]).view((22,50))\n",
    "Chr=torch.zeros(state.shape[0],200)\n",
    "CNV=torch.zeros(state.shape[0],200)\n",
    "End=torch.zeros(state.shape[0],200)\n",
    "state_copy=state.clone()\n",
    "#deconvolution\n",
    "Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "print(Chr)\n",
    "#check if the model picks the correct chromosome\n",
    "print(CNV)\n",
    "print(End)\n",
    "print(torch.nonzero(Chr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  2.,  4.,  5.,  6.,  6.,  6.,  6.,  8.,  8.,  8.,  8.,\n",
      "          9., 10., 10., 11., 12., 12., 12., 15., 16., 16., 17., 17., 17., 18.,\n",
      "         19., 19., 20., 20., 20., 21., 21., 21., 23., 23., 23., 24., 25., 25.,\n",
      "         26., 27., 27., 28., 28., 28., 28., 29., 31., 32., 33., 34., 35., 36.,\n",
      "         37., 38., 38., 40., 41., 41., 42., 43., 43., -1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  5.,  6.,  6.,  6.,  7.,  7.,  8.,  8.,  8., 10., 12., 13., 14.,\n",
      "         14., 16., 17., 17., 17., 17., 19., 20., 20., 20., 20., 21., 21., 21.,\n",
      "         21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
      "         21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
      "         21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
      "         21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
      "         21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
      "         21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
      "         21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
      "         21., 21., 21., 21.]])\n",
      "tensor([[43., 15.,  1.,  1.,  1., 39., 41., 37., 22.,  3., 87., 94., 89.,  1.,\n",
      "         11.,  1., 33., 99.,  1., 19., 25.,  1., 47., 71.,  1., 37., 26.,  1.,\n",
      "         65.,  1., 55., 67.,  1.,  7.,  5., 49., 13.,  1., 15.,  1., 29., 43.,\n",
      "          1.,  1., 67.,  1., 25., 41., 45., 39.,  1.,  1., 19.,  1., 13.,  3.,\n",
      "          1.,  1., 71.,  1.,  1., 93.,  1.,  1.,  7., -1., 42., 42., 42.,  0.,\n",
      "         42.,  0., 18., 40.,  2., 99., 72., 88., 86., 82.,  0., 18., 10.,  1.,\n",
      "         16., 68., 36., 99.,  0., 36., 48., 95.,  1.,  0., 54.,  4.,  4., 49.,\n",
      "          1.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,\n",
      "          4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,\n",
      "          4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,\n",
      "          4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,\n",
      "          4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,\n",
      "          4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,\n",
      "          4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,  4.,  5.,\n",
      "          4.,  5.,  4.,  5.]])\n",
      "tensor([[23., 21.,  6., 11., 50., 33., 21., 20., 12.,  9., 44., 50., 47., 14.,\n",
      "         47., 16., 50., 50.,  9., 12., 50.,  2., 34., 50., 24., 22., 18., 50.,\n",
      "         40., 24., 50., 47., 27., 16.,  3., 50.,  7.,  6., 23., 50., 15., 50.,\n",
      "         50.,  9., 50.,  9., 18., 21., 50., 21., 50., 50., 50., 50., 40.,  8.,\n",
      "          2., 34., 50.,  7., 40., 50., 50.,  2., 50., -1., 23., 23., 23., 23.,\n",
      "         23., 19., 11., 22., 11., 50., 41., 47., 47., 50., 16., 12.,  6.,  1.,\n",
      "         14., 35., 20., 50., 13., 22., 40., 50., 33., 50., 50.,  3.,  3., 50.,\n",
      "         24., 24., 50.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
      "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
      "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
      "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
      "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
      "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
      "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
      "          3.,  3.,  3.,  3.]])\n",
      "tensor([[  0,   0],\n",
      "        [  0,   1],\n",
      "        [  0,   2],\n",
      "        [  0,   3],\n",
      "        [  0,   4],\n",
      "        [  0,   5],\n",
      "        [  0,   6],\n",
      "        [  0,   7],\n",
      "        [  0,   8],\n",
      "        [  0,   9],\n",
      "        [  0,  10],\n",
      "        [  0,  11],\n",
      "        [  0,  12],\n",
      "        [  0,  13],\n",
      "        [  0,  14],\n",
      "        [  0,  15],\n",
      "        [  0,  16],\n",
      "        [  0,  17],\n",
      "        [  0,  18],\n",
      "        [  0,  19],\n",
      "        [  0,  20],\n",
      "        [  0,  21],\n",
      "        [  0,  22],\n",
      "        [  0,  23],\n",
      "        [  0,  24],\n",
      "        [  0,  25],\n",
      "        [  0,  26],\n",
      "        [  0,  27],\n",
      "        [  0,  28],\n",
      "        [  0,  29],\n",
      "        [  0,  30],\n",
      "        [  0,  31],\n",
      "        [  0,  32],\n",
      "        [  0,  33],\n",
      "        [  0,  34],\n",
      "        [  0,  35],\n",
      "        [  0,  36],\n",
      "        [  0,  37],\n",
      "        [  0,  38],\n",
      "        [  0,  39],\n",
      "        [  0,  40],\n",
      "        [  0,  41],\n",
      "        [  0,  42],\n",
      "        [  0,  43],\n",
      "        [  0,  44],\n",
      "        [  0,  45],\n",
      "        [  0,  46],\n",
      "        [  0,  47],\n",
      "        [  0,  48],\n",
      "        [  0,  49],\n",
      "        [  0,  50],\n",
      "        [  0,  51],\n",
      "        [  0,  52],\n",
      "        [  0,  53],\n",
      "        [  0,  54],\n",
      "        [  0,  55],\n",
      "        [  0,  56],\n",
      "        [  0,  57],\n",
      "        [  0,  58],\n",
      "        [  0,  59],\n",
      "        [  0,  60],\n",
      "        [  0,  61],\n",
      "        [  0,  62],\n",
      "        [  0,  63],\n",
      "        [  0,  64],\n",
      "        [  0,  65],\n",
      "        [  0,  66],\n",
      "        [  0,  67],\n",
      "        [  0,  68],\n",
      "        [  0,  69],\n",
      "        [  0,  70],\n",
      "        [  0,  71],\n",
      "        [  0,  72],\n",
      "        [  0,  73],\n",
      "        [  0,  74],\n",
      "        [  0,  75],\n",
      "        [  0,  76],\n",
      "        [  0,  77],\n",
      "        [  0,  78],\n",
      "        [  0,  79],\n",
      "        [  0,  80],\n",
      "        [  0,  81],\n",
      "        [  0,  82],\n",
      "        [  0,  83],\n",
      "        [  0,  84],\n",
      "        [  0,  85],\n",
      "        [  0,  86],\n",
      "        [  0,  87],\n",
      "        [  0,  88],\n",
      "        [  0,  89],\n",
      "        [  0,  90],\n",
      "        [  0,  91],\n",
      "        [  0,  92],\n",
      "        [  0,  93],\n",
      "        [  0,  94],\n",
      "        [  0,  95],\n",
      "        [  0,  96],\n",
      "        [  0,  97],\n",
      "        [  0,  98],\n",
      "        [  0,  99],\n",
      "        [  0, 100],\n",
      "        [  0, 101],\n",
      "        [  0, 102],\n",
      "        [  0, 103],\n",
      "        [  0, 104],\n",
      "        [  0, 105],\n",
      "        [  0, 106],\n",
      "        [  0, 107],\n",
      "        [  0, 108],\n",
      "        [  0, 109],\n",
      "        [  0, 110],\n",
      "        [  0, 111],\n",
      "        [  0, 112],\n",
      "        [  0, 113],\n",
      "        [  0, 114],\n",
      "        [  0, 115],\n",
      "        [  0, 116],\n",
      "        [  0, 117],\n",
      "        [  0, 118],\n",
      "        [  0, 119],\n",
      "        [  0, 120],\n",
      "        [  0, 121],\n",
      "        [  0, 122],\n",
      "        [  0, 123],\n",
      "        [  0, 124],\n",
      "        [  0, 125],\n",
      "        [  0, 126],\n",
      "        [  0, 127],\n",
      "        [  0, 128],\n",
      "        [  0, 129],\n",
      "        [  0, 130],\n",
      "        [  0, 131],\n",
      "        [  0, 132],\n",
      "        [  0, 133],\n",
      "        [  0, 134],\n",
      "        [  0, 135],\n",
      "        [  0, 136],\n",
      "        [  0, 137],\n",
      "        [  0, 138],\n",
      "        [  0, 139],\n",
      "        [  0, 140],\n",
      "        [  0, 141],\n",
      "        [  0, 142],\n",
      "        [  0, 143],\n",
      "        [  0, 144],\n",
      "        [  0, 145],\n",
      "        [  0, 146],\n",
      "        [  0, 147],\n",
      "        [  0, 148],\n",
      "        [  0, 149],\n",
      "        [  0, 150],\n",
      "        [  0, 151],\n",
      "        [  0, 152],\n",
      "        [  0, 153],\n",
      "        [  0, 154],\n",
      "        [  0, 155],\n",
      "        [  0, 156],\n",
      "        [  0, 157],\n",
      "        [  0, 158],\n",
      "        [  0, 159],\n",
      "        [  0, 160],\n",
      "        [  0, 161],\n",
      "        [  0, 162],\n",
      "        [  0, 163],\n",
      "        [  0, 164],\n",
      "        [  0, 165],\n",
      "        [  0, 166],\n",
      "        [  0, 167],\n",
      "        [  0, 168],\n",
      "        [  0, 169],\n",
      "        [  0, 170],\n",
      "        [  0, 171],\n",
      "        [  0, 172],\n",
      "        [  0, 173],\n",
      "        [  0, 174],\n",
      "        [  0, 175],\n",
      "        [  0, 176],\n",
      "        [  0, 177],\n",
      "        [  0, 178],\n",
      "        [  0, 179],\n",
      "        [  0, 180],\n",
      "        [  0, 181],\n",
      "        [  0, 182],\n",
      "        [  0, 183],\n",
      "        [  0, 184],\n",
      "        [  0, 185],\n",
      "        [  0, 186],\n",
      "        [  0, 187],\n",
      "        [  0, 188],\n",
      "        [  0, 189],\n",
      "        [  0, 190],\n",
      "        [  0, 191],\n",
      "        [  0, 192],\n",
      "        [  0, 193],\n",
      "        [  0, 194],\n",
      "        [  0, 195],\n",
      "        [  0, 196],\n",
      "        [  0, 197],\n",
      "        [  0, 198],\n",
      "        [  0, 199]])\n"
     ]
    }
   ],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,1]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,1]).view((22,50))\n",
    "Chr=torch.zeros(state.shape[0],200)\n",
    "CNV=torch.zeros(state.shape[0],200)\n",
    "End=torch.zeros(state.shape[0],200)\n",
    "state_copy=state.clone()\n",
    "#deconvolution\n",
    "Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "print(Chr)\n",
    "#check if the model picks the correct chromosome\n",
    "print(CNV)\n",
    "print(End)\n",
    "print(torch.nonzero(Chr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,1]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,1]).view((22,50))\n",
    "model.switch(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLevolution",
   "language": "python",
   "name": "rlevolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
