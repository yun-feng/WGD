{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reward.py\n",
    "#Define the reward function r(s,a) when a is not a special action END\n",
    "\n",
    "#For each chromosome, we consider 50 SNP loci\n",
    "chrom_width=50;\n",
    "#44 normal chromosomes in human genome\n",
    "num_chromosome=44\n",
    "\n",
    "\n",
    "#normalisation constant to make normal_const*(\\sum_i a_i) <1, so that the possibility of all CNV sum to a real value smaller than 1\n",
    "normal_const=1e-5;\n",
    "#probability of single locus gain/loss\n",
    "single_loci_loss=normal_const*(1-2e-1);\n",
    "#probability of WGD\n",
    "WGD=normal_const*0.6;\n",
    "\n",
    "#log probability of CNV\n",
    "#used for calculating the distribution of p(a) when a is a focal CNV\n",
    "const1=single_loci_loss\n",
    "const2=1;\n",
    "\n",
    "#Whole chromosome change probability\n",
    "Whole_Chromosome_CNV=single_loci_loss/4;\n",
    "Half_Chromosome_CNV=single_loci_loss/3;\n",
    "\n",
    "max_copy=20\n",
    "\n",
    "def Reward(Start,End):\n",
    "    Start=Start.to(torch.float32)\n",
    "    End=End.to(torch.float32)\n",
    "    reward=torch.log(const1/(const2+End-Start))\n",
    "    #chromosome changes\n",
    "    for i in range(Start.shape[0]):\n",
    "        #full chromosome\n",
    "        if End[i]-Start[i]>chrom_width-0.5:\n",
    "            reward[i]=math.log(Whole_Chromosome_CNV)\n",
    "        #arm level changes\n",
    "        if chrom_width-End[i]<0.5 and abs(chrom_width//2-Start[i])<1.5:\n",
    "            reward[i]=math.log(Half_Chromosome_CNV)\n",
    "        if Start[i]<0.5 and abs(chrom_width//2-End[i])<1.5:\n",
    "            reward[i]=math.log(Half_Chromosome_CNV)\n",
    "    return reward\n",
    "\n",
    "\n",
    "\n",
    "#Q-function.py\n",
    "#defining the Q-function \n",
    "#Q(s,a) in manuscript\n",
    "\n",
    "\n",
    "#switch structure mentioned in section 3.3.4\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_switch = [20,40,80]\n",
    "activatiion_wgd=torch.tanh\n",
    "class WGD_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WGD_Net, self).__init__()\n",
    "        #chromosome permutation invariant structure as described in section 3.3.3\n",
    "        #slide for chromosome is 1 and the filter length in this dimension is also 1\n",
    "        #thus, the same filter goes through all chromosomes in the same fashion\n",
    "        self.conv1=nn.Conv2d(1, nkernels_switch[0], (1,3),(1,1),(0,1))\n",
    "        self.conv2=nn.Conv2d(nkernels_switch[0],nkernels_switch[1] , (1,3),(1,1), (0,1))\n",
    "        self.conv3=nn.Conv2d(nkernels_switch[1],nkernels_switch[2] , (1,5),(1,1), (0,0))\n",
    "        self.linear=nn.Linear(nkernels_switch[2],1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #y=torch.clamp((F.relu(x.mean(3)-1)-0.5+0.5*F.relu(1-x.mean(3))),-1,1).sum((1,2))\n",
    "        y=20*(x.mean((1,2,3))-1.5)\n",
    "        y=y.reshape(x.shape[0],1).detach()\n",
    "        x=x.reshape(x.shape[0],1,num_chromosome,chrom_width)\n",
    "        x=F.max_pool2d(activatiion_wgd(self.conv1(x)),(1,5),(1,5),(0,0))\n",
    "        x=F.max_pool2d(activatiion_wgd(self.conv2(x)),(1,2),(1,2),(0,0))\n",
    "        x=(activatiion_wgd(self.conv3(x))).sum((2,3))\n",
    "        x=self.linear(x)\n",
    "        x=x/2\n",
    "        #residule representation in x as described in section 3.3.4\n",
    "        x=y/2+x\n",
    "        x=torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "#chromosome evaluation net \n",
    "#Used in Chrom_NN (which is Q_{phi_1}(s,c) in section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_chr = [80,120,160]\n",
    "activation_cnp=torch.tanh\n",
    "class CNP_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNP_Val, self).__init__()\n",
    "        self.conv1=nn.Conv2d(1, nkernels_chr[0], (1,5),(1,1),(0,2))\n",
    "        self.conv2=nn.Conv2d(nkernels_chr[0],nkernels_chr[1] , (1,3),(1,1), (0,1))\n",
    "        self.conv3=nn.Conv2d(nkernels_chr[1],nkernels_chr[2] , (1,3),(1,1), (0,1))\n",
    "        self.conv4=nn.Conv2d(nkernels_chr[2],1, (1,5),(1,1), (0,0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=F.max_pool2d(activation_cnp(self.conv1(x)),(1,3),(1,3),(0,1))\n",
    "        x=F.max_pool2d(activation_cnp(self.conv2(x)),(1,2),(1,2),(0,1))\n",
    "        x=F.max_pool2d(activation_cnp(self.conv3(x)),(1,2),(1,2),(0,1))\n",
    "        #KL divergence is always nonpositive\n",
    "        x=0.25+F.elu(self.conv4(x),0.25)\n",
    "        #number of sample * 44 chromosomes\n",
    "        x=x.reshape(x.shape[0],num_chromosome)\n",
    "        return x\n",
    "\n",
    "#Implemts Q_{phi_1}(s,c) in section 3.3.1\n",
    "#It combines two chromosome evaluation nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_1}(s,c)\n",
    "class Chrom_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Chrom_NN,self).__init__()\n",
    "        #two parts of the Chrom_NN\n",
    "        #NN for CNP without WGD \n",
    "        self.Val_noWGD=CNP_Val()\n",
    "        #NN for CNP with WGD\n",
    "        self.Val_WGD=CNP_Val()\n",
    "    \n",
    "    def forward(self,x,sigma):\n",
    "        #probability for WGD, which is computed by switch structure\n",
    "        sigma=sigma.expand(-1,num_chromosome)\n",
    "        #we assume the copy number for each loci ranges from 0~9\n",
    "        #for samples without WGD\n",
    "        \n",
    "        #y represents if a chromosome have abnormal copy numbers (positions with copy number other than 1)\n",
    "        y=torch.ceil((torch.abs(x-1)).mean(3)/max_copy)\n",
    "        y=y.reshape(x.shape[0],num_chromosome)\n",
    "        y=y.detach()\n",
    "        #Residule representation mentioned in section 3.3.4\n",
    "        #the value for Q_{phi_1}(s,c) is computed as Val_no (the value estimated by the neural net, the residule part)+ y (the empirial estimation) \n",
    "        Val_no=self.Val_noWGD.forward(x)\n",
    "        #chromosome with all 1 copies don't need any CNV and thus will be less likely mutated.\n",
    "        Val_no=-(1-y)*math.log(single_loci_loss)*2+((y*Val_no).sum(1)).reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
    "        #for samples with WGD\n",
    "        #it is similar to the previsou part, where z is an equivalent for y and Val_wgd is an equivalent for Val_no\n",
    "        z=torch.ceil((torch.abs(x-2*(x//2))).mean(3)/max_copy)\n",
    "        z=z.reshape(x.shape[0],num_chromosome)\n",
    "        z=z.detach()\n",
    "        Val_wgd=self.Val_WGD.forward(x)\n",
    "        Val_wgd=-(1-z)*math.log(single_loci_loss)*2+(z*Val_wgd).sum(1).reshape(x.shape[0],1).expand(-1,num_chromosome)-math.log(WGD)\n",
    "        \n",
    "        #combine two NN with switch as defined in Section 3.3.4\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        x=-x\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "#starting point and gain or loss (defined as sp in manuscript) \n",
    "#Used in CNV_NN (which is Q_{phi_2}(s,c,sp) on section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_CNV = [80,120,160,10]\n",
    "activation_cnv=torch.tanh\n",
    "class CNV_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNV_Val,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1, nkernels_CNV[0], (1,7),(1,1),(0,3))\n",
    "        self.conv2=nn.Conv2d(nkernels_CNV[0],nkernels_CNV[1] , (1,7),(1,1), (0,3))\n",
    "        self.conv3=nn.Conv2d(nkernels_CNV[1],nkernels_CNV[2] , (1,7),(1,1), (0,3))\n",
    "        self.conv4=nn.Conv2d(nkernels_CNV[2], nkernels_CNV[3], (1,7),(1,1), (0,3))\n",
    "        self.linear=nn.Linear(nkernels_CNV[3]*chrom_width,2*chrom_width-1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=activation_cnv(self.conv1(x))\n",
    "        x=activation_cnv(self.conv2(x))\n",
    "        x=activation_cnv(self.conv3(x))\n",
    "        x=activation_cnv(self.conv4(x))\n",
    "        x=x.reshape(x.shape[0],nkernels_CNV[3]*chrom_width)\n",
    "        x=self.linear(x)\n",
    "        #number of samples* [(50 regions)*(2(gain or loss))-1] \n",
    "        #Only have 50*2-1=99 output dimensions because we fix the average these output\n",
    "        #The average of them could be arbitrary because of the partitioning\n",
    "        return x\n",
    "\n",
    "#Implemts Q_{phi_2}(s,c,sp) in section 3.3.1\n",
    "#It combines two CNV_Val nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_2}(s,c,sp)\n",
    "class CNV_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNV_NN,self).__init__()\n",
    "        #two network setting\n",
    "        self.CNV_noWGD=CNV_Val()\n",
    "        self.CNV_WGD=CNV_Val()\n",
    "    \n",
    "    def forward(self,x,sigma):\n",
    "        #as in section 3.3.4\n",
    "        #y is the empirical estimation\n",
    "        #Val_no is the redidule representation\n",
    "        y=torch.Tensor(x.shape[0],chrom_width,2)\n",
    "        y[:,:,0]=F.relu(x-1)\n",
    "        y[:,:,1]=F.relu(1-x)\n",
    "        y=y.reshape(x.shape[0],2*chrom_width)\n",
    "        y=y[:,1:(2*chrom_width)]-y[:,0:1].expand(-1,2*chrom_width-1)\n",
    "        y=-y.detach()*math.log(single_loci_loss)\n",
    "        Val_no=self.CNV_noWGD.forward(x.reshape(x.shape[0],1,1,chrom_width))\n",
    "        Val_no=y+Val_no\n",
    "        \n",
    "        z=((torch.abs(x-2*(x//2))).reshape(x.shape[0],chrom_width,1)).expand(-1,-1,2)\n",
    "        z=z.reshape(x.shape[0],2*chrom_width)\n",
    "        z=z[:,1:(2*chrom_width)]-z[:,0:1].expand(-1,2*chrom_width-1)\n",
    "        z=-z.detach()*math.log(single_loci_loss)\n",
    "        Val_wgd=self.CNV_WGD.forward(x.reshape(x.shape[0],1,1,chrom_width))\n",
    "        Val_wgd=z+Val_wgd\n",
    "        #switch\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        return(x)\n",
    "    \n",
    "     \n",
    "    def find_one_cnv(self,chrom,sigma,last_cnv=-1):\n",
    "        #used for finding the cnv during deconvolution\n",
    "        #it is not used in training process\n",
    "        \n",
    "        res_cnv=self.forward(chrom,sigma)\n",
    "        #if there is originally a break point for start\n",
    "        #rule system \n",
    "        break_start=torch.zeros(chrom.shape[0],50,2,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(chrom.shape[0],50,requires_grad=False)\n",
    "        chrom_shift[:,1:]=chrom[:,:49]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_start[:,:,1]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_start[:,0,1]=1\n",
    "        #don't allow lose one copy when copy number equalls 1\n",
    "        break_start[:,:,0]=break_start[:,:,1]\n",
    "        break_start[:,:,0]=break_start[:,:,0]*torch.ceil((chrom/2-0.5)/max_copy)\n",
    "        break_start=break_start.reshape(chrom.shape[0],100)\n",
    "        res_cnv_full=torch.zeros(chrom.shape[0],100)\n",
    "        res_cnv_full[:,1:]=res_cnv\n",
    "        \n",
    "        if last_cnv>0: #not the first step or lossing one chromosome\n",
    "            forbidden=2*last_cnv//2+(1-last_cnv%2)\n",
    "            break_start[0][forbidden]=0\n",
    "        #Prior_rule=break_start\n",
    "        res_cnv_full=res_cnv_full+torch.log(break_start)\n",
    "        #best cnv according to the current Q\n",
    "        cnv_max_val,cnv_max=torch.max(res_cnv_full,1)\n",
    "        #print(res_cnv_full)\n",
    "        return int(cnv_max[0])\n",
    "    \n",
    "\n",
    "\n",
    "#end point\n",
    "#Used in End_Point_NN (which is Q_{phi_3}(s,c,sp,ep) on section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_End = [80,120,240]\n",
    "activation_end=torch.tanh\n",
    "class End_Point_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(End_Point_Val,self).__init__()\n",
    "        self.conv1=nn.Conv2d(2, nkernels_End[0], (1,7),(1,1),(0,3))\n",
    "        self.conv2=nn.Conv2d(nkernels_End[0],nkernels_End[1] , (1,7),(1,1), (0,3))\n",
    "        self.conv3=nn.Conv2d(nkernels_End[1],nkernels_End[2] , (1,7),(1,1), (0,3))\n",
    "        self.linear=nn.Linear(nkernels_End[2]*chrom_width,chrom_width-1)\n",
    "    \n",
    "    def forward(self,old,new):\n",
    "        x=torch.Tensor(old.shape[0],2,1,chrom_width)\n",
    "        x[:,0,0,:]=old\n",
    "        x[:,1,0,:]=new\n",
    "        x=x.detach()\n",
    "        x=activation_end(self.conv1(x))\n",
    "        x=activation_end(self.conv2(x))\n",
    "        x=activation_end(self.conv3(x))\n",
    "        x=x.reshape(x.shape[0],nkernels_End[2]*chrom_width)\n",
    "        x=self.linear(x)\n",
    "        #number of samples* [(chrom_width regions)-1] \n",
    "        #Only have chrom_width-1=49 output dimensions because we fix the average these output\n",
    "        #The average of them could be arbitrary because of the partitioning\n",
    "        return x\n",
    "    \n",
    "#Implemts Q_{phi_3}(s,c,sp,ep) in section 3.3.1\n",
    "#It combines two End_Point_Val nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_3}(s,c,sp,ep)\n",
    "class End_Point_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(End_Point_NN,self).__init__()\n",
    "        #two network setting\n",
    "        self.Val_noWGD=End_Point_Val()\n",
    "        self.Val_WGD=End_Point_Val()\n",
    "    \n",
    "    def forward(self,old,new,sigma):\n",
    "        \n",
    "        y=F.relu((old-1)*(old-new))\n",
    "        y=y[:,1:chrom_width]-y[:,0:1].expand(-1,chrom_width-1)\n",
    "        y=-y.detach()*math.log(single_loci_loss)\n",
    "        Val_no=self.Val_noWGD.forward(old,new)\n",
    "        Val_no=Val_no+y\n",
    "        \n",
    "        z=(old-2*(old//2))*(1-(new-2*(new//2)))\n",
    "        z=z[:,1:chrom_width]-z[:,0:1].expand(-1,chrom_width-1)\n",
    "        z=-z.detach()*math.log(single_loci_loss)\n",
    "        Val_wgd=self.Val_WGD.forward(old,new)\n",
    "        Val_wgd=Val_wgd+z\n",
    "        #switch\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def find_end(self,old,new,sigma,start_loci,cnv,valid):\n",
    "        #used for finding the end during loading data\n",
    "        res_end=self.forward(old,new,sigma)\n",
    "        \n",
    "        break_end=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:49]=old[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(old-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        \n",
    "        for i in range(old.shape[0]):\n",
    "            #can't end before starting point\n",
    "            break_end[i,:int(start_loci[i])]=0*break_end[i,:int(start_loci[i])]\n",
    "            #don't allow lose one copy when copy number equalls 1\n",
    "            if(cnv[i]<0.5):\n",
    "                j=int(start_loci[i])+1\n",
    "                while(j<chrom_width):\n",
    "                    if(old[i][j]<1.5):\n",
    "                        break\n",
    "                    j=j+1\n",
    "                break_end[i,j:chrom_width]=0*break_end[i,j:chrom_width]\n",
    "        res_end_full=torch.zeros(old.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        \n",
    "        #Prior_rule=break_end\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max=torch.max(res_end_full,1)\n",
    "        print(break_end,res_end_full)\n",
    "        return end_max+1\n",
    "    \n",
    "    \n",
    "    def find_one_end(self,old,new,sigma,start,cnv):\n",
    "        #used for finding the end during deconvolution\n",
    "        res_end=self.forward(old,new,sigma)\n",
    "        \n",
    "        break_end=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:chrom_width-1]=old[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(old-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        #can't end before starting point\n",
    "        break_end[0,:start]=0*break_end[0,:start]\n",
    "        #don't allow lose one copy when copy number equalls 1\n",
    "        if(cnv<0.5):\n",
    "            j=start+1\n",
    "            while(j<chrom_width):\n",
    "                if(old[0][j]<1.5):\n",
    "                    break\n",
    "                j=j+1\n",
    "            break_end[0,j:chrom_width]=0*break_end[0,j:chrom_width]\n",
    "        res_end_full=torch.zeros(old.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        #Prior_rule=break_end\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max=torch.max(res_end_full,1)\n",
    "        end_max=int(end_max[0])\n",
    "        return end_max+1\n",
    "        \n",
    "\n",
    "#combine all separate networks\n",
    "#add Rule system\n",
    "\n",
    "#calculating the softmax\n",
    "#prevent inf when taking log(exp(x))\n",
    "#log_exp is always gonna be between 1 and the total number of elements\n",
    "def Soft_update(val1,soft1,val2,soft2):\n",
    "    bias=val1.clone()\n",
    "    log_exp=soft1.clone()\n",
    "    set1=[torch.ge(val1,val2)]\n",
    "    bias[set1]=val1[set1]\n",
    "    log_exp[set1]=soft1[set1]+soft2[set1]*torch.exp(val2[set1]-val1[set1])\n",
    "    set2=[torch.lt(val1,val2)]\n",
    "    bias[set2]=val2[set2]\n",
    "    log_exp[set2]=soft2[set2]+soft1[set2]*torch.exp(val1[set2]-val2[set2])\n",
    "    return bias,log_exp\n",
    "\n",
    "\n",
    "\n",
    "#Combine all the separate modules mentioned above\n",
    "#Implementation of Q(s,a)\n",
    "class Q_learning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_learning,self).__init__()\n",
    "        self.switch=WGD_Net()\n",
    "        #the output refer to Q_{\\phi_1}(s,c)\n",
    "        self.Chrom_model=Chrom_NN()\n",
    "        #the output refer to Q_{\\phi_2}(s,c,sp)\n",
    "        self.CNV=CNV_NN()\n",
    "        #the output refer to Q_{\\phi_3}(s,sp,c,ep)\n",
    "        self.End=End_Point_NN()\n",
    "    \n",
    "    \n",
    "    def forward(self,state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid):\n",
    "        '''\n",
    "        computing the final advantage(loss) used for training\n",
    "        loss in Thereom1\n",
    "        state: s in Q(s,a)\n",
    "        next_state: s' in softmaxQ(s',a')\n",
    "        Chr,cnv,end_loci: a in Q(s,a)\n",
    "        chrom,chrom_new,start_loci,end_loci: intermediate results from s,a, which is preprossed to make computation faster\n",
    "            e.g. chrom is CNP of the Chr(part of a) from the state(s)\n",
    "            They could be seen as a mapping without parameters to learn:f(s,a)\n",
    "        valid: a boolean array, indicating if a training sample is valid (e.g. have non negative copy numbers for all loci)\n",
    "        '''\n",
    "        \n",
    "        #computing softmaxQ(s',a')\n",
    "        #It is a tradition in RL that gradient does not backpropogate through softmaxQ(s',a'), but only through Q(s,a) to make convergence faster\n",
    "        #there is no theoritical guarantee behind, and it is only a practical trick\n",
    "        sigma_next=self.switch(next_state)\n",
    "        x,y=self.Softmax(next_state,sigma_next)\n",
    "        x=x+torch.log(y)\n",
    "        #computing r(s,a)\n",
    "        x=x+Reward(start_loci,end_loci)\n",
    "        x=x.detach()\n",
    "        \n",
    "        #computing Q(s,a)\n",
    "        sigma=self.switch.forward(state)\n",
    "        if counter_global<3e6:\n",
    "            sigma=sigma.detach()\n",
    "        #Q_{phi_1}(s,c)\n",
    "        res_chrom=self.Chrom_model.forward(state,sigma)\n",
    "        \n",
    "        #Q_{phi_2}(s,c,sp)\n",
    "        res_cnv=self.CNV.forward(chrom,sigma)\n",
    "        #if there is originally a break point for start\n",
    "        #real world constraint as described in section 3.3.2\n",
    "        #only allow starting points (sp) to be the break points of CNP\n",
    "        break_start=torch.zeros(state.shape[0],chrom_width,2,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,1:]=chrom[:,:(chrom_width-1)]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_start[:,:,1]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_start[:,0,1]=1\n",
    "        #don't allow lose one copy when copy number equals 0, otherwise there is going to be negative copy numbers\n",
    "        break_start[:,:,0]=break_start[:,:,1]\n",
    "        break_start[:,:,0]=break_start[:,:,0]*torch.ceil((chrom/2-0.5)/max_copy)\n",
    "        break_start=break_start.reshape(state.shape[0],2*chrom_width)\n",
    "        res_cnv_full=torch.zeros(state.shape[0],2*chrom_width)\n",
    "        res_cnv_full[:,1:]=res_cnv\n",
    "        res_cnv_full=res_cnv_full+torch.log(break_start)\n",
    "        \n",
    "        #Q_{phi_2}(s,c,sp)-softmax(Q_{phi_2}(s,c,sp)) as described in section 3.3.1\n",
    "        cnv_max_val,cnv_max=torch.max(res_cnv_full,1)\n",
    "        cnv_softmax=res_cnv_full-cnv_max_val.reshape(state.shape[0],1).expand(-1,2*chrom_width)\n",
    "        cnv_softmax=torch.exp(cnv_softmax).sum(1)\n",
    "        x=x+cnv_max_val+torch.log(cnv_softmax)\n",
    "        \n",
    "        #Q_{phi_3}(s,c,sp,ep)\n",
    "        res_end=self.End.forward(chrom,chrom_new,sigma)\n",
    "        #if there is originally a break point for end\n",
    "        #and if this is after the starting point\n",
    "        #real world constraint in section 3.3.2\n",
    "        break_end=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:(chrom_width-1)]=chrom[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        for i in range(state.shape[0]):\n",
    "            #can't end before starting point\n",
    "            break_end[i,:int(start_loci[i])]=0*break_end[i,:int(start_loci[i])]\n",
    "            #don't allow lose one copy when copy number equalls 1\n",
    "            if(cnv[i]<0.5):\n",
    "                j=int(start_loci[i])+1\n",
    "                while(j<chrom_width):\n",
    "                    if(chrom[i][j]<1.5):\n",
    "                        break\n",
    "                    j=j+1\n",
    "                break_end[i,j:chrom_width]=0*break_end[i,j:chrom_width]\n",
    "            \n",
    "        res_end_full=torch.zeros(state.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        \n",
    "        #real world constraint described in section 3.3.2\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max_temp=torch.max(res_end_full,1)\n",
    "        end_softmax=res_end_full-end_max_val.reshape(state.shape[0],1).expand(-1,chrom_width)\n",
    "        end_softmax=torch.exp(end_softmax).sum(1)\n",
    "        #Q_{phi_3}(s,c,sp,ep)-softmax(Q_{phi_3}(s,c,sp,ep)) as described in section 3.3.1\n",
    "        x=x+end_max_val+torch.log(end_softmax)\n",
    "        \n",
    "        for i in range(state.shape[0]):\n",
    "            if valid[i]>0.5:#check validity to prevent inf-inf which ends in nan\n",
    "                x[i]=x[i]-res_chrom[i][int(Chr[i])]\n",
    "                cnv_rank=int(start_loci[i]*2+cnv[i])\n",
    "                x[i]=x[i]-res_cnv_full[i][cnv_rank]\n",
    "                end_rank=int(end_loci[i]-1)\n",
    "                x[i]=x[i]-res_end_full[i][end_rank]\n",
    "        \n",
    "        #remove training data which include invalid actions\n",
    "        x=x*valid\n",
    "        #return avdantage as well as a best cnv and sigma used for generating training data\n",
    "        #used for training in the next step\n",
    "        return x,cnv_max,sigma,res_chrom,res_cnv_full,res_end_full\n",
    "     \n",
    "    def Softmax(self,next_state,sigma):\n",
    "        #compute softmax_{a'} Q(s',a')\n",
    "        x=self.Chrom_model.forward(next_state,sigma)\n",
    "        max_chrom=torch.max(x,1)[0]\n",
    "        softmax_chrom=x-max_chrom.reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
    "        softmax_chrom=torch.exp(softmax_chrom).sum(1)\n",
    "        #special action END\n",
    "        #all the remaining abnormal loci are treated to be caused by several independent single locus copy number changes\n",
    "        end_val=torch.sum(torch.abs(next_state-1),(1,2,3))*math.log(single_loci_loss)\n",
    "        max_chrom,softmax_chrom=Soft_update(max_chrom,softmax_chrom,end_val,torch.ones(x.shape[0]))\n",
    "        #if there is a WGD followed immediately\n",
    "        for i in range(x.shape[0]):\n",
    "            #real world constraint as described in section 3.3.2\n",
    "            #do not allow (reversing) WGD when the CNP contain odd numbers for some loci\n",
    "            if (not torch.any(next_state[i]-2*torch.floor(next_state[i]/2)>0.5)) and torch.any(next_state[i]>0.5):\n",
    "                sigma_wgd=self.switch(torch.floor(next_state[i:(i+1)]/2))\n",
    "                sigma_wgd=sigma_wgd.detach()\n",
    "                wgd_val,wgd_soft=self.Softmax(torch.floor(next_state[i:(i+1)]/2),sigma_wgd)\n",
    "                max_chrom[i],softmax_chrom[i]=Soft_update(torch.ones(1)*max_chrom[i],torch.ones(1)*softmax_chrom[i],torch.ones(1)*wgd_val,torch.ones(1)*wgd_soft)\n",
    "        \n",
    "        return max_chrom,softmax_chrom\n",
    "  \n",
    "\n",
    "#Minimum example\n",
    "if __name__ == \"__main__\":\n",
    "    #test different parts separately\n",
    "    '''\n",
    "    switch=WGD_Net()\n",
    "    Chrom_model=Chrom_NN()\n",
    "    print(Chrom_model)\n",
    "    #test the structure of permutation invariant structure\n",
    "    x=torch.ones(3,1,num_chromosome,50)\n",
    "    x[0][0][0][0:50]=2\n",
    "    x[2][0][1][0:50]=2\n",
    "    prob=switch.forward(x)\n",
    "    print(prob)\n",
    "    res=Chrom_model.forward(x,prob)\n",
    "    print(res)\n",
    "    res=-float('inf')\n",
    "    res=torch.LongTensor(3)\n",
    "    \n",
    "    print(torch.log(res.type(torch.DoubleTensor)))\n",
    "    #CNV\n",
    "    CNV=CNV_NN()\n",
    "    res=CNV.forward(x[:,0,0,0:50],prob)\n",
    "    print(CNV)\n",
    "    print(res.shape)\n",
    "    #END\n",
    "    End=End_Point_NN()\n",
    "    res=End.forward(x[:,0,0,0:50],x[:,0,0,0:50]+1,prob)\n",
    "    print(End)\n",
    "    print(res.shape)\n",
    "    '''\n",
    "    #test Q-learning\n",
    "    x=torch.ones(3,1,num_chromosome,50)\n",
    "    y=torch.ones(3,1,num_chromosome,50)\n",
    "    x[0][0][0][0:50]=2\n",
    "    x[2][0][1][0:50]=2\n",
    "    chrom=x[:,0,0,:]\n",
    "    chrom_new=y[:,0,0,:]\n",
    "    Chr=torch.zeros(3)\n",
    "    cnv=torch.ones(3)\n",
    "    start_loci=torch.zeros(3)\n",
    "    end_loci=torch.ones(3)*50\n",
    "    valid=torch.ones(3)\n",
    "    Q_model=Q_learning()\n",
    "    #res,cnv_max,sigma,t,t2,t3=Q_model.forward(x,y,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid)\n",
    "    #print(res)\n",
    "    #print(cnv_max)\n",
    "    #loss=res.pow(2).mean()\n",
    "    #print(loss)\n",
    "    #loss.backward()\n",
    "    #params = list(Q_model.parameters())\n",
    "    #print(params[0].grad[0])\n",
    "    #print(Q_model.switch.conv1.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate data for testing\n",
    "def Simulate_data(batch_size=15,Number_of_step=70):\n",
    "    state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "    next_state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "    Chr=torch.ones(batch_size,requires_grad=False).type(torch.LongTensor)\n",
    "    step=torch.zeros(batch_size,requires_grad=False)\n",
    "    wgd=torch.zeros(batch_size,requires_grad=False)\n",
    "    valid=torch.ones(batch_size,requires_grad=False)\n",
    "    \n",
    "    start_loci=torch.randint(high=chrom_width,size=(batch_size,),requires_grad=False)\n",
    "    end_loci=torch.LongTensor(batch_size)\n",
    "    cnv=torch.ones(batch_size,requires_grad=False)\n",
    "    chrom=torch.Tensor(batch_size,chrom_width)\n",
    "    chrom_new=torch.Tensor(batch_size,chrom_width)\n",
    "    \n",
    "    Chr_truth=torch.zeros(batch_size,Number_of_step)\n",
    "    CNV_truth=torch.zeros(batch_size,Number_of_step)\n",
    "    End_truth=torch.zeros(batch_size,Number_of_step)\n",
    "    \n",
    "    \n",
    "    step_counter=0\n",
    "    while(step_counter<Number_of_step):\n",
    "        for i in range(batch_size):\n",
    "            #reset valid after they have been checked\n",
    "            valid[i]=1\n",
    "            start_loci[i]=torch.randint(high=chrom_width,size=(1,))[0]\n",
    "            end_loci[i]=1+torch.randint(low=start_loci[i],high=50,size=(1,))[0]\n",
    "            if torch.rand(1)[0]>0.2:\n",
    "                Chr[i]=torch.randint(high=num_chromosome,size=(1,))[0]\n",
    "            #adding probability to sample chromosomal changes during training\n",
    "            if torch.rand(1)[0]>0.8:\n",
    "                start_loci[i]=0\n",
    "                end_loci[i]=chrom_width\n",
    "            #cnv\n",
    "            if torch.rand(1)[0]>0.7:\n",
    "                cnv[i]=0\n",
    "            #modifying cnp\n",
    "            prob_wgd=0.1/(1+math.exp(-step[i]+15))\n",
    "            #wgd          \n",
    "            if  (torch.rand(1)[0]<prob_wgd and wgd[i]<1):\n",
    "                wgd[i]=1\n",
    "                state[i]=state[i]*2\n",
    "                next_state[i]=next_state[i]*2\n",
    "                Chr_truth[i][int(step[i])]=-1\n",
    "                CNV_truth[i][int(step[i])]=-1\n",
    "                End_truth[i][int(step[i])]=-1\n",
    "                step[i]=step[i]+1\n",
    "                continue\n",
    "                #adding cnv effect\n",
    "                #increasing copies when no wgd\n",
    "                #decreasing copies when wgd\n",
    "            if wgd[i]>0.5:\n",
    "                cnv[i]=1-cnv[i]\n",
    "            state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]=state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]-(cnv[i]-0.5)*2\n",
    "            chrom[i]=state[i][0][Chr[i]][:]\n",
    "            #reverse effect on chrom_new\n",
    "            chrom_new[i]=state[i][0][Chr[i]][:]\n",
    "            chrom_new[i][(start_loci[i]):]=chrom_new[i][(start_loci[i]):]+(cnv[i]-0.5)*2\n",
    "            #not going to negative values\n",
    "            if(torch.any(state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]< -0.5)):\n",
    "                valid[i]=0\n",
    "            #not joining breakpoints\n",
    "            if(start_loci[i]>0.5 and torch.abs(chrom[i][start_loci[i]]-chrom[i][start_loci[i]-1])<0.5):\n",
    "                valid[i]=0\n",
    "            if(end_loci[i]<chrom_width-0.5 and torch.abs(chrom[i][end_loci[i]-1]-chrom[i][end_loci[i]])<0.5):\n",
    "                valid[i]=0\n",
    "            if valid[i]>0 :\n",
    "                next_state[i]=state[i].clone()\n",
    "                Chr_truth[i][int(step[i])]=Chr[i]+1\n",
    "                CNV_truth[i][int(step[i])]=start_loci[i]*2+cnv[i]+1\n",
    "                End_truth[i][int(step[i])]=end_loci[i]\n",
    "                step[i]=step[i]+1\n",
    "                \n",
    "            #stay to further train the current step\n",
    "            #or resample another action\n",
    "            else:\n",
    "                state[i]=next_state[i].clone()\n",
    "        step_counter=step_counter+1\n",
    "    for i in range(batch_size):\n",
    "        temp_Chr=Chr_truth.clone()\n",
    "        temp_CNV=CNV_truth.clone()\n",
    "        temp_End=End_truth.clone()\n",
    "        for j in range(int(step[i].item())):\n",
    "            Chr_truth[i][int(step[i].item())-1-j]=temp_Chr[i][j]\n",
    "            CNV_truth[i][int(step[i].item())-1-j]=temp_CNV[i][j]\n",
    "            End_truth[i][int(step[i].item())-1-j]=temp_End[i][j]\n",
    "    return Chr_truth,CNV_truth,End_truth,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  7.,  7.,  8.,  9.,  9.,  9., 11., 11., 12., 14., 14., 15., 17.,\n",
      "         18., 18., 18., 19., 20., 20., 26., 26., 28., 32., 32., 33., 35., 36.,\n",
      "         37., 40., 41., 42., -1.,  8.,  9., 14., 17., 18., 18., 18., 19., 19.,\n",
      "         20., 26., 26., 26., 26., 27., 27., 28., 28., 28., 32., 32., 33., 42.,\n",
      "         23., 36., 41.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.]]) tensor([[23., 23., 19., 19., 19.,  7., 36., 20., 20., 11., 11., 35., 35.,  7.,\n",
      "         20., 40., 18., 18., 41., 12.,  8., 20., 28.,  4.,  9., 17., 17., 17.,\n",
      "         35., 33., 32., 32., 37., 14., 14., 42., 26., 15., -1., 28., 28., 27.,\n",
      "         27.,  9., 26., 26., 26., 17., 18.,  0.]])\n",
      "tensor([[ 50.,  96.,  62.,  76.,   2.,  81.,  51.,  94.,   2.,  62.,   2.,  99.,\n",
      "          40.,  75.,   2.,  78.,  10.,  80.,  64.,  22.,   3.,  83.,  54.,  38.,\n",
      "         100.,  14.,  42.,   2.,  62.,  26.,   2.,  22.,  -1.,  75.,   1.,   1.,\n",
      "           1.,   1.,   1.,  77.,   2., 100.,  63.,  89.,   1.,   1.,  83.,   5.,\n",
      "           1.,  53.,  27.,   1.,  37.,  99.,  13.,  21.,   2.,   1.,   1.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.]]) tensor([[ 2.,  2., 79.,  2.,  2., 96.,  1.,  2.,  1.,  2., 73.,  2.,  1., 62.,\n",
      "         51., 26.,  1., 76.,  1., 62., 75., 22., 53., 50.,  1.,  2., 76.,  1.,\n",
      "         42., 13., 98., 37., 62.,  1., 86., 21.,  4., 40., -1., 27.,  1.,  1.,\n",
      "          5., 51.,  1., 83.,  1.,  1.,  1.,  0.]])\n",
      "tensor([[50., 49., 40., 45., 25., 50., 40., 50., 36., 38., 42., 50., 36., 50.,\n",
      "          4., 50., 37., 49., 33., 25., 41., 44., 34., 48., 50., 48., 47., 50.,\n",
      "         39., 45., 50., 23., -1., 45., 40., 42., 37.,  4., 37., 50., 39., 50.,\n",
      "         33., 50., 50.,  1., 47., 50., 49., 34., 50., 37., 48., 50., 48., 23.,\n",
      "         50., 50., 50.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.]]) tensor([[50., 50., 49., 50., 50., 49., 50., 50., 50., 50., 46., 50., 50., 40.,\n",
      "         33., 45., 50., 38., 50., 38., 45., 31., 34., 50., 50., 50., 50., 50.,\n",
      "         47., 48., 49., 50., 39., 50., 49., 23., 44., 36., -1., 37., 50., 50.,\n",
      "         49., 40., 50., 47., 50., 50.,  4.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "#Deconvolution.py\n",
    "#used for deconvolution of CNP history\n",
    "\n",
    "def Deconvolute(model,cnp,Chr,CNV,End):\n",
    "    '''\n",
    "    Deconvolution samples the maximum action in a greedy way\n",
    "    model:the trained Q-learning model\n",
    "    cnp: the input CNP, shape: Number of CNP,1,44 (#Chr),50 (#regions for one chromosome)\n",
    "    Chr,CNV,END: output tensor,shape: Number of CNP, maximum length of history\n",
    "    output: for Chr: -1 indicates WGD, 0 indicates no action, 1~44 the chromosome\n",
    "            for CNV: only valid if Chr is not -1 or 0\n",
    "                     indicates the starting point (CNV//2) and the type of CNV (CNV%2==1 for gain and CNV%2==0 for loss)\n",
    "            for End: only valid if Chr is not -1 or 0\n",
    "                     indicates the end point for a CNV.\n",
    "    '''\n",
    "    max_step=int(Chr.shape[1])\n",
    "    \n",
    "    for i in range(cnp.shape[0]):\n",
    "        flag=False\n",
    "        current_cnp=cnp[i:(i+1)]\n",
    "        sigma=model.switch(current_cnp)\n",
    "        if sigma[0]<0.5:\n",
    "            flag=True\n",
    "        step=0\n",
    "        while(step<max_step):\n",
    "            #it is also possible to manually set the switch if deemed necessary\n",
    "            sigma=model.switch(current_cnp)\n",
    "            #hard classification\n",
    "            if not flag:\n",
    "                sigma=torch.ceil(sigma)\n",
    "            #else:\n",
    "            #    sigma=torch.zeros_like(sigma)\n",
    "            #print(sigma)\n",
    "            res_chrom=model.Chrom_model(current_cnp,sigma)\n",
    "            #print(res_chrom)\n",
    "            #find the chromosome with the maximum probability\n",
    "            val,temp_Chr=res_chrom.max(1)\n",
    "            temp_Chr=int(temp_Chr)\n",
    "            Chr[i][step]=temp_Chr+1\n",
    "            #WGD\n",
    "            if (not torch.any(current_cnp-2*torch.floor(current_cnp/2)>0.5)) and torch.any(current_cnp>0.5):\n",
    "                sigma_wgd=model.switch(torch.floor(current_cnp/2))\n",
    "                res_chrom_wgd=model.Chrom_model(torch.floor(current_cnp/2),sigma_wgd)\n",
    "                val_wgd,temp=res_chrom_wgd.max(1)\n",
    "                if not flag:#val_wgd>=val and not flag:\n",
    "                    val=val_wgd\n",
    "                    Chr[i][step]=-1\n",
    "                    CNV[i][step]=-1\n",
    "                    End[i][step]=-1\n",
    "                    flag=True\n",
    "            #special action END\n",
    "            val_end=torch.sum(torch.abs(current_cnp-1))*math.log(single_loci_loss)\n",
    "            if val_end>=val:\n",
    "                val=val_end\n",
    "                Chr[i][step]=0\n",
    "                #print(val_end)\n",
    "                break\n",
    "            #if WGD\n",
    "            if Chr[i][step]< -0.5:\n",
    "                current_cnp=(current_cnp/2).floor()\n",
    "                flag=True\n",
    "            #if not WGD or END\n",
    "            elif Chr[i][step]>0.5:\n",
    "                #find best CNV\n",
    "                chrom=current_cnp[:,0,temp_Chr,:]\n",
    "                last_step=-1\n",
    "                if step>1 and Chr[i][step]==Chr[i][step-1]:\n",
    "                    last_step=int(CNV[i][step-1].item())\n",
    "                CNV[i][step]=model.CNV.find_one_cnv(chrom,sigma,last_step)\n",
    "                cnv_temp=int(CNV[i][step]%2)\n",
    "                start_temp=int(CNV[i][step]//2)\n",
    "                #find best End\n",
    "                chrom_new=chrom.clone()\n",
    "                chrom_new[:,start_temp:]=chrom_new[:,start_temp:]+(cnv_temp-0.5)*2\n",
    "                End[i][step]=model.End.find_one_end(chrom,chrom_new,sigma,start_temp,cnv_temp)\n",
    "                #updata cnp\n",
    "                #print(chrom)\n",
    "                #print(start_temp,End[i][step],cnv_temp)\n",
    "                \n",
    "                current_cnp[:,0,temp_Chr,start_temp:int(End[i][step])]=current_cnp[:,0,temp_Chr,start_temp:int(End[i][step])]+(cnv_temp-0.5)*2\n",
    "                \n",
    "            step=step+1\n",
    "    return Chr,CNV,End,current_cnp\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    model = Q_learning()\n",
    "    model.load_state_dict(torch.load(\"/data/suzaku/ted/HOME/model_tanh\"))\n",
    "    model.eval()\n",
    "    counter_global=torch.randint(10000,(1,))[0]\n",
    "    #loading simulated data\n",
    "    num_step=50\n",
    "    Chr_truth,CNV_truth,End_truth,state=Simulate_data(batch_size=1,Number_of_step=num_step)\n",
    "    Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "    CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "    End=torch.zeros(state.shape[0],num_step*2)\n",
    "    state_copy=state.clone()\n",
    "    #deconvolution\n",
    "    Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "    CNV[Chr>0.5]=CNV[Chr>0.5]+1\n",
    "    print(Chr,Chr_truth)\n",
    "    #check if the model picks the correct chromosome\n",
    "    print(CNV,CNV_truth)\n",
    "    print(End,End_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(),\"/data/suzaku/ted/HOME/model_tanh2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  2.,  3.,  3.,  3.,  3.,  3.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "          4.,  4.,  4.,  6.,  6.,  6.,  6.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "          7.,  8.,  8.,  8.,  9., 10., 11., 11., 11., 11., 12., 12., 12., 12.,\n",
      "         13., 14., 15., 15., 15., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
      "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
      "         16., 16., 16., 16., 17., 17., 17., 17., 17., 17., 18., 19., 20., 20.,\n",
      "         20., 20., 21., 22., 23., 23., 23., 23., 24., 25., 26., 27., 27., 27.,\n",
      "         27., 27.]]) tensor([[17., 17., 17., 34., 16.,  5., 44.,  8.,  2.,  7.,  6., -1.,  7., 29.,\n",
      "         15., 40., 20., 44.,  4., 37.,  3.,  3., 33., 44.,  4., 12., 40., 27.,\n",
      "         27.,  6., 11., 43., 43., 16., 16., 16., 41., 28., 28., 28., 23., 16.,\n",
      "         37., 33., 33., 32., 41., 41., 41., 41.]])\n",
      "tensor([[ 1.,  1., 71.,  1.,  1.,  1.,  1.,  1.,  1., 61., 61., 61., 99., 99.,\n",
      "         99., 99., 99.,  1., 13., 13., 13.,  1., 21., 21., 21., 59., 59., 59.,\n",
      "         59.,  1., 67., 67.,  1.,  1.,  1., 11., 11., 11.,  1.,  9.,  9.,  9.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  3.,  3.,  3.,  3.,\n",
      "          3.,  3.,  3., 23., 23., 23., 23., 23., 23., 23., 23., 93., 93., 93.,\n",
      "         93., 93., 93., 93.,  1., 31., 31., 55., 61., 61.,  1.,  1.,  1.,  5.,\n",
      "          5.,  5.,  1.,  1.,  1., 81., 81., 81.,  1.,  1.,  1.,  1., 21., 21.,\n",
      "         21., 33.]]) tensor([[31., 48., 61., 62., 23.,  2., 14., 67., 60., 59., 62., -1., 21., 81.,\n",
      "          1., 55.,  5.,  1., 61., 67.,  1.,  1., 61., 93., 99.,  9., 75., 21.,\n",
      "         33., 13., 11., 49., 21.,  3.,  1.,  1.,  1., 93., 87.,  7., 81., 93.,\n",
      "         99., 71.,  5., 95., 49., 85.,  3., 59.]])\n",
      "tensor([[50., 29., 50., 50., 50., 50., 50., 50., 30., 49., 49., 49., 50., 50.,\n",
      "         50., 50., 50.,  6., 25., 25., 30., 10., 29., 29., 29., 36., 42., 42.,\n",
      "         50., 33., 49., 50., 50., 50.,  5., 44., 44., 50.,  4., 22., 22., 50.,\n",
      "         50., 50., 50., 50., 50.,  1.,  1.,  1.,  1.,  1., 11., 11., 11., 11.,\n",
      "         11., 11., 11., 33., 44., 44., 46., 46., 46., 46., 46., 48., 48., 50.,\n",
      "         50., 50., 50., 50., 15., 23., 24., 30., 37., 50., 50., 50.,  2., 44.,\n",
      "         44., 50., 50., 50., 40., 43., 43., 50., 50., 50., 50., 10., 16., 16.,\n",
      "         16., 27.]]) tensor([[24., 27., 37., 46., 33., 50., 19., 49., 35., 36., 50., -1., 42., 46.,\n",
      "         50., 34., 44., 50., 50., 44., 50., 50., 43., 49., 50., 22., 40., 27.,\n",
      "         41., 25., 44., 48., 29., 44., 50., 50., 50., 50., 48., 49., 43., 48.,\n",
      "         50., 45., 47., 49., 37., 44., 18., 37.]])\n"
     ]
    }
   ],
   "source": [
    "#Heuristic method one\n",
    "def Heur1(cnp,Chr,CNV,End):\n",
    "    max_step=int(Chr.shape[1])\n",
    "    #treat each sample differently\n",
    "    for i in range(cnp.shape[0]):\n",
    "        current_cnp=cnp[i:(i+1)]\n",
    "        current_locus=0\n",
    "        step=0\n",
    "        while(step<max_step and current_locus<num_chromosome*chrom_width):\n",
    "            current_Chr=current_locus//chrom_width\n",
    "            current_start=current_locus%chrom_width\n",
    "            #no CNA\n",
    "            if current_cnp[0][0][current_Chr][current_start]==1:\n",
    "                current_locus+=1\n",
    "                continue\n",
    "            #determine if it is a loss or gain\n",
    "            elif current_cnp[0][0][current_Chr][current_start]<1:\n",
    "                current_cnv=1\n",
    "            else:\n",
    "                current_cnv=-1\n",
    "            current_end=current_start\n",
    "            #find the end point\n",
    "            while(current_end<50):\n",
    "                if current_cnp[0][0][current_Chr][current_end]!=current_cnp[0][0][current_Chr][current_start]:\n",
    "                    break\n",
    "                current_end+=1\n",
    "            current_cnp[0,0,current_Chr,current_start:current_end]+=current_cnv\n",
    "            Chr[i][step]=current_Chr+1\n",
    "            CNV[i][step]=2*current_start+(current_cnv+1)//2+1\n",
    "            End[i][step]=current_end\n",
    "            step=step+1\n",
    "    return Chr,CNV,End,current_cnp\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #loading simulated data\n",
    "    num_step=50\n",
    "    Chr_truth,CNV_truth,End_truth,state=Simulate_data(batch_size=1,Number_of_step=num_step)\n",
    "    Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "    CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "    End=torch.zeros(state.shape[0],num_step*2)\n",
    "    state_copy=state.clone()\n",
    "    #deconvolution\n",
    "    Chr,CNV,End,state=Heur1(state,Chr,CNV,End)\n",
    "    print(Chr,Chr_truth)\n",
    "    #check if the model picks the correct chromosome\n",
    "    print(CNV,CNV_truth)\n",
    "    print(End,End_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor([[ 2.,  2.,  2.,  2.,  2.,  5.,  9.,  9.,  9.,  9., 10., 16., 18., 20.,\n",
      "         20., 20., 20., 20., 20., 21., 21., 22., 22., 24., 25., 26., 27., 27.,\n",
      "         28., 31., 35., 35., 35., 38., 39., 39., 40., 42., 42., 43., 44., 44.,\n",
      "         -1.,  1., 10., 12., 15., 21., 22., 26., 27., 31., 33., 36., 37., 37.,\n",
      "         40., 44.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.]])\n",
      "tensor([[  2.,   1.,  26.,  83.,  92.,  10.,   2.,   1.,  28.,  95.,  96.,  99.,\n",
      "          43.,   2.,   1.,   4.,  11.,  56.,  97.,  56.,  68.,  55.,  87., 100.,\n",
      "          16.,  81.,  55.,  62.,  86.,  43.,  11.,  54.,  91.,  69.,  68.,  93.,\n",
      "          64.,  46.,  59.,  81.,  54.,  59.,  -1.,  92.,  88.,  56.,  98.,  60.,\n",
      "          63.,  16.,  66.,  86.,  48.,  40.,   2., 100.,  80.,  14.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.]])\n",
      "tensor([[50., 12., 41., 45., 50., 46., 50., 13., 47., 50., 49., 50., 30., 50.,\n",
      "          1.,  5., 11., 36., 50., 29., 46., 31., 45., 50.,  9., 45., 30., 32.,\n",
      "         46., 24., 26., 45., 46., 38., 44., 49., 36., 27., 36., 45., 29., 49.,\n",
      "         -1., 47., 44., 41., 50., 33., 43., 24., 41., 48., 34., 36.,  5., 50.,\n",
      "         49., 26.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "#Heuristic 2\n",
    "def Heur2(cnp,Chr,CNV,End):\n",
    "    max_step=int(Chr.shape[1])\n",
    "    \n",
    "    for i in range(cnp.shape[0]):\n",
    "        current_cnp=cnp[i:(i+1)]\n",
    "        current_locus=0\n",
    "        step=0\n",
    "        flag=False\n",
    "        #determine if this is a CNP with WGD\n",
    "        if current_cnp.mean()<1.7:\n",
    "            flag=True\n",
    "        while(step<max_step and ((not flag) or current_locus<num_chromosome*chrom_width)):\n",
    "            #all loci contain even copy number, do a reverse of WGD\n",
    "            if current_locus==num_chromosome*chrom_width:\n",
    "                flag=True\n",
    "                current_locus=0\n",
    "                current_cnp//=2\n",
    "                Chr[i][step]=-1\n",
    "                CNV[i][step]=-1\n",
    "                End[i][step]=-1\n",
    "                step+=1\n",
    "                continue\n",
    "            current_Chr=current_locus//chrom_width\n",
    "            #determine if it is a whole chromosome change\n",
    "            baseline=2\n",
    "            if flag:\n",
    "                baseline=1\n",
    "            if  current_cnp[0][0][current_Chr].mean()>1.5*baseline:\n",
    "                current_cnp[0][0][current_Chr]-=1\n",
    "                Chr[i][step]=current_Chr+1\n",
    "                CNV[i][step]=1\n",
    "                End[i][step]=50\n",
    "                current_locus=current_Chr*chrom_width\n",
    "                step+=1\n",
    "                continue\n",
    "            elif  current_cnp[0][0][current_Chr].mean()<0.5*baseline:\n",
    "                current_cnp[0][0][current_Chr]+=1\n",
    "                Chr[i][step]=current_Chr+1\n",
    "                CNV[i][step]=2\n",
    "                End[i][step]=50\n",
    "                current_locus=current_Chr*chrom_width\n",
    "                step+=1\n",
    "                continue\n",
    "            current_start=current_locus%chrom_width\n",
    "            if (flag and current_cnp[0][0][current_Chr][current_start]==1) or ((not flag) and current_cnp[0][0][current_Chr][current_start]%2==0):\n",
    "                current_locus+=1\n",
    "                continue\n",
    "            elif current_cnp[0][0][current_Chr][current_start]<2:\n",
    "                current_cnv=1\n",
    "            else:\n",
    "                current_cnv=-1\n",
    "            current_end=current_start\n",
    "            while(current_end<50):\n",
    "                if current_cnp[0][0][current_Chr][current_end]!=current_cnp[0][0][current_Chr][current_start]:\n",
    "                    break\n",
    "                current_end+=1\n",
    "            current_cnp[0,0,current_Chr,current_start:current_end]+=current_cnv\n",
    "            Chr[i][step]=current_Chr+1\n",
    "            CNV[i][step]=2*current_start+(current_cnv+1)//2+1\n",
    "            End[i][step]=current_end\n",
    "            step=step+1\n",
    "    return Chr,CNV,End,current_cnp\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #loading simulated data\n",
    "    num_step=50\n",
    "    Chr_truth,CNV_truth,End_truth,state=Simulate_data(batch_size=1,Number_of_step=num_step)\n",
    "    Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "    CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "    End=torch.zeros(state.shape[0],num_step*2)\n",
    "    #deconvolution\n",
    "    Chr,CNV,End,state=Heur2(state,Chr,CNV,End)\n",
    "    print((state-1).abs().sum())\n",
    "    print(Chr)\n",
    "    #check if the model picks the correct chromosome\n",
    "    print(CNV)\n",
    "    print(End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dirc=\"/data/suzaku/ted/HOME/noWGD\"\n",
    "num_step=50\n",
    "\n",
    "Chr_truth,CNV_truth,End_truth,state=Simulate_data(batch_size=50,Number_of_step=num_step)\n",
    "pd.DataFrame(Chr_truth.numpy()).astype(int).to_csv(dirc+\"Chr_truth.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV_truth.numpy()).astype(int).to_csv(dirc+\"CNV_truth.csv\",header=False,index=False)\n",
    "pd.DataFrame(End_truth.numpy()).astype(int).to_csv(dirc+\"End_truth.csv\",header=False,index=False)\n",
    "\n",
    "Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "End=torch.zeros(state.shape[0],num_step*2)\n",
    "state_copy=state.clone()\n",
    "Chr,CNV,End,state=Heur1(state,Chr,CNV,End)\n",
    "\n",
    "pd.DataFrame(Chr.numpy()).astype(int).to_csv(dirc+\"Chr_Heur1.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV.numpy()).astype(int).to_csv(dirc+\"CNV_Heur1.csv\",header=False,index=False)\n",
    "pd.DataFrame(End.numpy()).astype(int).to_csv(dirc+\"End_Heur1.csv\",header=False,index=False)\n",
    "\n",
    "state=state_copy.clone()\n",
    "Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "End=torch.zeros(state.shape[0],num_step*2)\n",
    "Chr,CNV,End,state=Heur2(state,Chr,CNV,End)\n",
    "\n",
    "pd.DataFrame(Chr.numpy()).astype(int).to_csv(dirc+\"Chr_Heur2.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV.numpy()).astype(int).to_csv(dirc+\"CNV_Heur2.csv\",header=False,index=False)\n",
    "pd.DataFrame(End.numpy()).astype(int).to_csv(dirc+\"End_Heur2.csv\",header=False,index=False)\n",
    "\n",
    "state=state_copy.clone()\n",
    "Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "End=torch.zeros(state.shape[0],num_step*2)\n",
    "Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "CNV[Chr>0.5]=CNV[Chr>0.5]+1\n",
    "pd.DataFrame(Chr.numpy()).astype(int).to_csv(dirc+\"Chr_RL.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV.numpy()).astype(int).to_csv(dirc+\"CNV_RL.csv\",header=False,index=False)\n",
    "pd.DataFrame(End.numpy()).astype(int).to_csv(dirc+\"End_RL.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulate_data(batch_size=15,Number_of_step=70):\n",
    "    state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "    next_state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "    Chr=torch.ones(batch_size,requires_grad=False).type(torch.LongTensor)\n",
    "    step=torch.zeros(batch_size,requires_grad=False)\n",
    "    wgd=torch.zeros(batch_size,requires_grad=False)\n",
    "    valid=torch.ones(batch_size,requires_grad=False)\n",
    "    \n",
    "    start_loci=torch.randint(high=chrom_width,size=(batch_size,),requires_grad=False)\n",
    "    end_loci=torch.LongTensor(batch_size)\n",
    "    cnv=torch.ones(batch_size,requires_grad=False)\n",
    "    chrom=torch.Tensor(batch_size,chrom_width)\n",
    "    chrom_new=torch.Tensor(batch_size,chrom_width)\n",
    "    \n",
    "    Chr_truth=torch.zeros(batch_size,Number_of_step)\n",
    "    CNV_truth=torch.zeros(batch_size,Number_of_step)\n",
    "    End_truth=torch.zeros(batch_size,Number_of_step)\n",
    "    \n",
    "    \n",
    "    step_counter=0\n",
    "    while(step_counter<Number_of_step):\n",
    "        for i in range(batch_size):\n",
    "            #reset valid after they have been checked\n",
    "            valid[i]=1\n",
    "            start_loci[i]=torch.randint(high=chrom_width,size=(1,))[0]\n",
    "            end_loci[i]=1+torch.randint(low=start_loci[i],high=50,size=(1,))[0]\n",
    "            if torch.rand(1)[0]>0.3:\n",
    "                Chr[i]=torch.randint(high=num_chromosome,size=(1,))[0]\n",
    "            #adding probability to sample chromosomal changes during training\n",
    "            if torch.rand(1)[0]>0.8:\n",
    "                start_loci[i]=0\n",
    "                end_loci[i]=chrom_width\n",
    "            #cnv\n",
    "            if torch.rand(1)[0]>0.7:\n",
    "                cnv[i]=0\n",
    "            #modifying cnp\n",
    "            prob_wgd=0.1/(1+math.exp(-step[i]+15))\n",
    "            #wgd          \n",
    "            if   ((torch.rand(1)[0]<prob_wgd or step[i]>30) and wgd[i]<1):\n",
    "                wgd[i]=1\n",
    "                state[i]=state[i]*2\n",
    "                next_state[i]=next_state[i]*2\n",
    "                Chr_truth[i][int(step[i])]=-1\n",
    "                CNV_truth[i][int(step[i])]=-1\n",
    "                End_truth[i][int(step[i])]=-1\n",
    "                step[i]=step[i]+1\n",
    "                continue\n",
    "                #adding cnv effect\n",
    "                #increasing copies when no wgd\n",
    "                #decreasing copies when wgd\n",
    "            if wgd[i]>0.5:\n",
    "                cnv[i]=1-cnv[i]\n",
    "            state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]=state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]-(cnv[i]-0.5)*2\n",
    "            chrom[i]=state[i][0][Chr[i]][:]\n",
    "            #reverse effect on chrom_new\n",
    "            chrom_new[i]=state[i][0][Chr[i]][:]\n",
    "            chrom_new[i][(start_loci[i]):]=chrom_new[i][(start_loci[i]):]+(cnv[i]-0.5)*2\n",
    "            #not going to negative values\n",
    "            if(torch.any(state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]< -0.5)):\n",
    "                valid[i]=0\n",
    "            #not joining breakpoints\n",
    "            if(start_loci[i]>0.5 and torch.abs(chrom[i][start_loci[i]]-chrom[i][start_loci[i]-1])<0.5):\n",
    "                valid[i]=0\n",
    "            if(end_loci[i]<chrom_width-0.5 and torch.abs(chrom[i][end_loci[i]-1]-chrom[i][end_loci[i]])<0.5):\n",
    "                valid[i]=0\n",
    "            if valid[i]>0 :\n",
    "                next_state[i]=state[i].clone()\n",
    "                Chr_truth[i][int(step[i])]=Chr[i]+1\n",
    "                CNV_truth[i][int(step[i])]=start_loci[i]*2+cnv[i]+1\n",
    "                End_truth[i][int(step[i])]=end_loci[i]\n",
    "                step[i]=step[i]+1\n",
    "                \n",
    "            #stay to further train the current step\n",
    "            #or resample another action\n",
    "            else:\n",
    "                state[i]=next_state[i].clone()\n",
    "        step_counter=step_counter+1\n",
    "    for i in range(batch_size):\n",
    "        temp_Chr=Chr_truth.clone()\n",
    "        temp_CNV=CNV_truth.clone()\n",
    "        temp_End=End_truth.clone()\n",
    "        for j in range(int(step[i].item())):\n",
    "            Chr_truth[i][int(step[i].item())-1-j]=temp_Chr[i][j]\n",
    "            CNV_truth[i][int(step[i].item())-1-j]=temp_CNV[i][j]\n",
    "            End_truth[i][int(step[i].item())-1-j]=temp_End[i][j]\n",
    "    return Chr_truth,CNV_truth,End_truth,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dirc=\"/data/suzaku/ted/HOME/WGD4\"\n",
    "num_step=50\n",
    "\n",
    "Chr_truth,CNV_truth,End_truth,state=Simulate_data(batch_size=50,Number_of_step=num_step)\n",
    "pd.DataFrame(Chr_truth.numpy()).astype(int).to_csv(dirc+\"Chr_truth.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV_truth.numpy()).astype(int).to_csv(dirc+\"CNV_truth.csv\",header=False,index=False)\n",
    "pd.DataFrame(End_truth.numpy()).astype(int).to_csv(dirc+\"End_truth.csv\",header=False,index=False)\n",
    "\n",
    "Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "End=torch.zeros(state.shape[0],num_step*2)\n",
    "state_copy=state.clone()\n",
    "Chr,CNV,End,state=Heur1(state,Chr,CNV,End)\n",
    "\n",
    "pd.DataFrame(Chr.numpy()).astype(int).to_csv(dirc+\"Chr_Heur1.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV.numpy()).astype(int).to_csv(dirc+\"CNV_Heur1.csv\",header=False,index=False)\n",
    "pd.DataFrame(End.numpy()).astype(int).to_csv(dirc+\"End_Heur1.csv\",header=False,index=False)\n",
    "\n",
    "state=state_copy.clone()\n",
    "Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "End=torch.zeros(state.shape[0],num_step*2)\n",
    "Chr,CNV,End,state=Heur2(state,Chr,CNV,End)\n",
    "\n",
    "pd.DataFrame(Chr.numpy()).astype(int).to_csv(dirc+\"Chr_Heur2.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV.numpy()).astype(int).to_csv(dirc+\"CNV_Heur2.csv\",header=False,index=False)\n",
    "pd.DataFrame(End.numpy()).astype(int).to_csv(dirc+\"End_Heur2.csv\",header=False,index=False)\n",
    "\n",
    "state=state_copy.clone()\n",
    "Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "End=torch.zeros(state.shape[0],num_step*2)\n",
    "Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "CNV[Chr>0.5]=CNV[Chr>0.5]+1\n",
    "pd.DataFrame(Chr.numpy()).astype(int).to_csv(dirc+\"Chr_RL.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV.numpy()).astype(int).to_csv(dirc+\"CNV_RL.csv\",header=False,index=False)\n",
    "pd.DataFrame(End.numpy()).astype(int).to_csv(dirc+\"End_RL.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1742]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state=torch.ones(1,1,num_chromosome,50)\n",
    "state[0][0][:((num_chromosome//2)+0),:]=1\n",
    "state[0][0][((num_chromosome//2)+0):,:]=2\n",
    "model.switch.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0021]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.switch.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(state).mean(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 1207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0][0][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.5296, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 1208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Chrom_model(state,model.switch.forward(state))[0][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])"
      ]
     },
     "execution_count": 1320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0589]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 1284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,1]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,1]).view((22,50))\n",
    "model.switch.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major=np.loadtxt(\"/data/suzaku/ted/WGD/major_Kuramoch_comp_samp.txt\")\n",
    "minor=np.loadtxt(\"/data/suzaku/ted/WGD/minor_Kuramoch_comp_samp.txt\")\n",
    "major.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  4.,  5.,  6.,  6.,  6.,  6.,  8.,  8.,  8.,  8.,  8.,  9.,\n",
      "         10., 10., 10., 11., 12., 12., 12., 13., 13., 14., 15., 16., 16., 17.,\n",
      "         17., 18., 19., 19., 20., 20., 21., 21., 21., 23., 24., 25., 25., 26.,\n",
      "         27., 27., 27., 28., 28., 28., 29., 30., 31., 32., 32., 33., 34., 35.,\n",
      "         36., 36., 37., 38., 38., 39., 40., 41., 41., 42., 43., 43., -1.,  1.,\n",
      "          1.,  5.,  6.,  6.,  6.,  6.,  7.,  8.,  8.,  8.,  8., 10., 12., 13.,\n",
      "         13., 14., 14., 14., 16., 17., 17., 19., 19., 20., 21., 21., 21., 21.,\n",
      "         21., 21., 21., 23., 23., 25., 25., 27., 28., 28., 29., 29., 30., 32.,\n",
      "         32., 32., 33., 35., 35., 36., 36., 36., 36., 37., 39., 39., 40., 41.,\n",
      "         43., 44.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[ 1.,  1.,  1., 39., 37., 19., 71.,  3., 37., 95., 87., 89.,  1., 11.,\n",
      "          1., 23., 33., 99., 91., 19.,  1., 15., 11., 95.,  1., 47., 71.,  7.,\n",
      "         21.,  1., 21., 49.,  1., 47.,  9.,  1.,  7.,  1.,  1., 43., 29.,  1.,\n",
      "         97., 67.,  1.,  1., 19., 41., 61., 37.,  1., 77., 47., 19.,  1., 11.,\n",
      "         97.,  3.,  1., 71.,  1., 37.,  1., 93.,  1.,  1.,  1.,  3., -1., 42.,\n",
      "          0.,  0., 18., 70., 40.,  2., 99., 88., 36., 60., 86.,  0., 18., 10.,\n",
      "         14.,  1., 16., 94., 68., 99.,  0., 48., 48., 54.,  4.,  4.,  4.,  0.,\n",
      "          8.,  1.,  0., 42., 42.,  1., 28., 39., 18., 37.,  1., 99.,  1.,  1.,\n",
      "         71., 22.,  1., 81., 10., 96., 33.,  1., 99.,  5.,  1., 45., 15., 81.,\n",
      "         27.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[21., 10., 50., 34., 20., 11., 49.,  9., 19., 50., 44., 47., 14., 47.,\n",
      "         16., 13., 50., 50., 50., 12.,  3., 40.,  6., 50.,  2., 34., 50., 24.,\n",
      "         20., 50., 24., 40.,  2., 27., 50.,  2.,  4., 21., 50., 50., 15., 50.,\n",
      "         50., 34.,  9., 18., 11., 50., 35., 19., 50., 50., 35., 50., 50., 40.,\n",
      "         49.,  8.,  2., 50., 34., 22.,  7., 50., 40., 50., 13.,  2., -1., 23.,\n",
      "         23., 19., 11., 49., 22., 11., 50., 47., 25., 50., 50., 16., 12.,  6.,\n",
      "         40.,  1., 16., 50., 35., 50., 20., 40., 41., 50.,  3.,  3.,  3.,  1.,\n",
      "         50.,  2.,  3., 23., 23., 21., 15., 33., 11., 20., 19., 50., 18., 23.,\n",
      "         38., 13.,  9., 50.,  6., 49., 48.,  1., 50., 50., 18., 50., 50., 46.,\n",
      "         50., 50.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[  0,   0],\n",
      "        [  0,   1],\n",
      "        [  0,   2],\n",
      "        [  0,   3],\n",
      "        [  0,   4],\n",
      "        [  0,   5],\n",
      "        [  0,   6],\n",
      "        [  0,   7],\n",
      "        [  0,   8],\n",
      "        [  0,   9],\n",
      "        [  0,  10],\n",
      "        [  0,  11],\n",
      "        [  0,  12],\n",
      "        [  0,  13],\n",
      "        [  0,  14],\n",
      "        [  0,  15],\n",
      "        [  0,  16],\n",
      "        [  0,  17],\n",
      "        [  0,  18],\n",
      "        [  0,  19],\n",
      "        [  0,  20],\n",
      "        [  0,  21],\n",
      "        [  0,  22],\n",
      "        [  0,  23],\n",
      "        [  0,  24],\n",
      "        [  0,  25],\n",
      "        [  0,  26],\n",
      "        [  0,  27],\n",
      "        [  0,  28],\n",
      "        [  0,  29],\n",
      "        [  0,  30],\n",
      "        [  0,  31],\n",
      "        [  0,  32],\n",
      "        [  0,  33],\n",
      "        [  0,  34],\n",
      "        [  0,  35],\n",
      "        [  0,  36],\n",
      "        [  0,  37],\n",
      "        [  0,  38],\n",
      "        [  0,  39],\n",
      "        [  0,  40],\n",
      "        [  0,  41],\n",
      "        [  0,  42],\n",
      "        [  0,  43],\n",
      "        [  0,  44],\n",
      "        [  0,  45],\n",
      "        [  0,  46],\n",
      "        [  0,  47],\n",
      "        [  0,  48],\n",
      "        [  0,  49],\n",
      "        [  0,  50],\n",
      "        [  0,  51],\n",
      "        [  0,  52],\n",
      "        [  0,  53],\n",
      "        [  0,  54],\n",
      "        [  0,  55],\n",
      "        [  0,  56],\n",
      "        [  0,  57],\n",
      "        [  0,  58],\n",
      "        [  0,  59],\n",
      "        [  0,  60],\n",
      "        [  0,  61],\n",
      "        [  0,  62],\n",
      "        [  0,  63],\n",
      "        [  0,  64],\n",
      "        [  0,  65],\n",
      "        [  0,  66],\n",
      "        [  0,  67],\n",
      "        [  0,  68],\n",
      "        [  0,  69],\n",
      "        [  0,  70],\n",
      "        [  0,  71],\n",
      "        [  0,  72],\n",
      "        [  0,  73],\n",
      "        [  0,  74],\n",
      "        [  0,  75],\n",
      "        [  0,  76],\n",
      "        [  0,  77],\n",
      "        [  0,  78],\n",
      "        [  0,  79],\n",
      "        [  0,  80],\n",
      "        [  0,  81],\n",
      "        [  0,  82],\n",
      "        [  0,  83],\n",
      "        [  0,  84],\n",
      "        [  0,  85],\n",
      "        [  0,  86],\n",
      "        [  0,  87],\n",
      "        [  0,  88],\n",
      "        [  0,  89],\n",
      "        [  0,  90],\n",
      "        [  0,  91],\n",
      "        [  0,  92],\n",
      "        [  0,  93],\n",
      "        [  0,  94],\n",
      "        [  0,  95],\n",
      "        [  0,  96],\n",
      "        [  0,  97],\n",
      "        [  0,  98],\n",
      "        [  0,  99],\n",
      "        [  0, 100],\n",
      "        [  0, 101],\n",
      "        [  0, 102],\n",
      "        [  0, 103],\n",
      "        [  0, 104],\n",
      "        [  0, 105],\n",
      "        [  0, 106],\n",
      "        [  0, 107],\n",
      "        [  0, 108],\n",
      "        [  0, 109],\n",
      "        [  0, 110],\n",
      "        [  0, 111],\n",
      "        [  0, 112],\n",
      "        [  0, 113],\n",
      "        [  0, 114],\n",
      "        [  0, 115],\n",
      "        [  0, 116],\n",
      "        [  0, 117],\n",
      "        [  0, 118],\n",
      "        [  0, 119],\n",
      "        [  0, 120],\n",
      "        [  0, 121],\n",
      "        [  0, 122],\n",
      "        [  0, 123],\n",
      "        [  0, 124],\n",
      "        [  0, 125],\n",
      "        [  0, 126],\n",
      "        [  0, 127]])\n"
     ]
    }
   ],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,0]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,0]).view((22,50))\n",
    "Chr=torch.zeros(state.shape[0],200)\n",
    "CNV=torch.zeros(state.shape[0],200)\n",
    "End=torch.zeros(state.shape[0],200)\n",
    "state_copy=state.clone()\n",
    "#deconvolution\n",
    "Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "print(Chr)\n",
    "#check if the model picks the correct chromosome\n",
    "print(CNV)\n",
    "print(End)\n",
    "print(torch.nonzero(Chr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  2.,  4.,  5.,  6.,  6.,  6.,  6.,  8.,  8.,  8.,  8.,\n",
      "          9., 10., 10., 11., 12., 12., 12., 15., 16., 16., 17., 17., 18., 19.,\n",
      "         19., 20., 20., 20., 21., 21., 23., 23., 23., 24., 25., 25., 26., 27.,\n",
      "         27., 28., 28., 28., 28., 29., 31., 32., 33., 34., 35., 36., 37., 38.,\n",
      "         38., 40., 41., 41., 42., 43., 43., -1.,  1.,  1.,  1.,  1.,  1.,  5.,\n",
      "          6.,  6.,  6.,  6.,  7.,  7.,  8.,  8.,  8., 10., 12., 13., 14., 14.,\n",
      "         16., 17., 17., 17., 19., 20., 21., 21., 21., 21., 21., 23., 25., 25.,\n",
      "         27., 28., 28., 28., 28., 28., 29., 29., 30., 33., 35., 36., 36., 37.,\n",
      "         39., 39., 40., 41., 44.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[43., 15.,  1.,  1.,  1., 39., 37.,  3., 19., 41., 95., 87., 89.,  1.,\n",
      "         11.,  1., 33., 99., 19.,  1., 25.,  1., 47., 71.,  1., 45.,  1., 64.,\n",
      "          1., 95., 55.,  1.,  5., 49., 13., 15.,  1.,  1., 43., 29.,  1., 67.,\n",
      "          1., 25.,  1., 45., 41., 39.,  1.,  1., 19.,  1., 13.,  3.,  1., 71.,\n",
      "          1.,  1., 93.,  1.,  1.,  1.,  7., -1., 42., 42., 42., 42.,  0.,  0.,\n",
      "         18., 18., 40.,  2., 99., 72., 88., 86., 82.,  0., 18., 10.,  1., 16.,\n",
      "         68., 99.,  0., 36., 48., 54.,  4.,  4., 48.,  4.,  8., 12.,  1., 28.,\n",
      "         39., 41., 40.,  1., 37.,  0.,  1., 99.,  1.,  1., 81., 33.,  1.,  5.,\n",
      "          1., 41., 15., 81.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[23., 21.,  6., 11., 50., 33., 20., 12., 11., 21., 50., 44., 47., 14.,\n",
      "         47., 16., 50., 50., 12.,  9., 50.,  2., 34., 50., 13., 24., 50., 40.,\n",
      "         24., 50., 33., 27., 16., 50.,  7., 23.,  6., 50., 50., 15., 50., 50.,\n",
      "          9., 18.,  9., 50., 21., 21., 50., 50., 50., 50., 40.,  8.,  2., 50.,\n",
      "         34.,  7., 50., 40., 50.,  2., 50., -1., 23., 23., 23., 23., 23., 19.,\n",
      "         11., 11., 22., 12., 50., 41., 50., 47., 50., 16., 12.,  6.,  1., 14.,\n",
      "         35., 50., 13., 20., 32., 50.,  3.,  3., 50.,  3., 16.,  7., 21., 15.,\n",
      "         33., 50., 50., 12., 20.,  9., 19., 50., 18.,  9., 50., 50.,  1., 50.,\n",
      "         18., 50., 50., 46., 50.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[  0,   0],\n",
      "        [  0,   1],\n",
      "        [  0,   2],\n",
      "        [  0,   3],\n",
      "        [  0,   4],\n",
      "        [  0,   5],\n",
      "        [  0,   6],\n",
      "        [  0,   7],\n",
      "        [  0,   8],\n",
      "        [  0,   9],\n",
      "        [  0,  10],\n",
      "        [  0,  11],\n",
      "        [  0,  12],\n",
      "        [  0,  13],\n",
      "        [  0,  14],\n",
      "        [  0,  15],\n",
      "        [  0,  16],\n",
      "        [  0,  17],\n",
      "        [  0,  18],\n",
      "        [  0,  19],\n",
      "        [  0,  20],\n",
      "        [  0,  21],\n",
      "        [  0,  22],\n",
      "        [  0,  23],\n",
      "        [  0,  24],\n",
      "        [  0,  25],\n",
      "        [  0,  26],\n",
      "        [  0,  27],\n",
      "        [  0,  28],\n",
      "        [  0,  29],\n",
      "        [  0,  30],\n",
      "        [  0,  31],\n",
      "        [  0,  32],\n",
      "        [  0,  33],\n",
      "        [  0,  34],\n",
      "        [  0,  35],\n",
      "        [  0,  36],\n",
      "        [  0,  37],\n",
      "        [  0,  38],\n",
      "        [  0,  39],\n",
      "        [  0,  40],\n",
      "        [  0,  41],\n",
      "        [  0,  42],\n",
      "        [  0,  43],\n",
      "        [  0,  44],\n",
      "        [  0,  45],\n",
      "        [  0,  46],\n",
      "        [  0,  47],\n",
      "        [  0,  48],\n",
      "        [  0,  49],\n",
      "        [  0,  50],\n",
      "        [  0,  51],\n",
      "        [  0,  52],\n",
      "        [  0,  53],\n",
      "        [  0,  54],\n",
      "        [  0,  55],\n",
      "        [  0,  56],\n",
      "        [  0,  57],\n",
      "        [  0,  58],\n",
      "        [  0,  59],\n",
      "        [  0,  60],\n",
      "        [  0,  61],\n",
      "        [  0,  62],\n",
      "        [  0,  63],\n",
      "        [  0,  64],\n",
      "        [  0,  65],\n",
      "        [  0,  66],\n",
      "        [  0,  67],\n",
      "        [  0,  68],\n",
      "        [  0,  69],\n",
      "        [  0,  70],\n",
      "        [  0,  71],\n",
      "        [  0,  72],\n",
      "        [  0,  73],\n",
      "        [  0,  74],\n",
      "        [  0,  75],\n",
      "        [  0,  76],\n",
      "        [  0,  77],\n",
      "        [  0,  78],\n",
      "        [  0,  79],\n",
      "        [  0,  80],\n",
      "        [  0,  81],\n",
      "        [  0,  82],\n",
      "        [  0,  83],\n",
      "        [  0,  84],\n",
      "        [  0,  85],\n",
      "        [  0,  86],\n",
      "        [  0,  87],\n",
      "        [  0,  88],\n",
      "        [  0,  89],\n",
      "        [  0,  90],\n",
      "        [  0,  91],\n",
      "        [  0,  92],\n",
      "        [  0,  93],\n",
      "        [  0,  94],\n",
      "        [  0,  95],\n",
      "        [  0,  96],\n",
      "        [  0,  97],\n",
      "        [  0,  98],\n",
      "        [  0,  99],\n",
      "        [  0, 100],\n",
      "        [  0, 101],\n",
      "        [  0, 102],\n",
      "        [  0, 103],\n",
      "        [  0, 104],\n",
      "        [  0, 105],\n",
      "        [  0, 106],\n",
      "        [  0, 107],\n",
      "        [  0, 108],\n",
      "        [  0, 109],\n",
      "        [  0, 110],\n",
      "        [  0, 111],\n",
      "        [  0, 112],\n",
      "        [  0, 113],\n",
      "        [  0, 114],\n",
      "        [  0, 115],\n",
      "        [  0, 116]])\n"
     ]
    }
   ],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,1]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,1]).view((22,50))\n",
    "Chr=torch.zeros(state.shape[0],200)\n",
    "CNV=torch.zeros(state.shape[0],200)\n",
    "End=torch.zeros(state.shape[0],200)\n",
    "state_copy=state.clone()\n",
    "#deconvolution\n",
    "Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "print(Chr)\n",
    "#check if the model picks the correct chromosome\n",
    "print(CNV)\n",
    "print(End)\n",
    "print(torch.nonzero(Chr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,1]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,1]).view((22,50))\n",
    "model.switch(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLevolution",
   "language": "python",
   "name": "rlevolution"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
