{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-function.py\n",
    "#defining the Q-function \n",
    "#Q(s,a) in manuscript\n",
    "\n",
    "num_chromosome=44\n",
    "chrom_width=50\n",
    "max_copy=20\n",
    "normal_const=1e-5;\n",
    "single_loci_loss=normal_const*(1-2e-1)\n",
    "WGD=normal_const*0.6\n",
    "\n",
    "#switch structure mentioned in section 3.3.4\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_switch = [20,40,80]\n",
    "activatiion_wgd=torch.tanh\n",
    "class WGD_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WGD_Net, self).__init__()\n",
    "        #chromosome permutation invariant structure as described in section 3.3.3\n",
    "        #slide for chromosome is 1 and the filter length in this dimension is also 1\n",
    "        #thus, the same filter goes through all chromosomes in the same fashion\n",
    "        self.conv1=nn.Conv2d(1, nkernels_switch[0], (1,3),(1,1),(0,1))\n",
    "        self.conv2=nn.Conv2d(nkernels_switch[0],nkernels_switch[1] , (1,3),(1,1), (0,1))\n",
    "        self.conv3=nn.Conv2d(nkernels_switch[1],nkernels_switch[2] , (1,5),(1,1), (0,0))\n",
    "        self.linear=nn.Linear(nkernels_switch[2],1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #y=torch.clamp((F.relu(x.mean(3)-1)-0.5+0.5*F.relu(1-x.mean(3))),-1,1).sum((1,2))\n",
    "        y=20*(x.mean((1,2,3))-1.5)\n",
    "        y=y.reshape(x.shape[0],1).detach()\n",
    "        x=x.reshape(x.shape[0],1,num_chromosome,chrom_width)\n",
    "        x=F.max_pool2d(activatiion_wgd(self.conv1(x)),(1,5),(1,5),(0,0))\n",
    "        x=F.max_pool2d(activatiion_wgd(self.conv2(x)),(1,2),(1,2),(0,0))\n",
    "        x=(activatiion_wgd(self.conv3(x))).sum((2,3))\n",
    "        x=self.linear(x)\n",
    "        x=x/2\n",
    "        #residule representation in x as described in section 3.3.4\n",
    "        x=y/2+x\n",
    "        x=torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "#chromosome evaluation net \n",
    "#Used in Chrom_NN (which is Q_{phi_1}(s,c) in section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_chr = [80,120,160]\n",
    "activation_cnp=torch.tanh\n",
    "class CNP_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNP_Val, self).__init__()\n",
    "        self.conv1=nn.Conv2d(1, nkernels_chr[0], (1,5),(1,1),(0,2))\n",
    "        self.conv2=nn.Conv2d(nkernels_chr[0],nkernels_chr[1] , (1,3),(1,1), (0,1))\n",
    "        self.conv3=nn.Conv2d(nkernels_chr[1],nkernels_chr[2] , (1,3),(1,1), (0,1))\n",
    "        self.conv4=nn.Conv2d(nkernels_chr[2],1, (1,5),(1,1), (0,0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=F.max_pool2d(activation_cnp(self.conv1(x)),(1,3),(1,3),(0,1))\n",
    "        x=F.max_pool2d(activation_cnp(self.conv2(x)),(1,2),(1,2),(0,1))\n",
    "        x=F.max_pool2d(activation_cnp(self.conv3(x)),(1,2),(1,2),(0,1))\n",
    "        #KL divergence is always nonpositive\n",
    "        x=0.25+F.elu(self.conv4(x),0.25)\n",
    "        #number of sample * 44 chromosomes\n",
    "        x=x.reshape(x.shape[0],num_chromosome)\n",
    "        return x\n",
    "\n",
    "#Implemts Q_{phi_1}(s,c) in section 3.3.1\n",
    "#It combines two chromosome evaluation nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_1}(s,c)\n",
    "class Chrom_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Chrom_NN,self).__init__()\n",
    "        #two parts of the Chrom_NN\n",
    "        #NN for CNP without WGD \n",
    "        self.Val_noWGD=CNP_Val()\n",
    "        #NN for CNP with WGD\n",
    "        self.Val_WGD=CNP_Val()\n",
    "    \n",
    "    def forward(self,x,sigma):\n",
    "        #probability for WGD, which is computed by switch structure\n",
    "        sigma=sigma.expand(-1,num_chromosome)\n",
    "        #we assume the copy number for each loci ranges from 0~9\n",
    "        #for samples without WGD\n",
    "        \n",
    "        #y represents if a chromosome have abnormal copy numbers (positions with copy number other than 1)\n",
    "        y=torch.ceil((torch.abs(x-1)).mean(3)/max_copy)\n",
    "        y=y.reshape(x.shape[0],num_chromosome)\n",
    "        y=y.detach()\n",
    "        #Residule representation mentioned in section 3.3.4\n",
    "        #the value for Q_{phi_1}(s,c) is computed as Val_no (the value estimated by the neural net, the residule part)+ y (the empirial estimation) \n",
    "        Val_no=self.Val_noWGD.forward(x)\n",
    "        #chromosome with all 1 copies don't need any CNV and thus will be less likely mutated.\n",
    "        Val_no=-(1-y)*math.log(single_loci_loss)*2+((y*Val_no).sum(1)).reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
    "        #for samples with WGD\n",
    "        #it is similar to the previsou part, where z is an equivalent for y and Val_wgd is an equivalent for Val_no\n",
    "        z=torch.ceil((torch.abs(x-2*(x//2))).mean(3)/max_copy)\n",
    "        z=z.reshape(x.shape[0],num_chromosome)\n",
    "        z=z.detach()\n",
    "        Val_wgd=self.Val_WGD.forward(x)\n",
    "        Val_wgd=-(1-z)*math.log(single_loci_loss)*2+(z*Val_wgd).sum(1).reshape(x.shape[0],1).expand(-1,num_chromosome)-math.log(WGD)\n",
    "        \n",
    "        #combine two NN with switch as defined in Section 3.3.4\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        x=-x\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "#starting point and gain or loss (defined as sp in manuscript) \n",
    "#Used in CNV_NN (which is Q_{phi_2}(s,c,sp) on section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_CNV = [80,120,160,10]\n",
    "activation_cnv=torch.tanh\n",
    "class CNV_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNV_Val,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1, nkernels_CNV[0], (1,7),(1,1),(0,3))\n",
    "        self.conv2=nn.Conv2d(nkernels_CNV[0],nkernels_CNV[1] , (1,7),(1,1), (0,3))\n",
    "        self.conv3=nn.Conv2d(nkernels_CNV[1],nkernels_CNV[2] , (1,7),(1,1), (0,3))\n",
    "        self.conv4=nn.Conv2d(nkernels_CNV[2], nkernels_CNV[3], (1,7),(1,1), (0,3))\n",
    "        self.linear=nn.Linear(nkernels_CNV[3]*chrom_width,2*chrom_width-1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=activation_cnv(self.conv1(x))\n",
    "        x=activation_cnv(self.conv2(x))\n",
    "        x=activation_cnv(self.conv3(x))\n",
    "        x=activation_cnv(self.conv4(x))\n",
    "        x=x.reshape(x.shape[0],nkernels_CNV[3]*chrom_width)\n",
    "        x=self.linear(x)\n",
    "        #number of samples* [(50 regions)*(2(gain or loss))-1] \n",
    "        #Only have 50*2-1=99 output dimensions because we fix the average these output\n",
    "        #The average of them could be arbitrary because of the partitioning\n",
    "        return x\n",
    "\n",
    "#Implemts Q_{phi_2}(s,c,sp) in section 3.3.1\n",
    "#It combines two CNV_Val nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_2}(s,c,sp)\n",
    "class CNV_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNV_NN,self).__init__()\n",
    "        #two network setting\n",
    "        self.CNV_noWGD=CNV_Val()\n",
    "        self.CNV_WGD=CNV_Val()\n",
    "    \n",
    "    def forward(self,x,sigma):\n",
    "        #as in section 3.3.4\n",
    "        #y is the empirical estimation\n",
    "        #Val_no is the redidule representation\n",
    "        y=torch.Tensor(x.shape[0],chrom_width,2)\n",
    "        y[:,:,0]=F.relu(x-1)\n",
    "        y[:,:,1]=F.relu(1-x)\n",
    "        y=y.reshape(x.shape[0],2*chrom_width)\n",
    "        y=y[:,1:(2*chrom_width)]-y[:,0:1].expand(-1,2*chrom_width-1)\n",
    "        y=-y.detach()*math.log(single_loci_loss)\n",
    "        Val_no=self.CNV_noWGD.forward(x.reshape(x.shape[0],1,1,chrom_width))\n",
    "        Val_no=y+Val_no\n",
    "        \n",
    "        z=((torch.abs(x-2*(x//2))).reshape(x.shape[0],chrom_width,1)).expand(-1,-1,2)\n",
    "        z=z.reshape(x.shape[0],2*chrom_width)\n",
    "        z=z[:,1:(2*chrom_width)]-z[:,0:1].expand(-1,2*chrom_width-1)\n",
    "        z=-z.detach()*math.log(single_loci_loss)\n",
    "        Val_wgd=self.CNV_WGD.forward(x.reshape(x.shape[0],1,1,chrom_width))\n",
    "        Val_wgd=z+Val_wgd\n",
    "        #switch\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        return(x)\n",
    "    \n",
    "     \n",
    "    def find_one_cnv(self,chrom,sigma,last_cnv=-1):\n",
    "        #used for finding the cnv during deconvolution\n",
    "        #it is not used in training process\n",
    "        \n",
    "        res_cnv=self.forward(chrom,sigma)\n",
    "        #if there is originally a break point for start\n",
    "        #rule system \n",
    "        break_start=torch.zeros(chrom.shape[0],50,2,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(chrom.shape[0],50,requires_grad=False)\n",
    "        chrom_shift[:,1:]=chrom[:,:49]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_start[:,:,1]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_start[:,0,1]=1\n",
    "        #don't allow lose one copy when copy number equalls 1\n",
    "        break_start[:,:,0]=break_start[:,:,1]\n",
    "        break_start[:,:,0]=break_start[:,:,0]*torch.ceil((chrom/2-0.5)/max_copy)\n",
    "        break_start=break_start.reshape(chrom.shape[0],100)\n",
    "        res_cnv_full=torch.zeros(chrom.shape[0],100)\n",
    "        res_cnv_full[:,1:]=res_cnv\n",
    "        \n",
    "        if last_cnv>0: #not the first step or lossing one chromosome\n",
    "            forbidden=2*last_cnv//2+(1-last_cnv%2)\n",
    "            break_start[0][forbidden]=0\n",
    "        #Prior_rule=break_start\n",
    "        res_cnv_full=res_cnv_full+torch.log(break_start)\n",
    "        #best cnv according to the current Q\n",
    "        cnv_max_val,cnv_max=torch.max(res_cnv_full,1)\n",
    "        #print(res_cnv_full)\n",
    "        return int(cnv_max[0])\n",
    "    \n",
    "\n",
    "\n",
    "#end point\n",
    "#Used in End_Point_NN (which is Q_{phi_3}(s,c,sp,ep) on section 3.3.1)\n",
    "#kernel sizes for convolution layers\n",
    "nkernels_End = [80,120,240]\n",
    "activation_end=torch.tanh\n",
    "class End_Point_Val(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(End_Point_Val,self).__init__()\n",
    "        self.conv1=nn.Conv2d(2, nkernels_End[0], (1,7),(1,1),(0,3))\n",
    "        self.conv2=nn.Conv2d(nkernels_End[0],nkernels_End[1] , (1,7),(1,1), (0,3))\n",
    "        self.conv3=nn.Conv2d(nkernels_End[1],nkernels_End[2] , (1,7),(1,1), (0,3))\n",
    "        self.linear=nn.Linear(nkernels_End[2]*chrom_width,chrom_width-1)\n",
    "    \n",
    "    def forward(self,old,new):\n",
    "        x=torch.Tensor(old.shape[0],2,1,chrom_width)\n",
    "        x[:,0,0,:]=old\n",
    "        x[:,1,0,:]=new\n",
    "        x=x.detach()\n",
    "        x=activation_end(self.conv1(x))\n",
    "        x=activation_end(self.conv2(x))\n",
    "        x=activation_end(self.conv3(x))\n",
    "        x=x.reshape(x.shape[0],nkernels_End[2]*chrom_width)\n",
    "        x=self.linear(x)\n",
    "        #number of samples* [(chrom_width regions)-1] \n",
    "        #Only have chrom_width-1=49 output dimensions because we fix the average these output\n",
    "        #The average of them could be arbitrary because of the partitioning\n",
    "        return x\n",
    "    \n",
    "#Implemts Q_{phi_3}(s,c,sp,ep) in section 3.3.1\n",
    "#It combines two End_Point_Val nets mentioned above,\n",
    "#with a switch structure in section 3.3.4 to form Q_{phi_3}(s,c,sp,ep)\n",
    "class End_Point_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(End_Point_NN,self).__init__()\n",
    "        #two network setting\n",
    "        self.Val_noWGD=End_Point_Val()\n",
    "        self.Val_WGD=End_Point_Val()\n",
    "    \n",
    "    def forward(self,old,new,sigma):\n",
    "        \n",
    "        y=F.relu((old-1)*(old-new))\n",
    "        y=y[:,1:chrom_width]-y[:,0:1].expand(-1,chrom_width-1)\n",
    "        y=-y.detach()*math.log(single_loci_loss)\n",
    "        Val_no=self.Val_noWGD.forward(old,new)\n",
    "        Val_no=Val_no+y\n",
    "        \n",
    "        z=(old-2*(old//2))*(1-(new-2*(new//2)))\n",
    "        z=z[:,1:chrom_width]-z[:,0:1].expand(-1,chrom_width-1)\n",
    "        z=-z.detach()*math.log(single_loci_loss)\n",
    "        Val_wgd=self.Val_WGD.forward(old,new)\n",
    "        Val_wgd=Val_wgd+z\n",
    "        #switch\n",
    "        x=sigma*Val_wgd+(1-sigma)*Val_no\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def find_end(self,old,new,sigma,start_loci,cnv,valid):\n",
    "        #used for finding the end during loading data\n",
    "        res_end=self.forward(old,new,sigma)\n",
    "        \n",
    "        break_end=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:49]=old[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(old-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        \n",
    "        for i in range(old.shape[0]):\n",
    "            #can't end before starting point\n",
    "            break_end[i,:int(start_loci[i])]=0*break_end[i,:int(start_loci[i])]\n",
    "            #don't allow lose one copy when copy number equalls 1\n",
    "            if(cnv[i]<0.5):\n",
    "                j=int(start_loci[i])+1\n",
    "                while(j<chrom_width):\n",
    "                    if(old[i][j]<1.5):\n",
    "                        break\n",
    "                    j=j+1\n",
    "                break_end[i,j:chrom_width]=0*break_end[i,j:chrom_width]\n",
    "        res_end_full=torch.zeros(old.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        \n",
    "        #Prior_rule=break_end\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max=torch.max(res_end_full,1)\n",
    "        return end_max+1\n",
    "    \n",
    "    \n",
    "    def find_one_end(self,old,new,sigma,start,cnv):\n",
    "        #used for finding the end during deconvolution\n",
    "        res_end=self.forward(old,new,sigma)\n",
    "        \n",
    "        break_end=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(old.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:chrom_width-1]=old[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(old-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        #can't end before starting point\n",
    "        break_end[0,:start]=0*break_end[0,:start]\n",
    "        #don't allow lose one copy when copy number equalls 1\n",
    "        if(cnv<0.5):\n",
    "            j=start+1\n",
    "            while(j<chrom_width):\n",
    "                if(old[0][j]<1.5):\n",
    "                    break\n",
    "                j=j+1\n",
    "            break_end[0,j:chrom_width]=0*break_end[0,j:chrom_width]\n",
    "        res_end_full=torch.zeros(old.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        #Prior_rule=break_end\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max=torch.max(res_end_full,1)\n",
    "        end_max=int(end_max[0])\n",
    "        return end_max+1\n",
    "        \n",
    "\n",
    "#combine all separate networks\n",
    "#add Rule system\n",
    "\n",
    "#calculating the softmax\n",
    "#prevent inf when taking log(exp(x))\n",
    "#log_exp is always gonna be between 1 and the total number of elements\n",
    "def Soft_update(val1,soft1,val2,soft2):\n",
    "    bias=val1.clone()\n",
    "    log_exp=soft1.clone()\n",
    "    set1=[torch.ge(val1,val2)]\n",
    "    bias[set1]=val1[set1]\n",
    "    log_exp[set1]=soft1[set1]+soft2[set1]*torch.exp(val2[set1]-val1[set1])\n",
    "    set2=[torch.lt(val1,val2)]\n",
    "    bias[set2]=val2[set2]\n",
    "    log_exp[set2]=soft2[set2]+soft1[set2]*torch.exp(val1[set2]-val2[set2])\n",
    "    return bias,log_exp\n",
    "\n",
    "\n",
    "\n",
    "#Combine all the separate modules mentioned above\n",
    "#Implementation of Q(s,a)\n",
    "class Q_learning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_learning,self).__init__()\n",
    "        self.switch=WGD_Net()\n",
    "        #the output refer to Q_{\\phi_1}(s,c)\n",
    "        self.Chrom_model=Chrom_NN()\n",
    "        #the output refer to Q_{\\phi_2}(s,c,sp)\n",
    "        self.CNV=CNV_NN()\n",
    "        #the output refer to Q_{\\phi_3}(s,sp,c,ep)\n",
    "        self.End=End_Point_NN()\n",
    "    \n",
    "    \n",
    "    def forward(self,state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid):\n",
    "        '''\n",
    "        computing the final advantage(loss) used for training\n",
    "        loss in Thereom1\n",
    "        state: s in Q(s,a)\n",
    "        next_state: s' in softmaxQ(s',a')\n",
    "        Chr,cnv,end_loci: a in Q(s,a)\n",
    "        chrom,chrom_new,start_loci,end_loci: intermediate results from s,a, which is preprossed to make computation faster\n",
    "            e.g. chrom is CNP of the Chr(part of a) from the state(s)\n",
    "            They could be seen as a mapping without parameters to learn:f(s,a)\n",
    "        valid: a boolean array, indicating if a training sample is valid (e.g. have non negative copy numbers for all loci)\n",
    "        '''\n",
    "        \n",
    "        #computing softmaxQ(s',a')\n",
    "        #It is a tradition in RL that gradient does not backpropogate through softmaxQ(s',a'), but only through Q(s,a) to make convergence faster\n",
    "        #there is no theoritical guarantee behind, and it is only a practical trick\n",
    "        sigma_next=self.switch(next_state)\n",
    "        x,y=self.Softmax(next_state,sigma_next)\n",
    "        x=x+torch.log(y)\n",
    "        #computing r(s,a)\n",
    "        x=x+Reward(start_loci,end_loci)\n",
    "        x=x.detach()\n",
    "        \n",
    "        #computing Q(s,a)\n",
    "        sigma=self.switch.forward(state)\n",
    "        #Q_{phi_1}(s,c)\n",
    "        res_chrom=self.Chrom_model.forward(state,sigma)\n",
    "        \n",
    "        #Q_{phi_2}(s,c,sp)\n",
    "        res_cnv=self.CNV.forward(chrom,sigma)\n",
    "        #if there is originally a break point for start\n",
    "        #real world constraint as described in section 3.3.2\n",
    "        #only allow starting points (sp) to be the break points of CNP\n",
    "        break_start=torch.zeros(state.shape[0],chrom_width,2,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,1:]=chrom[:,:(chrom_width-1)]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_start[:,:,1]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_start[:,0,1]=1\n",
    "        #don't allow lose one copy when copy number equals 0, otherwise there is going to be negative copy numbers\n",
    "        break_start[:,:,0]=break_start[:,:,1]\n",
    "        break_start[:,:,0]=break_start[:,:,0]*torch.ceil((chrom/2-0.5)/max_copy)\n",
    "        break_start=break_start.reshape(state.shape[0],2*chrom_width)\n",
    "        res_cnv_full=torch.zeros(state.shape[0],2*chrom_width)\n",
    "        res_cnv_full[:,1:]=res_cnv\n",
    "        res_cnv_full=res_cnv_full+torch.log(break_start)\n",
    "        \n",
    "        #Q_{phi_2}(s,c,sp)-softmax(Q_{phi_2}(s,c,sp)) as described in section 3.3.1\n",
    "        cnv_max_val,cnv_max=torch.max(res_cnv_full,1)\n",
    "        cnv_softmax=res_cnv_full-cnv_max_val.reshape(state.shape[0],1).expand(-1,2*chrom_width)\n",
    "        cnv_softmax=torch.exp(cnv_softmax).sum(1)\n",
    "        x=x+cnv_max_val+torch.log(cnv_softmax)\n",
    "        \n",
    "        #Q_{phi_3}(s,c,sp,ep)\n",
    "        res_end=self.End.forward(chrom,chrom_new,sigma)\n",
    "        #if there is originally a break point for end\n",
    "        #and if this is after the starting point\n",
    "        #real world constraint in section 3.3.2\n",
    "        break_end=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
    "        chrom_shift[:,:(chrom_width-1)]=chrom[:,1:]\n",
    "        #allow adding one copy for every breakpoint\n",
    "        break_end[:,:]=torch.ceil(torch.abs(chrom-chrom_shift)/max_copy)\n",
    "        #always allow adding one chromosone\n",
    "        break_end[:,chrom_width-1]=1\n",
    "        for i in range(state.shape[0]):\n",
    "            #can't end before starting point\n",
    "            break_end[i,:int(start_loci[i])]=0*break_end[i,:int(start_loci[i])]\n",
    "            #don't allow lose one copy when copy number equalls 1\n",
    "            if(cnv[i]<0.5):\n",
    "                j=int(start_loci[i])+1\n",
    "                while(j<chrom_width):\n",
    "                    if(chrom[i][j]<1.5):\n",
    "                        break\n",
    "                    j=j+1\n",
    "                break_end[i,j:chrom_width]=0*break_end[i,j:chrom_width]\n",
    "            \n",
    "        res_end_full=torch.zeros(state.shape[0],chrom_width)\n",
    "        res_end_full[:,1:]=res_end\n",
    "        \n",
    "        #real world constraint described in section 3.3.2\n",
    "        res_end_full=res_end_full+torch.log(break_end)\n",
    "        end_max_val,end_max_temp=torch.max(res_end_full,1)\n",
    "        end_softmax=res_end_full-end_max_val.reshape(state.shape[0],1).expand(-1,chrom_width)\n",
    "        end_softmax=torch.exp(end_softmax).sum(1)\n",
    "        #Q_{phi_3}(s,c,sp,ep)-softmax(Q_{phi_3}(s,c,sp,ep)) as described in section 3.3.1\n",
    "        x=x+end_max_val+torch.log(end_softmax)\n",
    "        \n",
    "        for i in range(state.shape[0]):\n",
    "            if valid[i]>0.5:#check validity to prevent inf-inf which ends in nan\n",
    "                x[i]=x[i]-res_chrom[i][int(Chr[i])]\n",
    "                cnv_rank=int(start_loci[i]*2+cnv[i])\n",
    "                x[i]=x[i]-res_cnv_full[i][cnv_rank]\n",
    "                end_rank=int(end_loci[i]-1)\n",
    "                x[i]=x[i]-res_end_full[i][end_rank]\n",
    "        \n",
    "        #remove training data which include invalid actions\n",
    "        x=x*valid\n",
    "        #return avdantage as well as a best cnv and sigma used for generating training data\n",
    "        #used for training in the next step\n",
    "        return x,cnv_max,sigma,res_chrom,res_cnv_full,res_end_full\n",
    "     \n",
    "    def Softmax(self,next_state,sigma):\n",
    "        #compute softmax_{a'} Q(s',a')\n",
    "        x=self.Chrom_model.forward(next_state,sigma)\n",
    "        max_chrom=torch.max(x,1)[0]\n",
    "        softmax_chrom=x-max_chrom.reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
    "        softmax_chrom=torch.exp(softmax_chrom).sum(1)\n",
    "        #special action END\n",
    "        #all the remaining abnormal loci are treated to be caused by several independent single locus copy number changes\n",
    "        end_val=torch.sum(torch.abs(next_state-1),(1,2,3))*math.log(single_loci_loss)\n",
    "        max_chrom,softmax_chrom=Soft_update(max_chrom,softmax_chrom,end_val,torch.ones(x.shape[0]))\n",
    "        #if there is a WGD followed immediately\n",
    "        for i in range(x.shape[0]):\n",
    "            #real world constraint as described in section 3.3.2\n",
    "            #do not allow (reversing) WGD when the CNP contain odd numbers for some loci\n",
    "            if (not torch.any(next_state[i]-2*torch.floor(next_state[i]/2)>0.5)) and torch.any(next_state[i]>0.5):\n",
    "                sigma_wgd=self.switch(torch.floor(next_state[i:(i+1)]/2))\n",
    "                sigma_wgd=sigma_wgd.detach()\n",
    "                wgd_val,wgd_soft=self.Softmax(torch.floor(next_state[i:(i+1)]/2),sigma_wgd)\n",
    "                max_chrom[i],softmax_chrom[i]=Soft_update(torch.ones(1)*max_chrom[i],torch.ones(1)*softmax_chrom[i],torch.ones(1)*wgd_val,torch.ones(1)*wgd_soft)\n",
    "        \n",
    "        return max_chrom,softmax_chrom\n",
    "  \n",
    "\n",
    "#Minimum example\n",
    "if __name__ == \"__main__\":\n",
    "    #test different parts separately\n",
    "    '''\n",
    "    switch=WGD_Net()\n",
    "    Chrom_model=Chrom_NN()\n",
    "    print(Chrom_model)\n",
    "    #test the structure of permutation invariant structure\n",
    "    x=torch.ones(3,1,num_chromosome,50)\n",
    "    x[0][0][0][0:50]=2\n",
    "    x[2][0][1][0:50]=2\n",
    "    prob=switch.forward(x)\n",
    "    print(prob)\n",
    "    res=Chrom_model.forward(x,prob)\n",
    "    print(res)\n",
    "    res=-float('inf')\n",
    "    res=torch.LongTensor(3)\n",
    "    \n",
    "    print(torch.log(res.type(torch.DoubleTensor)))\n",
    "    #CNV\n",
    "    CNV=CNV_NN()\n",
    "    res=CNV.forward(x[:,0,0,0:50],prob)\n",
    "    print(CNV)\n",
    "    print(res.shape)\n",
    "    #END\n",
    "    End=End_Point_NN()\n",
    "    res=End.forward(x[:,0,0,0:50],x[:,0,0,0:50]+1,prob)\n",
    "    print(End)\n",
    "    print(res.shape)\n",
    "    '''\n",
    "    #test Q-learning\n",
    "    x=torch.ones(3,1,num_chromosome,50)\n",
    "    y=torch.ones(3,1,num_chromosome,50)\n",
    "    x[0][0][0][0:50]=2\n",
    "    x[2][0][1][0:50]=2\n",
    "    chrom=x[:,0,0,:]\n",
    "    chrom_new=y[:,0,0,:]\n",
    "    Chr=torch.zeros(3)\n",
    "    cnv=torch.ones(3)\n",
    "    start_loci=torch.zeros(3)\n",
    "    end_loci=torch.ones(3)*50\n",
    "    valid=torch.ones(3)\n",
    "    Q_model=Q_learning()\n",
    "    #res,cnv_max,sigma,t,t2,t3=Q_model.forward(x,y,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid)\n",
    "    #print(res)\n",
    "    #print(cnv_max)\n",
    "    #loss=res.pow(2).mean()\n",
    "    #print(loss)\n",
    "    #loss.backward()\n",
    "    #params = list(Q_model.parameters())\n",
    "    #print(params[0].grad[0])\n",
    "    #print(Q_model.switch.conv1.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate data for testing\n",
    "def Simulate_data(batch_size=15,Number_of_step=70):\n",
    "    state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "    next_state=torch.ones(batch_size,1,num_chromosome,chrom_width,requires_grad=False)\n",
    "    Chr=torch.ones(batch_size,requires_grad=False).type(torch.LongTensor)\n",
    "    step=torch.zeros(batch_size,requires_grad=False)\n",
    "    wgd=torch.zeros(batch_size,requires_grad=False)\n",
    "    valid=torch.ones(batch_size,requires_grad=False)\n",
    "    \n",
    "    start_loci=torch.randint(high=chrom_width,size=(batch_size,),requires_grad=False)\n",
    "    end_loci=torch.LongTensor(batch_size)\n",
    "    cnv=torch.ones(batch_size,requires_grad=False)\n",
    "    chrom=torch.Tensor(batch_size,chrom_width)\n",
    "    chrom_new=torch.Tensor(batch_size,chrom_width)\n",
    "    \n",
    "    Chr_truth=torch.zeros(batch_size,Number_of_step)\n",
    "    CNV_truth=torch.zeros(batch_size,Number_of_step)\n",
    "    End_truth=torch.zeros(batch_size,Number_of_step)\n",
    "    \n",
    "    \n",
    "    step_counter=0\n",
    "    while(step_counter<Number_of_step):\n",
    "        for i in range(batch_size):\n",
    "            #reset valid after they have been checked\n",
    "            valid[i]=1\n",
    "            start_loci[i]=torch.randint(high=chrom_width,size=(1,))[0]\n",
    "            end_loci[i]=1+torch.randint(low=start_loci[i],high=50,size=(1,))[0]\n",
    "            if torch.rand(1)[0]>0.3:\n",
    "                Chr[i]=torch.randint(high=num_chromosome,size=(1,))[0]\n",
    "            #adding probability to sample chromosomal changes during training\n",
    "            if torch.rand(1)[0]>0.8:\n",
    "                start_loci[i]=0\n",
    "                end_loci[i]=chrom_width\n",
    "            #cnv\n",
    "            if torch.rand(1)[0]>0.7:\n",
    "                cnv[i]=0\n",
    "            #modifying cnp\n",
    "            prob_wgd=0.1/(1+math.exp(-step[i]+15))\n",
    "            #wgd          \n",
    "            if False and (torch.rand(1)[0]<prob_wgd and wgd[i]<1):\n",
    "                wgd[i]=1\n",
    "                state[i]=state[i]*2\n",
    "                next_state[i]=next_state[i]*2\n",
    "                Chr_truth[i][int(step[i])]=-1\n",
    "                CNV_truth[i][int(step[i])]=-1\n",
    "                End_truth[i][int(step[i])]=-1\n",
    "                step[i]=step[i]+1\n",
    "                continue\n",
    "                #adding cnv effect\n",
    "                #increasing copies when no wgd\n",
    "                #decreasing copies when wgd\n",
    "            if wgd[i]>0.5:\n",
    "                cnv[i]=1-cnv[i]\n",
    "            state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]=state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]-(cnv[i]-0.5)*2\n",
    "            chrom[i]=state[i][0][Chr[i]][:]\n",
    "            #reverse effect on chrom_new\n",
    "            chrom_new[i]=state[i][0][Chr[i]][:]\n",
    "            chrom_new[i][(start_loci[i]):]=chrom_new[i][(start_loci[i]):]+(cnv[i]-0.5)*2\n",
    "            #not going to negative values\n",
    "            if(torch.any(state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]< -0.5)):\n",
    "                valid[i]=0\n",
    "            #not joining breakpoints\n",
    "            if(start_loci[i]>0.5 and torch.abs(chrom[i][start_loci[i]]-chrom[i][start_loci[i]-1])<0.5):\n",
    "                valid[i]=0\n",
    "            if(end_loci[i]<chrom_width-0.5 and torch.abs(chrom[i][end_loci[i]-1]-chrom[i][end_loci[i]])<0.5):\n",
    "                valid[i]=0\n",
    "            if valid[i]>0 :\n",
    "                next_state[i]=state[i].clone()\n",
    "                Chr_truth[i][int(step[i])]=Chr[i]+1\n",
    "                CNV_truth[i][int(step[i])]=start_loci[i]*2+cnv[i]+1\n",
    "                End_truth[i][int(step[i])]=end_loci[i]\n",
    "                step[i]=step[i]+1\n",
    "                \n",
    "            #stay to further train the current step\n",
    "            #or resample another action\n",
    "            else:\n",
    "                state[i]=next_state[i].clone()\n",
    "        step_counter=step_counter+1\n",
    "    for i in range(batch_size):\n",
    "        temp_Chr=Chr_truth.clone()\n",
    "        temp_CNV=CNV_truth.clone()\n",
    "        temp_End=End_truth.clone()\n",
    "        for j in range(int(step[i].item())):\n",
    "            Chr_truth[i][int(step[i].item())-1-j]=temp_Chr[i][j]\n",
    "            CNV_truth[i][int(step[i].item())-1-j]=temp_CNV[i][j]\n",
    "            End_truth[i][int(step[i].item())-1-j]=temp_End[i][j]\n",
    "    return Chr_truth,CNV_truth,End_truth,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Deconvolution.py\n",
    "#used for deconvolution of CNP history\n",
    "\n",
    "def Deconvolute(model,cnp,Chr,CNV,End):\n",
    "    '''\n",
    "    Deconvolution samples the maximum action in a greedy way\n",
    "    model:the trained Q-learning model\n",
    "    cnp: the input CNP, shape: Number of CNP,1,44 (#Chr),50 (#regions for one chromosome)\n",
    "    Chr,CNV,END: output tensor,shape: Number of CNP, maximum length of history\n",
    "    output: for Chr: -1 indicates WGD, 0 indicates no action, 1~44 the chromosome\n",
    "            for CNV: only valid if Chr is not -1 or 0\n",
    "                     indicates the starting point (CNV//2) and the type of CNV (CNV%2==1 for gain and CNV%2==0 for loss)\n",
    "            for End: only valid if Chr is not -1 or 0\n",
    "                     indicates the end point for a CNV.\n",
    "    '''\n",
    "    max_step=int(Chr.shape[1])\n",
    "    \n",
    "    for i in range(cnp.shape[0]):\n",
    "        flag=False\n",
    "        current_cnp=cnp[i:(i+1)]\n",
    "        sigma=model.switch(current_cnp)\n",
    "        if sigma[0]<0.5:\n",
    "            flag=True\n",
    "        step=0\n",
    "        while(step<max_step):\n",
    "            #it is also possible to manually set the switch if deemed necessary\n",
    "            sigma=model.switch(current_cnp)\n",
    "            #hard classification\n",
    "            if not flag:\n",
    "                sigma=torch.ceil(sigma)\n",
    "            #else:\n",
    "            #    sigma=torch.zeros_like(sigma)\n",
    "            #print(sigma)\n",
    "            res_chrom=model.Chrom_model(current_cnp,sigma)\n",
    "            #print(res_chrom)\n",
    "            #find the chromosome with the maximum probability\n",
    "            val,temp_Chr=res_chrom.max(1)\n",
    "            temp_Chr=int(temp_Chr)\n",
    "            Chr[i][step]=temp_Chr+1\n",
    "            #WGD\n",
    "            if (not torch.any(current_cnp-2*torch.floor(current_cnp/2)>0.5)) and torch.any(current_cnp>0.5):\n",
    "                sigma_wgd=model.switch(torch.floor(current_cnp/2))\n",
    "                res_chrom_wgd=model.Chrom_model(torch.floor(current_cnp/2),sigma_wgd)\n",
    "                val_wgd,temp=res_chrom_wgd.max(1)\n",
    "                if not flag:#val_wgd>=val and not flag:\n",
    "                    val=val_wgd\n",
    "                    Chr[i][step]=-1\n",
    "                    CNV[i][step]=-1\n",
    "                    End[i][step]=-1\n",
    "                    flag=True\n",
    "            #special action END\n",
    "            val_end=torch.sum(torch.abs(current_cnp-1))*math.log(single_loci_loss)\n",
    "            if val_end>=val:\n",
    "                val=val_end\n",
    "                Chr[i][step]=0\n",
    "                #print(val_end)\n",
    "                break\n",
    "            #if WGD\n",
    "            if Chr[i][step]< -0.5:\n",
    "                current_cnp=(current_cnp/2).floor()\n",
    "                flag=True\n",
    "            #if not WGD or END\n",
    "            elif Chr[i][step]>0.5:\n",
    "                #find best CNV\n",
    "                chrom=current_cnp[:,0,temp_Chr,:]\n",
    "                last_step=-1\n",
    "                if step>1 and Chr[i][step]==Chr[i][step-1]:\n",
    "                    last_step=int(CNV[i][step-1].item())\n",
    "                CNV[i][step]=model.CNV.find_one_cnv(chrom,sigma,last_step)\n",
    "                cnv_temp=int(CNV[i][step]%2)\n",
    "                start_temp=int(CNV[i][step]//2)\n",
    "                #find best End\n",
    "                chrom_new=chrom.clone()\n",
    "                chrom_new[:,start_temp:]=chrom_new[:,start_temp:]+(cnv_temp-0.5)*2\n",
    "                End[i][step]=model.End.find_one_end(chrom,chrom_new,sigma,start_temp,cnv_temp)\n",
    "                #updata cnp\n",
    "                #print(chrom)\n",
    "                #print(start_temp,End[i][step],cnv_temp)\n",
    "                \n",
    "                current_cnp[:,0,temp_Chr,start_temp:int(End[i][step])]=current_cnp[:,0,temp_Chr,start_temp:int(End[i][step])]+(cnv_temp-0.5)*2\n",
    "                \n",
    "            step=step+1\n",
    "    return Chr,CNV,End,current_cnp\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #model = Q_learning()\n",
    "    #model.load_state_dict(torch.load(\"/data/suzaku/ted/HOME/model\"))\n",
    "    #model.eval()\n",
    "    counter_global=torch.randint(10000,(1,))[0]\n",
    "    #loading simulated data\n",
    "    num_step=50\n",
    "    Chr_truth,CNV_truth,End_truth,state=Simulate_data(batch_size=1,Number_of_step=num_step)\n",
    "    Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "    CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "    End=torch.zeros(state.shape[0],num_step*2)\n",
    "    state_copy=state.clone()\n",
    "    #deconvolution\n",
    "    Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "    CNV[Chr>0.5]=CNV[Chr>0.5]+1\n",
    "    print(Chr,Chr_truth)\n",
    "    #check if the model picks the correct chromosome\n",
    "    print(CNV,CNV_truth)\n",
    "    print(End,End_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(),\"/data/suzaku/ted/HOME/model_tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,\n",
      "          3.,  3.,  3.,  4.,  4.,  4.,  5.,  5.,  5.,  5.,  6.,  6.,  7.,  7.,\n",
      "          7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "          7.,  7.,  7.,  8.,  8.,  8.,  8.,  9.,  9.,  9., 10., 10., 10., 10.,\n",
      "         11., 11., 11., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
      "         12., 12., 12., 12., 12., 12., 12., 12., 13., 14., 15., 16., 16., 16.,\n",
      "         17., 17., 17., 17., 17., 17., 17., 17., 18., 19., 20., 20., 21., 22.,\n",
      "         23., 24.]]) tensor([[43., 43., 43., 17., 12.,  1.,  1.,  4., 20., 40., 38.,  3., 17., 19.,\n",
      "          6., -1., 17.,  7., 43., 17.,  8.,  7., 11., 36., 12., 10., 31., 16.,\n",
      "         12., 31., 29., 30.,  2., 29., 34.,  7.,  9., 37., 29., 12.,  5., 35.,\n",
      "         39., 41.,  7.,  1., 37., 31., 43., 30.]])\n",
      "tensor([[ 1.,  5.,  5., 29., 29., 29., 29., 79., 79., 79., 79.,  1.,  1.,  1.,\n",
      "          1., 57., 57.,  1., 17., 17.,  1., 65., 65., 65.,  1., 29.,  1.,  1.,\n",
      "          1., 13., 13., 13., 13., 13., 13., 13., 15., 15., 15., 15., 15., 15.,\n",
      "         15., 15., 15.,  1., 15., 15., 15.,  1.,  1.,  1.,  1., 79., 79., 79.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1., 45., 45., 45., 45., 65., 65., 65., 65.,\n",
      "         65., 87., 87., 87., 87., 87., 87., 87.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         16., 23., 29., 29., 33., 33., 33., 33.,  1.,  1.,  1., 95.,  1.,  1.,\n",
      "          1.,  1.]]) tensor([[21.,  4., 55., 16., 22.,  5., 36., 17., 62., 67.,  2., 57.,  2., 46.,\n",
      "         24., -1., 33., 15., 93., 23., 15., 13.,  1., 13., 87., 79., 57.,  1.,\n",
      "          1., 65.,  1., 37.,  1.,  1., 27.,  1.,  1., 17., 69., 45., 65., 41.,\n",
      "          9., 33., 13., 29., 11., 17.,  1.,  1.]])\n",
      "tensor([[ 2., 14., 14., 17., 39., 39., 39., 42., 46., 46., 50., 50., 50., 50.,\n",
      "         28., 32., 50.,  8., 47., 50., 32., 40., 40., 50., 11., 50.,  6.,  6.,\n",
      "          6.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  9.,  9., 12., 12., 49., 49.,\n",
      "         50., 50., 50.,  7., 13., 13., 50., 50., 50., 50., 39., 46., 46., 50.,\n",
      "         50., 50., 50., 10., 22., 22., 32., 32., 32., 32., 43., 43., 43., 43.,\n",
      "         43., 44., 44., 49., 49., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
      "         11., 14., 16., 16., 32., 32., 41., 41., 50., 22., 30., 50., 50., 50.,\n",
      "         50., 50.]]) tensor([[43.,  3., 41., 14., 32., 42., 39., 47., 47., 49., 50., 32., 50., 50.,\n",
      "         14., -1., 32., 12., 49., 41., 13., 49., 50., 10., 44., 46., 43., 50.,\n",
      "         50., 42., 50., 35., 50., 50., 39., 50., 50., 21., 40., 49., 40., 24.,\n",
      "         14., 38.,  9., 46., 10., 36., 50., 50.]])\n"
     ]
    }
   ],
   "source": [
    "def Heur1(cnp,Chr,CNV,End):\n",
    "    max_step=int(Chr.shape[1])\n",
    "    \n",
    "    for i in range(cnp.shape[0]):\n",
    "        current_cnp=cnp[i:(i+1)]\n",
    "        current_locus=0\n",
    "        step=0\n",
    "        while(step<max_step and current_locus<num_chromosome*chrom_width):\n",
    "            current_Chr=current_locus//chrom_width\n",
    "            current_start=current_locus%chrom_width\n",
    "            if current_cnp[0][0][current_Chr][current_start]==1:\n",
    "                current_locus+=1\n",
    "                continue\n",
    "            elif current_cnp[0][0][current_Chr][current_start]<1:\n",
    "                current_cnv=1\n",
    "            else:\n",
    "                current_cnv=-1\n",
    "            current_end=current_start\n",
    "            while(current_end<50):\n",
    "                if current_cnp[0][0][current_Chr][current_end]!=current_cnp[0][0][current_Chr][current_start]:\n",
    "                    break\n",
    "                current_end+=1\n",
    "            current_cnp[0,0,current_Chr,current_start:current_end]+=current_cnv\n",
    "            Chr[i][step]=current_Chr+1\n",
    "            CNV[i][step]=2*current_start+(current_cnv+1)//2+1\n",
    "            End[i][step]=current_end\n",
    "            step=step+1\n",
    "    return Chr,CNV,End,current_cnp\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #loading simulated data\n",
    "    num_step=50\n",
    "    Chr_truth,CNV_truth,End_truth,state=Simulate_data(batch_size=1,Number_of_step=num_step)\n",
    "    Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "    CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "    End=torch.zeros(state.shape[0],num_step*2)\n",
    "    state_copy=state.clone()\n",
    "    #deconvolution\n",
    "    Chr,CNV,End,state=Heur1(state,Chr,CNV,End)\n",
    "    print(Chr,Chr_truth)\n",
    "    #check if the model picks the correct chromosome\n",
    "    print(CNV,CNV_truth)\n",
    "    print(End,End_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor([[ 3.,  3.,  6.,  6., 10., 10., 10., 10., 12., 12., 12., 15., 15., 15.,\n",
      "         15., 18., 18., 21., 21., 21., 21., 26., 26., 26., 27., 27., 28., 28.,\n",
      "         28., 28., 28., 32., 32., 32., 32., 34., 34., 35., 35., 35., 37., 40.,\n",
      "         40., 40., 41., 44., -1.,  3.,  6.,  8., 10., 12., 15., 17., 20., 22.,\n",
      "         24., 28., 35., 35., 36., 39., 42., 43., 44.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.],\n",
      "        [ 2.,  2.,  6.,  7.,  9.,  9., 10., 11., 17., 17., 17., 17., 18., 18.,\n",
      "         19., 20., 20., 20., 20., 21., 22., 22., 22., 22., 23., 24., 24., 25.,\n",
      "         25., 26., 26., 26., 26., 26., 26., 27., 27., 31., 32., 36., 40., 42.,\n",
      "         42., 43., 44., 44., 44., 44., 44., 44., -1.,  9., 12., 13., 16., 17.,\n",
      "         17., 17., 18., 20., 21., 22., 22., 22., 22., 25., 44.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.]])\n",
      "tensor([[  1.,   1.,  35.,  54.,   1.,   1.,  70.,  81.,  36.,  64.,  81.,   1.,\n",
      "           1.,  23.,  80.,   1.,  71.,   1.,   1.,  15.,  71.,   8.,  17.,  70.,\n",
      "           1.,   1.,   1.,   2.,  49.,  97., 100.,   1.,   1.,  62.,  89.,  30.,\n",
      "          67.,  29.,  50.,  87.,  82.,   1.,   1.,  55.,  52.,  21.,  -1.,  49.,\n",
      "          27.,  11.,  89.,  60.,   1.,  79.,  43.,  35.,  89.,  97.,  27.,  97.,\n",
      "          97.,  73.,  84.,  97.,  61.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [ 58.,  88.,  49.,  30.,   1.,   1.,  61.,   1.,   1.,   2.,  87.,  92.,\n",
      "          26.,  77.,  10.,  10.,  65.,  87.,  89.,  32.,   1.,   1.,  12.,  49.,\n",
      "          32.,   2.,  66.,  37.,  57.,   1.,   1.,   2.,   5.,  65.,  78.,   1.,\n",
      "           1.,   2.,  75.,  26.,  97.,  42.,  93.,   2.,   1.,   1.,   1.,   1.,\n",
      "          26.,  49.,  -1.,  31.,  59.,  87.,  51.,  15.,  21.,  21.,  81.,  79.,\n",
      "          95.,   2.,  73.,  79.,  79.,  51.,  67.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.]])\n",
      "tensor([[50., 50., 26., 36., 50., 50., 40., 44., 29., 36., 46., 50., 50., 21.,\n",
      "         46., 27., 50., 50., 50., 11., 38.,  8., 33., 46., 50., 50., 50., 24.,\n",
      "         48., 49., 50., 50., 26., 39., 50., 32., 44., 24., 43., 48., 41., 50.,\n",
      "         50., 38., 33., 22., -1., 26., 17., 16., 47., 31.,  9., 47., 28., 26.,\n",
      "         50., 49., 14., 50., 50., 50., 45., 49., 37.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.],\n",
      "        [39., 47., 33., 15., 50., 50., 38., 50., 50.,  1., 45., 50., 38., 40.,\n",
      "         22., 32., 39., 44., 48., 26., 50., 50., 24., 32., 46., 50., 39., 25.,\n",
      "         30., 50., 50.,  2., 12., 38., 50., 50., 50., 50., 42., 40., 49., 29.,\n",
      "         50., 50., 50., 50., 50., 50., 24., 33., -1., 29., 38., 45., 37., 10.,\n",
      "         15., 18., 42., 44., 49.,  5., 39., 41., 47., 28., 44.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "def Heur2(cnp,Chr,CNV,End):\n",
    "    max_step=int(Chr.shape[1])\n",
    "    \n",
    "    for i in range(cnp.shape[0]):\n",
    "        current_cnp=cnp[i:(i+1)]\n",
    "        current_locus=0\n",
    "        step=0\n",
    "        flag=False\n",
    "        if current_cnp.mean()<1.7:\n",
    "            flag=True\n",
    "        while(step<max_step and ((not flag) or current_locus<num_chromosome*chrom_width)):\n",
    "            if current_locus==num_chromosome*chrom_width:\n",
    "                flag=True\n",
    "                current_locus=0\n",
    "                current_cnp//=2\n",
    "                Chr[i][step]=-1\n",
    "                CNV[i][step]=-1\n",
    "                End[i][step]=-1\n",
    "                step+=1\n",
    "                continue\n",
    "            current_Chr=current_locus//chrom_width\n",
    "            base_line=2\n",
    "            if flag:\n",
    "                base_line=1\n",
    "            if current_cnp[0][0][current_Chr].mean()>base_line*(1.5):\n",
    "                current_cnp[0][0][current_Chr]-=1\n",
    "                Chr[i][step]=current_Chr+1\n",
    "                CNV[i][step]=1\n",
    "                End[i][step]=50\n",
    "                current_locus=current_Chr*chrom_width\n",
    "                step+=1\n",
    "                continue\n",
    "            elif current_cnp[0][0][current_Chr].mean()<base_line*(0.5):\n",
    "                current_cnp[0][0][current_Chr]+=1\n",
    "                Chr[i][step]=current_Chr+1\n",
    "                CNV[i][step]=2\n",
    "                End[i][step]=50\n",
    "                current_locus=current_Chr*chrom_width\n",
    "                step+=1\n",
    "                continue\n",
    "            current_start=current_locus%chrom_width\n",
    "            if (flag and current_cnp[0][0][current_Chr][current_start]==1) or ((not flag) and current_cnp[0][0][current_Chr][current_start]%2==0):\n",
    "                current_locus+=1\n",
    "                continue\n",
    "            elif current_cnp[0][0][current_Chr][current_start]<base_line:\n",
    "                current_cnv=1\n",
    "            else:\n",
    "                current_cnv=-1\n",
    "            current_end=current_start\n",
    "            while(current_end<50):\n",
    "                if current_cnp[0][0][current_Chr][current_end]!=current_cnp[0][0][current_Chr][current_start]:\n",
    "                    break\n",
    "                current_end+=1\n",
    "            current_cnp[0,0,current_Chr,current_start:current_end]+=current_cnv\n",
    "            Chr[i][step]=current_Chr+1\n",
    "            CNV[i][step]=2*current_start+(current_cnv+1)//2+1\n",
    "            End[i][step]=current_end\n",
    "            step=step+1\n",
    "    return Chr,CNV,End,current_cnp\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #loading simulated data\n",
    "    num_step=50\n",
    "    Chr_truth,CNV_truth,End_truth,state=Simulate_data(batch_size=1,Number_of_step=num_step)\n",
    "    Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "    CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "    End=torch.zeros(state.shape[0],num_step*2)\n",
    "    #deconvolution\n",
    "    Chr,CNV,End,state=Heur2(state,Chr,CNV,End)\n",
    "    print((state-1).abs().sum())\n",
    "    print(Chr)\n",
    "    #check if the model picks the correct chromosome\n",
    "    print(CNV)\n",
    "    print(End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dirc=\"/data/suzaku/ted/HOME/noWGD\"\n",
    "num_step=50\n",
    "\n",
    "Chr_truth,CNV_truth,End_truth,state=Simulate_data(batch_size=50,Number_of_step=num_step)\n",
    "pd.DataFrame(Chr_truth.numpy()).astype(int).to_csv(dirc+\"Chr_truth.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV_truth.numpy()).astype(int).to_csv(dirc+\"CNV_truth.csv\",header=False,index=False)\n",
    "pd.DataFrame(End_truth.numpy()).astype(int).to_csv(dirc+\"End_truth.csv\",header=False,index=False)\n",
    "\n",
    "Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "End=torch.zeros(state.shape[0],num_step*2)\n",
    "state_copy=state.clone()\n",
    "Chr,CNV,End,state=Heur1(state,Chr,CNV,End)\n",
    "\n",
    "pd.DataFrame(Chr.numpy()).astype(int).to_csv(dirc+\"Chr_Heur1.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV.numpy()).astype(int).to_csv(dirc+\"CNV_Heur1.csv\",header=False,index=False)\n",
    "pd.DataFrame(End.numpy()).astype(int).to_csv(dirc+\"End_Heur1.csv\",header=False,index=False)\n",
    "\n",
    "state=state_copy.clone()\n",
    "Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "End=torch.zeros(state.shape[0],num_step*2)\n",
    "Chr,CNV,End,state=Heur2(state,Chr,CNV,End)\n",
    "\n",
    "pd.DataFrame(Chr.numpy()).astype(int).to_csv(dirc+\"Chr_Heur2.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV.numpy()).astype(int).to_csv(dirc+\"CNV_Heur2.csv\",header=False,index=False)\n",
    "pd.DataFrame(End.numpy()).astype(int).to_csv(dirc+\"End_Heur2.csv\",header=False,index=False)\n",
    "\n",
    "state=state_copy.clone()\n",
    "Chr=torch.zeros(state.shape[0],num_step*2)\n",
    "CNV=torch.zeros(state.shape[0],num_step*2)\n",
    "End=torch.zeros(state.shape[0],num_step*2)\n",
    "Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "CNV[Chr>0.5]=CNV[Chr>0.5]+1\n",
    "pd.DataFrame(Chr.numpy()).astype(int).to_csv(dirc+\"Chr_RL.csv\",header=False,index=False)\n",
    "pd.DataFrame(CNV.numpy()).astype(int).to_csv(dirc+\"CNV_RL.csv\",header=False,index=False)\n",
    "pd.DataFrame(End.numpy()).astype(int).to_csv(dirc+\"End_RL.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9621]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state=torch.ones(1,1,num_chromosome,50)\n",
    "state[0][0][:((num_chromosome//2)+0),:]=2\n",
    "state[0][0][((num_chromosome//2)+0):,:]=2\n",
    "model.switch.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0021]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.switch.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(state).mean(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 1207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0][0][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.5296, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 1208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Chrom_model(state,model.switch.forward(state))[0][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])"
      ]
     },
     "execution_count": 1320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0589]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 1284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,1]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,1]).view((22,50))\n",
    "model.switch.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major=np.loadtxt(\"/data/suzaku/ted/WGD/major_Kuramoch_comp_samp.txt\")\n",
    "minor=np.loadtxt(\"/data/suzaku/ted/WGD/minor_Kuramoch_comp_samp.txt\")\n",
    "major.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  4.,  5.,  6.,  6.,  6.,  6.,  8.,  8.,  8.,  8.,  8.,  9.,\n",
      "         10., 10., 10., 11., 12., 12., 12., 13., 13., 14., 15., 16., 16., 17.,\n",
      "         17., 18., 19., 19., 20., 20., 21., 21., 21., 23., 24., 25., 25., 26.,\n",
      "         27., 27., 27., 28., 28., 28., 29., 30., 31., 32., 32., 33., 34., 35.,\n",
      "         36., 36., 37., 38., 38., 39., 40., 41., 41., 42., 43., 43., -1.,  1.,\n",
      "          1.,  5.,  6.,  6.,  6.,  6.,  7.,  8.,  8.,  8.,  8., 10., 12., 13.,\n",
      "         13., 14., 14., 14., 16., 17., 17., 19., 19., 20., 21., 21., 21., 21.,\n",
      "         21., 21., 21., 23., 23., 25., 25., 27., 28., 28., 29., 29., 30., 32.,\n",
      "         32., 32., 33., 35., 35., 36., 36., 36., 36., 37., 39., 39., 40., 41.,\n",
      "         43., 44.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[ 1.,  1.,  1., 39., 37., 19., 71.,  3., 37., 95., 87., 89.,  1., 11.,\n",
      "          1., 23., 33., 99., 91., 19.,  1., 15., 11., 95.,  1., 47., 71.,  7.,\n",
      "         21.,  1., 21., 49.,  1., 47.,  9.,  1.,  7.,  1.,  1., 43., 29.,  1.,\n",
      "         97., 67.,  1.,  1., 19., 41., 61., 37.,  1., 77., 47., 19.,  1., 11.,\n",
      "         97.,  3.,  1., 71.,  1., 37.,  1., 93.,  1.,  1.,  1.,  3., -1., 42.,\n",
      "          0.,  0., 18., 70., 40.,  2., 99., 88., 36., 60., 86.,  0., 18., 10.,\n",
      "         14.,  1., 16., 94., 68., 99.,  0., 48., 48., 54.,  4.,  4.,  4.,  0.,\n",
      "          8.,  1.,  0., 42., 42.,  1., 28., 39., 18., 37.,  1., 99.,  1.,  1.,\n",
      "         71., 22.,  1., 81., 10., 96., 33.,  1., 99.,  5.,  1., 45., 15., 81.,\n",
      "         27.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[21., 10., 50., 34., 20., 11., 49.,  9., 19., 50., 44., 47., 14., 47.,\n",
      "         16., 13., 50., 50., 50., 12.,  3., 40.,  6., 50.,  2., 34., 50., 24.,\n",
      "         20., 50., 24., 40.,  2., 27., 50.,  2.,  4., 21., 50., 50., 15., 50.,\n",
      "         50., 34.,  9., 18., 11., 50., 35., 19., 50., 50., 35., 50., 50., 40.,\n",
      "         49.,  8.,  2., 50., 34., 22.,  7., 50., 40., 50., 13.,  2., -1., 23.,\n",
      "         23., 19., 11., 49., 22., 11., 50., 47., 25., 50., 50., 16., 12.,  6.,\n",
      "         40.,  1., 16., 50., 35., 50., 20., 40., 41., 50.,  3.,  3.,  3.,  1.,\n",
      "         50.,  2.,  3., 23., 23., 21., 15., 33., 11., 20., 19., 50., 18., 23.,\n",
      "         38., 13.,  9., 50.,  6., 49., 48.,  1., 50., 50., 18., 50., 50., 46.,\n",
      "         50., 50.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[  0,   0],\n",
      "        [  0,   1],\n",
      "        [  0,   2],\n",
      "        [  0,   3],\n",
      "        [  0,   4],\n",
      "        [  0,   5],\n",
      "        [  0,   6],\n",
      "        [  0,   7],\n",
      "        [  0,   8],\n",
      "        [  0,   9],\n",
      "        [  0,  10],\n",
      "        [  0,  11],\n",
      "        [  0,  12],\n",
      "        [  0,  13],\n",
      "        [  0,  14],\n",
      "        [  0,  15],\n",
      "        [  0,  16],\n",
      "        [  0,  17],\n",
      "        [  0,  18],\n",
      "        [  0,  19],\n",
      "        [  0,  20],\n",
      "        [  0,  21],\n",
      "        [  0,  22],\n",
      "        [  0,  23],\n",
      "        [  0,  24],\n",
      "        [  0,  25],\n",
      "        [  0,  26],\n",
      "        [  0,  27],\n",
      "        [  0,  28],\n",
      "        [  0,  29],\n",
      "        [  0,  30],\n",
      "        [  0,  31],\n",
      "        [  0,  32],\n",
      "        [  0,  33],\n",
      "        [  0,  34],\n",
      "        [  0,  35],\n",
      "        [  0,  36],\n",
      "        [  0,  37],\n",
      "        [  0,  38],\n",
      "        [  0,  39],\n",
      "        [  0,  40],\n",
      "        [  0,  41],\n",
      "        [  0,  42],\n",
      "        [  0,  43],\n",
      "        [  0,  44],\n",
      "        [  0,  45],\n",
      "        [  0,  46],\n",
      "        [  0,  47],\n",
      "        [  0,  48],\n",
      "        [  0,  49],\n",
      "        [  0,  50],\n",
      "        [  0,  51],\n",
      "        [  0,  52],\n",
      "        [  0,  53],\n",
      "        [  0,  54],\n",
      "        [  0,  55],\n",
      "        [  0,  56],\n",
      "        [  0,  57],\n",
      "        [  0,  58],\n",
      "        [  0,  59],\n",
      "        [  0,  60],\n",
      "        [  0,  61],\n",
      "        [  0,  62],\n",
      "        [  0,  63],\n",
      "        [  0,  64],\n",
      "        [  0,  65],\n",
      "        [  0,  66],\n",
      "        [  0,  67],\n",
      "        [  0,  68],\n",
      "        [  0,  69],\n",
      "        [  0,  70],\n",
      "        [  0,  71],\n",
      "        [  0,  72],\n",
      "        [  0,  73],\n",
      "        [  0,  74],\n",
      "        [  0,  75],\n",
      "        [  0,  76],\n",
      "        [  0,  77],\n",
      "        [  0,  78],\n",
      "        [  0,  79],\n",
      "        [  0,  80],\n",
      "        [  0,  81],\n",
      "        [  0,  82],\n",
      "        [  0,  83],\n",
      "        [  0,  84],\n",
      "        [  0,  85],\n",
      "        [  0,  86],\n",
      "        [  0,  87],\n",
      "        [  0,  88],\n",
      "        [  0,  89],\n",
      "        [  0,  90],\n",
      "        [  0,  91],\n",
      "        [  0,  92],\n",
      "        [  0,  93],\n",
      "        [  0,  94],\n",
      "        [  0,  95],\n",
      "        [  0,  96],\n",
      "        [  0,  97],\n",
      "        [  0,  98],\n",
      "        [  0,  99],\n",
      "        [  0, 100],\n",
      "        [  0, 101],\n",
      "        [  0, 102],\n",
      "        [  0, 103],\n",
      "        [  0, 104],\n",
      "        [  0, 105],\n",
      "        [  0, 106],\n",
      "        [  0, 107],\n",
      "        [  0, 108],\n",
      "        [  0, 109],\n",
      "        [  0, 110],\n",
      "        [  0, 111],\n",
      "        [  0, 112],\n",
      "        [  0, 113],\n",
      "        [  0, 114],\n",
      "        [  0, 115],\n",
      "        [  0, 116],\n",
      "        [  0, 117],\n",
      "        [  0, 118],\n",
      "        [  0, 119],\n",
      "        [  0, 120],\n",
      "        [  0, 121],\n",
      "        [  0, 122],\n",
      "        [  0, 123],\n",
      "        [  0, 124],\n",
      "        [  0, 125],\n",
      "        [  0, 126],\n",
      "        [  0, 127]])\n"
     ]
    }
   ],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,0]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,0]).view((22,50))\n",
    "Chr=torch.zeros(state.shape[0],200)\n",
    "CNV=torch.zeros(state.shape[0],200)\n",
    "End=torch.zeros(state.shape[0],200)\n",
    "state_copy=state.clone()\n",
    "#deconvolution\n",
    "Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "print(Chr)\n",
    "#check if the model picks the correct chromosome\n",
    "print(CNV)\n",
    "print(End)\n",
    "print(torch.nonzero(Chr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  2.,  4.,  5.,  6.,  6.,  6.,  6.,  8.,  8.,  8.,  8.,\n",
      "          9., 10., 10., 11., 12., 12., 12., 15., 16., 16., 17., 17., 18., 19.,\n",
      "         19., 20., 20., 20., 21., 21., 23., 23., 23., 24., 25., 25., 26., 27.,\n",
      "         27., 28., 28., 28., 28., 29., 31., 32., 33., 34., 35., 36., 37., 38.,\n",
      "         38., 40., 41., 41., 42., 43., 43., -1.,  1.,  1.,  1.,  1.,  1.,  5.,\n",
      "          6.,  6.,  6.,  6.,  7.,  7.,  8.,  8.,  8., 10., 12., 13., 14., 14.,\n",
      "         16., 17., 17., 17., 19., 20., 21., 21., 21., 21., 21., 23., 25., 25.,\n",
      "         27., 28., 28., 28., 28., 28., 29., 29., 30., 33., 35., 36., 36., 37.,\n",
      "         39., 39., 40., 41., 44.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[43., 15.,  1.,  1.,  1., 39., 37.,  3., 19., 41., 95., 87., 89.,  1.,\n",
      "         11.,  1., 33., 99., 19.,  1., 25.,  1., 47., 71.,  1., 45.,  1., 64.,\n",
      "          1., 95., 55.,  1.,  5., 49., 13., 15.,  1.,  1., 43., 29.,  1., 67.,\n",
      "          1., 25.,  1., 45., 41., 39.,  1.,  1., 19.,  1., 13.,  3.,  1., 71.,\n",
      "          1.,  1., 93.,  1.,  1.,  1.,  7., -1., 42., 42., 42., 42.,  0.,  0.,\n",
      "         18., 18., 40.,  2., 99., 72., 88., 86., 82.,  0., 18., 10.,  1., 16.,\n",
      "         68., 99.,  0., 36., 48., 54.,  4.,  4., 48.,  4.,  8., 12.,  1., 28.,\n",
      "         39., 41., 40.,  1., 37.,  0.,  1., 99.,  1.,  1., 81., 33.,  1.,  5.,\n",
      "          1., 41., 15., 81.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[23., 21.,  6., 11., 50., 33., 20., 12., 11., 21., 50., 44., 47., 14.,\n",
      "         47., 16., 50., 50., 12.,  9., 50.,  2., 34., 50., 13., 24., 50., 40.,\n",
      "         24., 50., 33., 27., 16., 50.,  7., 23.,  6., 50., 50., 15., 50., 50.,\n",
      "          9., 18.,  9., 50., 21., 21., 50., 50., 50., 50., 40.,  8.,  2., 50.,\n",
      "         34.,  7., 50., 40., 50.,  2., 50., -1., 23., 23., 23., 23., 23., 19.,\n",
      "         11., 11., 22., 12., 50., 41., 50., 47., 50., 16., 12.,  6.,  1., 14.,\n",
      "         35., 50., 13., 20., 32., 50.,  3.,  3., 50.,  3., 16.,  7., 21., 15.,\n",
      "         33., 50., 50., 12., 20.,  9., 19., 50., 18.,  9., 50., 50.,  1., 50.,\n",
      "         18., 50., 50., 46., 50.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]])\n",
      "tensor([[  0,   0],\n",
      "        [  0,   1],\n",
      "        [  0,   2],\n",
      "        [  0,   3],\n",
      "        [  0,   4],\n",
      "        [  0,   5],\n",
      "        [  0,   6],\n",
      "        [  0,   7],\n",
      "        [  0,   8],\n",
      "        [  0,   9],\n",
      "        [  0,  10],\n",
      "        [  0,  11],\n",
      "        [  0,  12],\n",
      "        [  0,  13],\n",
      "        [  0,  14],\n",
      "        [  0,  15],\n",
      "        [  0,  16],\n",
      "        [  0,  17],\n",
      "        [  0,  18],\n",
      "        [  0,  19],\n",
      "        [  0,  20],\n",
      "        [  0,  21],\n",
      "        [  0,  22],\n",
      "        [  0,  23],\n",
      "        [  0,  24],\n",
      "        [  0,  25],\n",
      "        [  0,  26],\n",
      "        [  0,  27],\n",
      "        [  0,  28],\n",
      "        [  0,  29],\n",
      "        [  0,  30],\n",
      "        [  0,  31],\n",
      "        [  0,  32],\n",
      "        [  0,  33],\n",
      "        [  0,  34],\n",
      "        [  0,  35],\n",
      "        [  0,  36],\n",
      "        [  0,  37],\n",
      "        [  0,  38],\n",
      "        [  0,  39],\n",
      "        [  0,  40],\n",
      "        [  0,  41],\n",
      "        [  0,  42],\n",
      "        [  0,  43],\n",
      "        [  0,  44],\n",
      "        [  0,  45],\n",
      "        [  0,  46],\n",
      "        [  0,  47],\n",
      "        [  0,  48],\n",
      "        [  0,  49],\n",
      "        [  0,  50],\n",
      "        [  0,  51],\n",
      "        [  0,  52],\n",
      "        [  0,  53],\n",
      "        [  0,  54],\n",
      "        [  0,  55],\n",
      "        [  0,  56],\n",
      "        [  0,  57],\n",
      "        [  0,  58],\n",
      "        [  0,  59],\n",
      "        [  0,  60],\n",
      "        [  0,  61],\n",
      "        [  0,  62],\n",
      "        [  0,  63],\n",
      "        [  0,  64],\n",
      "        [  0,  65],\n",
      "        [  0,  66],\n",
      "        [  0,  67],\n",
      "        [  0,  68],\n",
      "        [  0,  69],\n",
      "        [  0,  70],\n",
      "        [  0,  71],\n",
      "        [  0,  72],\n",
      "        [  0,  73],\n",
      "        [  0,  74],\n",
      "        [  0,  75],\n",
      "        [  0,  76],\n",
      "        [  0,  77],\n",
      "        [  0,  78],\n",
      "        [  0,  79],\n",
      "        [  0,  80],\n",
      "        [  0,  81],\n",
      "        [  0,  82],\n",
      "        [  0,  83],\n",
      "        [  0,  84],\n",
      "        [  0,  85],\n",
      "        [  0,  86],\n",
      "        [  0,  87],\n",
      "        [  0,  88],\n",
      "        [  0,  89],\n",
      "        [  0,  90],\n",
      "        [  0,  91],\n",
      "        [  0,  92],\n",
      "        [  0,  93],\n",
      "        [  0,  94],\n",
      "        [  0,  95],\n",
      "        [  0,  96],\n",
      "        [  0,  97],\n",
      "        [  0,  98],\n",
      "        [  0,  99],\n",
      "        [  0, 100],\n",
      "        [  0, 101],\n",
      "        [  0, 102],\n",
      "        [  0, 103],\n",
      "        [  0, 104],\n",
      "        [  0, 105],\n",
      "        [  0, 106],\n",
      "        [  0, 107],\n",
      "        [  0, 108],\n",
      "        [  0, 109],\n",
      "        [  0, 110],\n",
      "        [  0, 111],\n",
      "        [  0, 112],\n",
      "        [  0, 113],\n",
      "        [  0, 114],\n",
      "        [  0, 115],\n",
      "        [  0, 116]])\n"
     ]
    }
   ],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,1]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,1]).view((22,50))\n",
    "Chr=torch.zeros(state.shape[0],200)\n",
    "CNV=torch.zeros(state.shape[0],200)\n",
    "End=torch.zeros(state.shape[0],200)\n",
    "state_copy=state.clone()\n",
    "#deconvolution\n",
    "Chr,CNV,End,state=Deconvolute(model,state,Chr,CNV,End)\n",
    "print(Chr)\n",
    "#check if the model picks the correct chromosome\n",
    "print(CNV)\n",
    "print(End)\n",
    "print(torch.nonzero(Chr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[0][0][:22]=torch.Tensor(major[:,1]).view((22,50))\n",
    "state[0][0][22:]=torch.Tensor(minor[:,1]).view((22,50))\n",
    "model.switch(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLevolution",
   "language": "python",
   "name": "rlevolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
