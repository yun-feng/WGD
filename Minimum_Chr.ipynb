{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minimum_Chr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMv4PfnEyN9L4luRZ4iaUSl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yun-feng/WGD/blob/master/Minimum_Chr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKOvQcN-jHHv",
        "colab_type": "code",
        "outputId": "0f9e7074-5e9e-456c-f7ba-c1275c5d7b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#This is a smaller example of our model\n",
        "#In this example, we exclude the last neural net Q_{\\phi_3} as described in section 3.3.1\n",
        "#Thus, all CNV will have an ending point which is the end of the chromosome\n",
        "#We hope this example is clearer in explaining the general framework\n",
        "\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Reward.py\n",
        "#Define the reward function r(s,a) when a is not a special action END\n",
        "\n",
        "#For each chromosome, we consider 50 SNP loci\n",
        "chrom_width=50;\n",
        "#44 normal chromosomes in human genome\n",
        "num_chromosome=44\n",
        "\n",
        "\n",
        "#normalisation constant to make normal_const*(\\sum_i a_i) <1, so that the possibility of all CNV sum to a real value smaller than 1\n",
        "normal_const=5e-5;\n",
        "#probability of single locus gain/loss\n",
        "single_loci_loss=normal_const*(1-2e-1);\n",
        "#probability of WGD\n",
        "WGD=normal_const*0.6;\n",
        "\n",
        "#log probability of CNV\n",
        "#used for calculating the distribution of p(a) when a is a focal CNV\n",
        "const1=normal_const*(1-1e-1);\n",
        "const2=2;\n",
        "\n",
        "#Whole chromosome change probability\n",
        "Whole_Chromosome_CNV=normal_const*0.99/10;\n",
        "Half_Chromosome_CNV=normal_const*0.99/15;\n",
        "\n",
        "#The ending point is always the end of the chromosome\n",
        "def Reward(Start,End=chrom_width):\n",
        "  Start=Start.to(torch.float32)\n",
        "  End=End.to(torch.float32)\n",
        "  reward=torch.log(const1/(const2+torch.log(End-Start)))\n",
        "  #chromosome changes\n",
        "  for i in range(Start.shape[0]):\n",
        "    #full chromosome\n",
        "    if End[i]-Start[i]>chrom_width-0.5:\n",
        "      reward[i]=math.log(Whole_Chromosome_CNV)\n",
        "    #arm level changes\n",
        "    if chrom_width-End[i]<0.5 and abs(chrom_width//2-Start[i])<1.5:\n",
        "      reward[i]=math.log(Half_Chromosome_CNV)\n",
        "    if Start[i]<0.5 and abs(chrom_width//2-End[i])<1.5:\n",
        "      reward[i]=math.log(Half_Chromosome_CNV)\n",
        "  return reward\n",
        "\n",
        "\n",
        "\n",
        "#Q-function.py\n",
        "#defining the Q-function \n",
        "#Q(s,a) in manuscript\n",
        "\n",
        "\n",
        "#switch structure mentioned in section 3.3.4\n",
        "#kernel sizes for convolution layers\n",
        "nkernels_switch = [40,60,120]\n",
        "class WGD_Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(WGD_Net, self).__init__()\n",
        "    #chromosome permutation invariant structure as described in section 3.3.3\n",
        "    #slide for chromosome is 1 and the filter length in this dimension is also 1\n",
        "    #thus, the same filter goes through all chromosomes in the same fashion\n",
        "    self.conv1=nn.Conv2d(1, nkernels_switch[0], (1,3),(1,1),(0,1))\n",
        "    self.conv2=nn.Conv2d(nkernels_switch[0],nkernels_switch[1] , (1,3),(1,1), (0,1))\n",
        "    self.conv3=nn.Conv2d(nkernels_switch[1],nkernels_switch[2] , (1,5),(1,1), (0,0))\n",
        "    self.linear=nn.Linear(nkernels_switch[2],1)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    y=x.mean((1,2,3))\n",
        "    y=y.reshape(x.shape[0],1)\n",
        "    x=x.reshape(x.shape[0],1,num_chromosome,chrom_width)\n",
        "    x=F.max_pool2d(F.relu(self.conv1(x)),(1,5),(1,5),(0,0))\n",
        "    x=F.max_pool2d(F.relu(self.conv2(x)),(1,2),(1,2),(0,0))\n",
        "    x=(F.relu(self.conv3(x))).sum((2,3))\n",
        "    x=self.linear(x)\n",
        "    #residule representation in x as described in section 3.3.4\n",
        "    x=20*(y-1.5)+x\n",
        "    x=torch.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "#chromosome evaluation net \n",
        "#Used in Chrom_NN (which is Q_{phi_1}(s,c) in section 3.3.1)\n",
        "#kernel sizes for convolution layers\n",
        "nkernels_chr = [80,120,160]\n",
        "class CNP_Val(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNP_Val, self).__init__()\n",
        "    self.conv1=nn.Conv2d(1, nkernels_chr[0], (1,5),(1,1),(0,2))\n",
        "    self.conv2=nn.Conv2d(nkernels_chr[0],nkernels_chr[1] , (1,3),(1,1), (0,1))\n",
        "    self.conv3=nn.Conv2d(nkernels_chr[1],nkernels_chr[2] , (1,3),(1,1), (0,1))\n",
        "    self.conv4=nn.Conv2d(nkernels_chr[2],1, (1,5),(1,1), (0,0))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x=F.max_pool2d(F.relu(self.conv1(x)),(1,3),(1,3),(0,1))\n",
        "    x=F.max_pool2d(F.relu(self.conv2(x)),(1,2),(1,2),(0,1))\n",
        "    x=F.max_pool2d(F.relu(self.conv3(x)),(1,2),(1,2),(0,1))\n",
        "    #KL divergence is always nonpositive\n",
        "    x=0.25+F.elu(self.conv4(x),0.25)\n",
        "    #number of sample * 44 chromosomes\n",
        "    x=x.reshape(x.shape[0],num_chromosome)\n",
        "    return x\n",
        "\n",
        "#Implemts Q_{phi_1}(s,c) in section 3.3.1\n",
        "#It combines two chromosome evaluation nets mentioned above,\n",
        "#with a switch structure in section 3.3.4 to form Q_{phi_1}(s,c)\n",
        "class Chrom_NN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Chrom_NN,self).__init__()\n",
        "    #two parts of the Chrom_NN\n",
        "    #NN for CNP without WGD \n",
        "    self.Val_noWGD=CNP_Val()\n",
        "    #NN for CNP with WGD\n",
        "    self.Val_WGD=CNP_Val()\n",
        "    \n",
        "  def forward(self,x,sigma):\n",
        "    #probability for WGD, which is computed by switch structure\n",
        "    sigma=sigma.expand(-1,num_chromosome)\n",
        "    #we assume the copy number for each loci ranges from 0~9\n",
        "    #for samples without WGD\n",
        "\n",
        "    #y represents if a chromosome have abnormal copy numbers (positions with copy number other than 1)\n",
        "    y=torch.ceil((torch.abs(x-1)).mean(3)/12)\n",
        "    y=y.reshape(x.shape[0],num_chromosome)\n",
        "    #Residule representation mentioned in section 3.3.4\n",
        "    #the value for Q_{phi_1}(s,c) is computed as Val_no (the value estimated by the neural net, the residule part)+ y (the empirial estimation) \n",
        "    Val_no=self.Val_noWGD.forward(x)\n",
        "    Val_no=y*math.log(single_loci_loss)*2+(((1-y)*Val_no).sum(1)).reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
        "    #for samples with WGD\n",
        "    #it is similar to the previsou part, where z is an equivalent for y and Val_wgd is an equivalent for Val_no\n",
        "    z=torch.ceil((torch.abs(x-2*(x//2))).sum(3)/100)\n",
        "    z=z.reshape(x.shape[0],num_chromosome)\n",
        "    Val_wgd=self.Val_WGD.forward(x)\n",
        "    Val_wgd=z*math.log(single_loci_loss)*2+((1-z)*Val_wgd).sum(1).reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
        "    \n",
        "    #combine two NN with switch as defined in Section 3.3.4\n",
        "    x=sigma*Val_wgd+(1-sigma)*Val_no\n",
        "    x=-x\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "#starting point and gain or loss (defined as sp in manuscript) \n",
        "#Used in CNV_NN (which is Q_{phi_2}(s,c,sp) on section 3.3.1)\n",
        "#kernel sizes for convolution layers\n",
        "nkernels_CNV = [80,120,160,10]\n",
        "class CNV_Val(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNV_Val,self).__init__()\n",
        "    self.conv1=nn.Conv2d(1, nkernels_CNV[0], (1,7),(1,1),(0,3))\n",
        "    self.conv2=nn.Conv2d(nkernels_CNV[0],nkernels_CNV[1] , (1,7),(1,1), (0,3))\n",
        "    self.conv3=nn.Conv2d(nkernels_CNV[1],nkernels_CNV[2] , (1,7),(1,1), (0,3))\n",
        "    self.conv4=nn.Conv2d(nkernels_CNV[2], nkernels_CNV[3], (1,7),(1,1), (0,3))\n",
        "    self.linear=nn.Linear(nkernels_CNV[3]*chrom_width,2*chrom_width-1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=F.relu(self.conv1(x))\n",
        "    x=F.relu(self.conv2(x))\n",
        "    x=F.relu(self.conv3(x))\n",
        "    x=F.relu(self.conv4(x))\n",
        "    x=x.reshape(x.shape[0],nkernels_CNV[3]*chrom_width)\n",
        "    x=self.linear(x)\n",
        "    #number of samples* [(50 regions)*(2(gain or loss))-1] \n",
        "    #Only have 50*2-1=99 output dimensions because we fix the average these output\n",
        "    #The average of them could be arbitrary because of the partitioning\n",
        "    return x\n",
        "\n",
        "#Implemts Q_{phi_2}(s,c,sp) in section 3.3.1\n",
        "#It combines two CNV_Val nets mentioned above,\n",
        "#with a switch structure in section 3.3.4 to form Q_{phi_2}(s,c,sp)\n",
        "class CNV_NN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNV_NN,self).__init__()\n",
        "    #two network setting\n",
        "    self.CNV_noWGD=CNV_Val()\n",
        "    self.CNV_WGD=CNV_Val()\n",
        "    \n",
        "  def forward(self,x,sigma):\n",
        "    #as in section 3.3.4\n",
        "    #y is the empirical estimation\n",
        "    #Val_no is the redidule representation\n",
        "    y=torch.Tensor(x.shape[0],chrom_width,2)\n",
        "    y[:,:,0]=F.relu(1-x)\n",
        "    y[:,:,1]=F.relu(x-1)\n",
        "    y=y.reshape(x.shape[0],2*chrom_width)\n",
        "    y=y[:,1:(2*chrom_width)]-y[:,0:1].expand(-1,2*chrom_width-1)\n",
        "    Val_no=self.CNV_noWGD.forward(x.reshape(x.shape[0],1,1,chrom_width))\n",
        "    Val_no=y+Val_no\n",
        "    \n",
        "    z=((torch.abs(x-2*(x//2))).reshape(x.shape[0],chrom_width,1)).expand(-1,-1,2)\n",
        "    z=z.reshape(x.shape[0],2*chrom_width)\n",
        "    z=z[:,1:(2*chrom_width)]-z[:,0:1].expand(-1,2*chrom_width-1)\n",
        "    Val_wgd=self.CNV_WGD.forward(x.reshape(x.shape[0],1,1,chrom_width))\n",
        "    Val_wgd=z+Val_wgd\n",
        "    #switch\n",
        "    x=sigma*Val_wgd+(1-sigma)*Val_no\n",
        "    return(x)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#combine all separate networks\n",
        "#add Rule system\n",
        "\n",
        "#calculating the softmax\n",
        "#prevent inf when taking log(exp(x))\n",
        "#log_exp is always gonna be between 1 and the total number of elements\n",
        "def Soft_update(val1,soft1,val2,soft2):\n",
        "  bias=val1.clone()\n",
        "  log_exp=soft1.clone()\n",
        "  set1=[torch.ge(val1,val2)]\n",
        "  bias[set1]=val1[set1]\n",
        "  log_exp[set1]=soft1[set1]+soft2[set1]*torch.exp(val2[set1]-val1[set1])\n",
        "  set2=[torch.lt(val1,val2)]\n",
        "  bias[set2]=val2[set2]\n",
        "  log_exp[set2]=soft2[set2]+soft1[set2]*torch.exp(val1[set2]-val2[set2])\n",
        "  return bias,log_exp\n",
        "\n",
        "\n",
        "\n",
        "#Combine all the separate modules mentioned above\n",
        "#Implementation of Q(s,a)\n",
        "class Q_learning(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Q_learning,self).__init__()\n",
        "    self.switch=WGD_Net()\n",
        "    #the output refer to Q_{\\phi_1}(s,c)\n",
        "    self.Chrom_model=Chrom_NN()\n",
        "    #the output refer to Q_{\\phi_2}(s,c,sp)\n",
        "    self.CNV=CNV_NN()\n",
        "  \n",
        "  \n",
        "  def forward(self,state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid):\n",
        "    '''\n",
        "    computing the final advantage(loss) used for training\n",
        "    loss in Thereom1\n",
        "    state: s in Q(s,a)\n",
        "    next_state: s' in softmaxQ(s',a')\n",
        "    Chr,cnv,end_loci: a in Q(s,a)\n",
        "    chrom,chrom_new,start_loci,end_loci: intermediate results from s,a, which is preprossed to make computation faster\n",
        "      e.g. chrom is CNP of the Chr(part of a) from the state(s)\n",
        "      They could be seen as a mapping without parameters to learn:f(s,a)\n",
        "      In this example, end_loci is not used at all\n",
        "    valid: a boolean array, indicating if a training sample is valid (e.g. have non negative copy numbers for all loci)\n",
        "    '''\n",
        "\n",
        "    #computing softmaxQ(s',a')\n",
        "    #It is a tradition in RL that gradient does not backpropogate through softmaxQ(s',a'), but only through Q(s,a) to make convergence faster\n",
        "    #there is no theoritical guarantee behind, and it is only a practical trick\n",
        "    sigma_next=self.switch(next_state)\n",
        "    sigma_next=sigma_next.detach() #gradient does not flow back through softmaxQ(s',a')\n",
        "    x,y=self.Softmax(next_state,sigma_next)\n",
        "    x=x+torch.log(y)\n",
        "    #computing r(s,a)\n",
        "    x=x+Reward(start_loci,end_loci)\n",
        "\n",
        "\n",
        "    #computing Q(s,a)\n",
        "    sigma=self.switch.forward(state)\n",
        "    #Q_{phi_1}(s,c)\n",
        "    res_chrom=self.Chrom_model.forward(state,sigma)\n",
        "    #Q_{phi_2}(s,c,sp)\n",
        "    res_cnv=self.CNV.forward(chrom,sigma)\n",
        "    #if there is originally a break point for start\n",
        "    #real world constraint as described in section 3.3.2\n",
        "    #only allow starting points (sp) to be the break points of CNP\n",
        "    break_start=torch.zeros(state.shape[0],chrom_width,2,requires_grad=False)\n",
        "    chrom_shift=torch.zeros(state.shape[0],chrom_width,requires_grad=False)\n",
        "    chrom_shift[:,1:]=chrom[:,:(chrom_width-1)]\n",
        "    #allow adding one copy for every breakpoint\n",
        "    break_start[:,:,1]=torch.ceil(torch.abs(chrom-chrom_shift)/10)\n",
        "    #always allow adding one chromosone\n",
        "    break_start[:,0,1]=1\n",
        "    #don't allow lose one copy when copy number equals 0, otherwise there is going to be negative copy numbers\n",
        "    break_start[:,:,0]=break_start[:,:,1]\n",
        "    break_start[:,:,0]=break_start[:,:,0]*torch.ceil(chrom/10)\n",
        "    break_start=break_start.reshape(state.shape[0],2*chrom_width)\n",
        "    res_cnv_full=torch.zeros(state.shape[0],2*chrom_width)\n",
        "    res_cnv_full[:,1:]=res_cnv\n",
        "    res_cnv_full=res_cnv_full+torch.log(break_start)\n",
        "    #Q_{phi_2}(s,c,sp)-softmax(Q_{phi_2}(s,c,sp)) as described in section 3.3.1\n",
        "    cnv_max_val,cnv_max=torch.max(res_cnv_full,1)\n",
        "    cnv_softmax=res_cnv_full-cnv_max_val.reshape(state.shape[0],1).expand(-1,2*chrom_width)\n",
        "    cnv_softmax=torch.exp(cnv_softmax).sum(1)\n",
        "    x=x+cnv_max_val+torch.log(cnv_softmax)\n",
        "    \n",
        "    #remove training data which include invalid actions\n",
        "    x=x*valid\n",
        "    #return avdantage as well as a best cnv and sigma used for generating training data\n",
        "    #used for training in the next step\n",
        "    return x,cnv_max,sigma,res_chrom,res_cnv_full\n",
        "  \n",
        "  def Softmax(self,next_state,sigma):\n",
        "    #compute softmax_{a'} Q(s',a')\n",
        "    x=self.Chrom_model.forward(next_state,sigma)\n",
        "    max_chrom=torch.max(x,1)[0]\n",
        "    softmax_chrom=x-max_chrom.reshape(x.shape[0],1).expand(-1,num_chromosome)\n",
        "    softmax_chrom=torch.exp(softmax_chrom).sum(1)\n",
        "    #special action END\n",
        "    #all the remaining abnormal loci are treated to be caused by several independent single locus copy number changes\n",
        "    end_val=torch.sum(torch.abs(next_state-1),(1,2,3))*math.log(single_loci_loss)\n",
        "    max_chrom,softmax_chrom=Soft_update(max_chrom,softmax_chrom,end_val,torch.ones(x.shape[0]))\n",
        "    #if there is a WGD followed immediately\n",
        "    for i in range(x.shape[0]):\n",
        "      #real world constraint as described in section 3.3.2\n",
        "      #do not allow (reversing) WGD when the CNP contain odd numbers for some loci\n",
        "      if (not torch.any(next_state[i]-2*torch.floor(next_state[i]/2)>0.5)) and torch.any(next_state[i]>0.5):\n",
        "        sigma_wgd=self.switch(torch.floor(next_state[i:(i+1)]/2))\n",
        "        sigma_wgd=sigma_wgd.detach()\n",
        "        wgd_val,wgd_soft=self.Softmax(torch.floor(next_state[i:(i+1)]/2),sigma_wgd)\n",
        "        max_chrom[i],softmax_chrom[i]=Soft_update(torch.ones(1)*max_chrom[i],torch.ones(1)*softmax_chrom[i],torch.ones(1)*wgd_val,torch.ones(1)*wgd_soft)\n",
        "    #in Q-learning\n",
        "    #gradient does not flow through the softmax part in order to mimic regression problems\n",
        "    max_chrom=max_chrom.detach()\n",
        "    softmax_chrom=softmax_chrom.detach()\n",
        "    return max_chrom,softmax_chrom\n",
        "\n",
        "batch_size=15\n",
        "#\n",
        "#during training\n",
        "#data are simulated backwards\n",
        "#when step==0, it means it is the last step for the trajectory\n",
        "#and step++ to make CNP more complex\n",
        "def Simulate_train_data(first_step_flag=True,state=None,next_state=None,advantage=None,Chr=None,step=None,wgd=None,valid=None):\n",
        "  #Simulate data for training (similar to the case when a machine is playing a game against itself)\n",
        "  #Thus, we don't need real world data during training, as long as the reward is similar to the real world probability\n",
        "  #As in theorem 1, there is no specific destribution required to compute expectation over (s,a) pairs\n",
        "  #Any distribution with broad support over all (s,a) will do the job\n",
        "  if first_step_flag:\n",
        "    #The first simulated sample\n",
        "    state=torch.ones(batch_size,1,44,chrom_width,requires_grad=False)\n",
        "    next_state=torch.ones(batch_size,1,44,chrom_width,requires_grad=False)\n",
        "    Chr=torch.ones(batch_size,requires_grad=False).type(torch.LongTensor)\n",
        "    step=torch.zeros(batch_size,requires_grad=False)\n",
        "    advantage=torch.zeros(batch_size)\n",
        "    wgd=torch.zeros(batch_size,requires_grad=False)\n",
        "    valid=torch.ones(batch_size,requires_grad=False)\n",
        "\n",
        "  #sample starting point, end point, gain or loss  \n",
        "  #because of the permutation invariant structure in section 3.3.3\n",
        "  #it is not necessary to resample the chromosome everytime\n",
        "  start_loci=torch.randint(high=chrom_width,size=(batch_size,),requires_grad=False)\n",
        "  end_loci=torch.LongTensor(batch_size)\n",
        "  cnv=torch.ones(batch_size,requires_grad=False)\n",
        "  chrom=torch.Tensor(batch_size,chrom_width)\n",
        "  chrom_new=torch.Tensor(batch_size,chrom_width)\n",
        "  #probability of resetting the training trajectory back to step=0\n",
        "  step_prob=0.15+0.4/(1+math.exp(-1e-4*counter_global+2))\n",
        "  for i in range(batch_size):\n",
        "    #if the model is poorly trained until the current step\n",
        "    #go back to the state 0\n",
        "    #to ensure small error for short trajectories\n",
        "    if(torch.rand(1)[0]>step_prob or torch.abs(advantage[i])>=15):\n",
        "      state[i]=torch.ones(1,44,chrom_width,requires_grad=False)\n",
        "      next_state[i]=torch.ones(1,44,chrom_width,requires_grad=False)\n",
        "      step[i]=0\n",
        "    #if model is fully trained for the current step\n",
        "    #and there is no invalid operations been sampled\n",
        "    #go to next step\n",
        "    elif(valid[i]>0 and torch.abs(advantage[i])<7):\n",
        "      next_state[i]=state[i].clone()\n",
        "      step[i]=step[i]+1\n",
        "    #stay to further train the current step\n",
        "    #or resample another action\n",
        "    else:\n",
        "      state[i]=next_state[i].clone()\n",
        "    \n",
        "    #reset advantage and valid after they have been checked\n",
        "    advantage[i]=0\n",
        "    valid[i]=1\n",
        "    #end loci are the end of chromosomes in this example\n",
        "    end_loci[i]=chrom_width\n",
        "\n",
        "    #change the chromosone that CNV is on with some probability\n",
        "    #otherwise, all CNV will be on the same chromosome\n",
        "    if torch.rand(1)[0]>0.5:\n",
        "      Chr[i]=torch.randint(high=44,size=(1,))[0]\n",
        "    #adding probability to sample whole chromosomal changes during training\n",
        "    if torch.rand(1)[0]>0.8:\n",
        "      start_loci[i]=0\n",
        "      end_loci[i]=chrom_width\n",
        "    #adding probability to sample losses starting from the start of chromosome\n",
        "    if torch.rand(1)[0]>0.7:\n",
        "      cnv[i]=0\n",
        "    #increasing the probability to sample WGD during training\n",
        "    prob_wgd=0.4/(1+math.exp(-step[i]+5))\n",
        "    #starting to modify state and next state\n",
        "    #extract preprocessing data\n",
        "    #wgd          \n",
        "    if (torch.abs(advantage[i])<2*chrom_width and torch.rand(1)[0]<prob_wgd and wgd[i]<1):\n",
        "      wgd[i]=1\n",
        "      state[i]=state[i]*2\n",
        "      next_state[i]=next_state[i]*2\n",
        "    #adding cnv effect\n",
        "    #increasing copies when no wgd\n",
        "    #decreasing copies when wgd\n",
        "    if wgd[i]>0.5:\n",
        "      cnv[i]=1-cnv[i]\n",
        "    state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]=state[i][0][Chr[i]][(start_loci[i]):(end_loci[i])]-(cnv[i]-0.5)*2\n",
        "    chrom[i]=state[i][0][Chr[i]][:]\n",
        "    #reverse effect on chrom_new\n",
        "    chrom_new[i]=state[i][0][Chr[i]][:]\n",
        "    chrom_new[i][(start_loci[i]):]=chrom_new[i][(start_loci[i]):]+(cnv[i]-0.5)*2\n",
        "    #not going to negative values\n",
        "    if(torch.any(state[i][0][Chr[i]][(start_loci[i])]< -0.5)):\n",
        "      valid[i]=0\n",
        "    #not joining breakpoints\n",
        "    if(start_loci[i]>0.5 and torch.abs(chrom[i][start_loci[i]]-chrom[i][start_loci[i]-1])<0.5):\n",
        "      valid[i]=0\n",
        "    if(end_loci[i]<chrom_width-0.5 and torch.abs(chrom[i][end_loci[i]-1]-chrom[i][end_loci[i]])<0.5):\n",
        "      valid[i]=0\n",
        "  return state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,wgd,step,advantage,valid\n",
        "\n",
        "import torch.optim as optim\n",
        "#setting up counter\n",
        "counter_global=0\n",
        "#Model\n",
        "Q_model=Q_learning()\n",
        "#Load initial data\n",
        "state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,wgd,step,advantage,valid=Simulate_train_data()\n",
        "#setting up optimizer\n",
        "optimizer = optim.Adam(Q_model.parameters(), lr=1e-3,betas=(0.9, 0.99), eps=1e-08, weight_decay=1e-6)\n",
        "\n",
        "\n",
        "#start training\n",
        "while(counter_global< 30):\n",
        "  counter_global=counter_global+1\n",
        "  #load data\n",
        "  state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,wgd,step,advantage,valid=Simulate_train_data(False,state,next_state,advantage,Chr,step,wgd,valid)\n",
        "  #compute advantage\n",
        "  optimizer.zero_grad()\n",
        "  advantage,cnv_max,sigma,temp,t2=Q_model.forward(state,next_state,chrom,chrom_new,Chr,cnv,start_loci,end_loci,valid)\n",
        "  #compute loss\n",
        "  loss=advantage.pow(2).mean()\n",
        "  if(counter_global%10==0):\n",
        "    print(loss)\n",
        "    #print(advantage)\n",
        "    #print(temp[0])\n",
        "    print(step.mean())\n",
        "  #train the model\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(30.6462, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0667)\n",
            "tensor(8.3975, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0667)\n",
            "tensor(38.3963, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3333)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}